## 17 Log-Structured Database & Vector Database

1.  先讲了需要给 redis 的信息加入一个超时的时间，如果一开始的时候、就把所有书都放入到 redis 里，当 3600s 到了之后，那么这些 book 对象全部失效了，那么大量的访问书的请求就需要重新去数据库里拿。这个情况就是雪崩的情况。如何解决呢、比如说可以把这个 10K 的分成 10 个 1K 的，然后让他们逐个失效。第二个情况是、如果 10w 个请求同时读一个不在 redis 里的数据、不加处理的话、会导致 10w 个全部往数据库里去拿、这个就是击穿的情况。如何解决呢、可以加一个锁，当所有请求都访问一个数据的时候、只有一个请求去数据库里拿。第三个情况是，大量访问数据库中不存在的数据，那么全都会访问数据库，这个就是缓存的穿透现象。如何解决呢，可以在前面做一个过滤、比如用布隆过滤器，可以马上告诉你这个数据不在数据库里。（穿透其实是一种攻击）

2.  日志结构数据库：先前的 mysql、mongo 和 neo4j，都有一个隐含的前提、就是所有的数据访问的次数都是差不多的（体现不出新订单和旧订单访问次数的差异（没法处理热点数据的处理情况）（可以用 partition 做、但是还是需要用 timestamp 做、再根据其他字段做子分区也不方便）），所以就有了日志结构数据库。

3.  日志结构数据库用了 LSM-tree

    - Log 需要提高写入性能
    - Log 以 Append 的模式追加写入，不存在删除和修改
    - 这种结构虽然大大提升了数据的写入能力，却是以牺牲部分读取性能为代价，故此这种结构通常适合于写多读少的场景
    - C0 树（常驻内存）
    - C1-N 树(位于磁盘)

4.  因为要落盘的时间、所以会有 immutable memtable （rockdb 做了优化、这个 immutable table 可能会有多个）

5.  **Immutable Memtable 的概念**
    **Immutable Memtable** 是一种在数据写入后变为不可修改的 Memtable。它通常是在以下情况下使用的：

    - 当内存中的 Memtable 达到一定大小时，它会变为不可变的（Immutable）。
    - 不可变的 Memtable 会被标记为“不可修改”状态，任何新的写入操作都不会影响这个 Memtable。新的数据会被写入到一个新的 **Memtable** 中。
    - 一旦这个不可变的 Memtable 被生成，它会被异步地刷写到磁盘上的 SSTable（Sorted String Table）文件中。通过这种方式，数据库可以持续地将内存中的数据持久化到磁盘，同时又能避免频繁的磁盘写入操作。

6.  **为什么使用 Immutable Memtable**
    使用 **Immutable Memtable** 有几个优点：

    - **提高写入性能**：由于 Memtable 是内存中的数据结构，当数据被写入到 Memtable 后，它不会立即写入磁盘，而是会先暂存到内存中。当 Memtable 满了之后，它会变成 Immutable，避免了频繁的写入磁盘。新的写入操作会被加入到新的 Memtable 中。

    - **减少锁的争用**：当 Memtable 一旦变为 Immutable 后，它不再允许写入，因此可以避免多个线程在写操作时产生锁的争用，提升并发性能。

    - **异步持久化**：通过将数据从 Immutable Memtable 批量地刷新到磁盘，数据库可以通过异步方式持久化数据，避免每次写入都发生磁盘 I/O 操作，从而提高系统性能。

7.  **工作流程**

    1. **写操作**：当用户执行写操作时，数据首先被写入到 Memtable 中。如果 Memtable 是不可变的，那么数据会被写入到一个新的 Memtable 中。
    2. **转为 Immutable Memtable**：当 Memtable 达到一定大小时，它会变成 Immutable Memtable。
    3. **刷写到磁盘**：Immutable Memtable 被异步刷写到磁盘，通常是以 **SSTable** 格式进行持久化。
    4. **清理**：一旦 Immutable Memtable 被成功写入磁盘，旧的 Memtable 会被释放或清空，以为新的 Memtable 留出空间。

8.  **总结**

    - **Immutable Memtable** 是内存表的一种优化形式，它在数据写入后变为不可变，并且在满了之后会将数据批量写入磁盘。它主要用于提升数据库的写入性能，减少内存操作和磁盘操作之间的冲突，提供异步持久化的能力，从而实现高效的数据存储和查询。

9.  要先写到 WAL log （write ahead log）里面、才能再写到内存里面

    在 **LSM-Tree (Log-Structured Merge-Tree)** 中，**WAL (Write-Ahead Log)** 的作用与在传统的数据库系统中类似，它是一种确保数据持久性和一致性的重要机制。LSM-Tree 是一种专为优化写入和读取负载设计的数据结构，通常用于 NoSQL 数据库（例如：LevelDB、RocksDB 等）。在 LSM-Tree 中，WAL 主要是用来处理写入操作，并确保即使在系统崩溃时，数据也不会丢失。

    ### LSM-Tree 中的 WAL 工作原理

    LSM-Tree 的设计理念是通过将所有写操作先写入内存中的 **MemTable**，然后定期将 MemTable 的内容以批量的形式合并到磁盘上的 **SSTable**（Sorted String Table）中，来优化写入性能并降低磁盘 I/O。但是为了确保数据的可靠性，所有对 MemTable 的修改都会先记录到 WAL 中。

    #### WAL 在 LSM-Tree 中的流程

    1. **写入 WAL**：

    - 每当有新的写操作（如插入、更新或删除）时，LSM-Tree 会先将这些操作记录到 WAL 中，而不是直接修改 MemTable 或磁盘上的 SSTable。
    - 这种写操作会以 **顺序日志** 的形式保存在 WAL 中，这样可以减少随机写入的开销，提高性能。

    2. **MemTable 更新**：

    - 在写入 WAL 后，数据会被写入到 MemTable 中。MemTable 是一个内存中的结构，它会不断地接受写入操作，直到 MemTable 满。

    3. **刷新 MemTable 到 SSTable**：

    - 当 MemTable 满了之后，它会被刷新到磁盘上的 SSTable 中。这个过程涉及将 MemTable 中的数据持久化到磁盘，并将其与现有的 SSTable 合并。这里的合并过程可能会产生新的 SSTable 文件。

    4. **故障恢复**：

    - 如果在将 MemTable 中的数据刷新到磁盘之前，系统发生崩溃或故障，WAL 中的日志记录可以用于恢复数据。系统会在启动时读取 WAL 日志，重新执行日志中的写操作，并将这些数据恢复到 MemTable 中。这样，即使系统在数据写入磁盘之前崩溃，也能保证数据的完整性。

    5. **删除旧的 WAL**：

    - 当 MemTable 被成功写入到 SSTable 后，WAL 文件就可以被丢弃或重用。这是因为 MemTable 中的内容已经被永久保存到磁盘。

    #### WAL 在 LSM-Tree 中的优势

    1. **保证数据持久性**：

    - WAL 提供了故障恢复机制，即使在系统崩溃的情况下，未持久化到磁盘的数据仍然可以通过 WAL 恢复。

    2. **提高写入性能**：

    - 写入 WAL 是顺序操作，减少了磁盘的随机写入，提升了性能。这与 LSM-Tree 的设计哲学一致，旨在优化写入性能。

    3. **避免数据丢失**：

    - 即使 MemTable 在刷写前发生了崩溃，WAL 中的日志可以确保这些数据在崩溃后不会丢失。

    4. **延迟合并操作**：

    - LSM-Tree 延迟将 MemTable 中的数据合并到 SSTable 中，这有助于减少写入操作的频率。WAL 记录使得这一过程在后台进行，不影响数据的一致性。

    #### LSM-Tree 中 WAL 和传统数据库 WAL 的区别

    - **传统数据库的 WAL**：在传统的关系型数据库中，WAL 通常是为了保证每个单独的事务一致性和持久性。而 LSM-Tree 中的 WAL 更多的是作为一个写入缓存，确保数据能够在崩溃时恢复，避免丢失数据。

    - **写入方式**：传统数据库系统在每次数据修改时会先写 WAL，再进行磁盘更新；而在 LSM-Tree 中，写操作通常会先写入 WAL，并写入 MemTable（内存中），然后定期将数据批量刷写到磁盘的 SSTable 中。

    #### LSM-Tree 中的 WAL 需要注意的问题

    1. **WAL 大小**：WAL 文件可能会变得很大，特别是在系统大量写入时。通常系统会定期清理 WAL 文件（例如，在 MemTable 被成功持久化之后）。

    2. **恢复效率**：恢复过程中，如果 WAL 中有大量未写入磁盘的记录，可能会影响恢复时间。不过，LSM-Tree 的合并策略（例如，通过 `compaction`）可以有效地减少 WAL 恢复时的工作量。

10. 优点

    - 大幅度提高插入（修改、删除）性能
    - 空间放大率降低（可能会产生不同版本的数据，一个数据多个版本在里面的话、空间放大率就会变大）
    - 访问新数据更快，适合时序、实时存储（适合存双十一的订单）
    - 数据热度分布和 level 相关（经常不被访问到的数据就会沉到底层）

11. 缺点

    - 牺牲了读性能（一次可能访问多个层）
    - 读、写放大率提升

12. ppt p6 有 sstable 的定义

13. 会有一个 shared key 的概念、就是 abcd、abcd、abcf 前面共同的前缀 abc 就是 shared key（一个同样的 shared key 会存在一个 record group 里面）

14. 写放大：一直要合并到最后一层的时候，会涉及到很多层的合并

15. L0 是无序的，L1-Ln 是有序的

16. 读放大：比如读一个 key=61 的，但是不存在这个数据库里，如果每一层都找不到、就要一路找到最底层（如果不存在布隆过滤器的时候）

17. 老的数据也会在 compaction 的时候被 remove 掉（所有的 sstable 都不可更改、都是在 compaction 的时候重新写进行处理的）

18. 给出的 rocksdb 的例子是嵌入式执行的

19. 热点数据不是一个大量的数据，只需要尽可能快地访问到热点数据就可以了。如果热点数据不再成为热点数据之后、应该把高层的数据放到其他的数据库里（因为会有读放大和写放大）

20. HTAP Hybrid Transactional/Analytical Processing 混合事务分析处理 （这里提到了数据是应该按行存储还是按列存储）（因为在一个系统里面，下面这两种需求都会有，最好不要只有一种、应该混合，或者也可以按行存一轮、按列存一轮（但是也会有缺点、就是空间的浪费和维护一致性的两个问题））

21. Online Transaction Processing(OLTP) （要对随机的读写做支持，要做索引的维护、随机访问数据，可以按行存）（这里就不能按列存、因为需要在不同列地方存同一个订单的信息）

    - 在线事务处理（订单之类的）

    - 低延迟
    - 高并发
    - 数据量小

22. Online Analytical Processing(OLAP)

    - 在线分析处理（统计之类的）（这个如果按行存储的话、导致每一行的长度不一样、会造成很多的随机访问、所以可以按列存储，这样按顺序读就很快）

    - 高延迟
    - 低并发
    - 大量数据

23. LSM-Tree 抽象结构

    - 分层结构
    - 提供写优化
    - TP 事务友好

24. 这里有一个列族的概念（相当于垂直分区，但是管理起来比较麻烦、但是 LSM-tree 在 rocksDB 里面可以支持（可以共用一个 WAL 日志和 manifest files）

    - 比如说可以对和 sale 相关的信息用列存、然后做 OLAP、放在一个列族里面，用 lsm-tree 来存储列信息
    - 然后其他的数据用 row 存、然后支持 OLTP、存在另一个列族里面，用 lsm-tree 来存储行信息

25. RocksDB 写流程

    - 优 - 低延迟插入
    - 写入内存后直接返回
    - 后台异步写入磁盘

    - 劣 - 写阻塞问题
    - L0 层满时将阻塞内存到磁盘的 Flush 过程
    - L0 层下沉 Compaction 过程无法多任务执行
    - 异步写写放大严重，容易磁盘变成瓶颈

26. RocksDB 读流程

    - 劣 - 读放大问题
    - 不同层级存储着不同版本的数据
    - 需要访问所有可能的数据文件

27. 写阻塞问题降低了 TP 事务的可用性，读放大问题限制了 AP 查询的性能

28. 解决写阻塞问题，在 RocksDB 和数据源之间增加一个收集分发层，写阻塞时缓存数据（collector）（基于 apache flink）

    - 采用中心化设计：
      Master 采用主从备份
      监控集群负载，
      调控负载均衡。

    - 负载均衡策略基于剩余内存大小，
      即分配到某个节点的概率与剩余内存大小正比

    - 在 RocksDB 和数据源之间增加一个 **收集分发层 (collector)**，并基于 **Apache Flink** 来缓存数据，旨在解决写阻塞问题。这个 **collector** 的作用是管理、缓存和转发写入数据，以减少写操作对数据库性能的影响。具体来说，它会在以下几个方面起到作用：

    1. **缓解写阻塞（Write Backpressure）**

    - 在传统的写入过程中，如果 RocksDB 因为写入压力过大或者硬盘 I/O 瓶颈导致的写阻塞，数据的写入就会变得非常缓慢，甚至可能导致系统崩溃。
    - **Collector** 层将充当一个缓冲区，接收从数据源发来的写请求。当 RocksDB 无法立即处理写入时，Collector 会先缓存这些写入数据。缓存区的存在避免了直接的写阻塞现象，从而减少了系统的压力。

    2. **流量控制与数据流优化**

    - 基于 **Apache Flink** 这样的流处理框架，Collector 可以实时地收集数据，并通过 Flink 的流处理机制来优化数据的流向和批处理过程。
    - Flink 能够提供 **流量控制**，当 RocksDB 压力过大时，Flink 可以减缓写入速度，或者通过批量处理来提升写入的效率，从而避免数据堆积。
    - **Flink** 的时间窗口和流式操作（如批处理）可以帮助以更高效的方式将数据写入 RocksDB，从而缓解单个请求造成的阻塞问题。

    3. **数据预处理与转换**

    - 在数据写入 RocksDB 之前，**Collector** 层不仅仅是一个简单的缓存，还可以执行一些数据预处理和转换任务。例如，对数据进行格式化、过滤、聚合等操作，以减少数据写入时的计算压力。
    - **Flink** 提供强大的流处理能力，可以对数据进行实时计算和预处理，然后将处理后的数据推送到 RocksDB。

    4. **提高写入吞吐量（Throughput）**

    - 当数据源产生大量数据时，Collector 层能够缓存这些数据，并通过 Flink 的批处理机制来优化写入过程。即使 RocksDB 一时无法承载所有写入请求，Collector 也能按照一定的批次逐步将数据写入 RocksDB，确保系统的吞吐量不受过多影响。
    - Flink 的并行处理能力可以显著提高写入效率，确保即使在高并发的情况下，写操作也不会造成明显的延迟或阻塞。

    5. **容错性与数据可靠性**

    - Flink 提供内建的容错机制（如 **checkpointing**），使得 Collector 在出现故障时能够恢复数据，防止数据丢失。
    - 通过这种机制，Collector 能够确保在面对系统故障或者写阻塞时，数据不会丢失，并且能够继续流畅地处理后续写入操作。

    6. **流量平滑（Backpressure Handling）**

    - 在数据流入系统时，**backpressure** 可能会发生，尤其是当写操作过多，数据积压在 Collector 中时，Collector 可以通过 Flink 的流控制机制进行流量平滑，减缓数据的写入速度，避免系统崩溃。

29. 解决读放大问题：在 RocksDB 中增加列式存储，列式存储在访问少量列时磁盘读取量更小，可以减少读放大的开销（列式存储也是按照数据块的）

30. 提出混合存储策略

    - 常做事务的数据以行式存储，
    - 常做查询的数据以列式存储
    - 以磁盘读写开销为格式转换的指标

31. 然后讲向量数据库

    - 会有 one-hot 编码（所有的单词的点积为 0，每个单词都无关、但是这个缺点就是需要的维数太大）
    - 也会有一个对一个人的各种特征做向量化的处理（降维，然后可以计算两个向量之间的相似性，最后得到的数字都是数值型的，然后找最相似的时候，就去找和向量最相似的（最 naive 的方法，其实需要做更多预处理））
    - 所以向量数据库就是先存、然后做搜索（范围内的搜索，区别就是不是精确匹配、找的是最相似的，和 mongo 的 2d 的区别就是向量数据库支持更高维的）

32. 数据插入向量数据库之后也会需要做索引

33. 随机投影：为了降低数据的维数

    - 用一个随机的矩阵 random matrix generator，然后把原来的数据乘上这个矩阵，就可以得到一个降维或者升维的数据

34. 建立索引的时候、还可以做一个 product quantization，就是把向量分成几个部分，然后每个部分再做一个聚类（通过 codebook generator 转化成 code，然后查找的时候也把查找的 vector 变成多个 code 进行匹配）（比如就是把 500 种组合压缩成 10 种 code，这样就可以做近似匹配（因为不需要做精确匹配））

35. 还可以做位置敏感哈希 locality sensitive hashing，通过 hash 值决定它在哪个 bucket 里面，然后 query 的时候还是找最近的

36. HNSW（hierarchical navigable small world）就是多层的索引

37. 相似度计算：

    - 余弦相似度 Cosine similarity：It ranges from -1 to 1, where 1 represents identical vectors, 0 represents orthogonal vectors, and -1 represents vectors that are diametrically opposed. $\text{Cosine Similarity}(A, B) = \frac{A \cdot B}{|A| |B|}$

    - Euclidean distance: 
      It ranges from 0 to infinity, where 0 represents identical vectors, and larger values represent increasingly dissimilar vectors.

    - Dot product: 
      It ranges from -∞ to ∞, where a positive value represents vectors that point in the same direction, 0 represents orthogonal vectors, and a negative value represents vectors that point in opposite directions.

38. 向量数据库中的相似度计算方法用于衡量向量之间的相似度，常见的计算方式包括**余弦相似度 (Cosine Similarity)**、**欧几里得距离 (Euclidean Distance)** 和**点积 (Dot Product)**，每种方法采用不同的计算方式。以下是它们的具体计算方式：

### 1. **余弦相似度 (Cosine Similarity)**

余弦相似度衡量的是两个向量在角度上的相似度，忽略它们的大小，仅关注方向。它的计算方式如下：

$
\text{Cosine Similarity}(A, B) = \frac{A \cdot B}{\|A\| \|B\|}
$

其中：

- \(A $\cdot$ B\) 是向量 \(A\) 和 \(B\) 的点积。
- \(\|A\|\) 和 \(\|B\|\) 分别是向量 \(A\) 和 \(B\) 的模（即它们的长度）。

**范围**：余弦相似度的值范围是从 -1 到 1：

- 1 表示两个向量完全相同（方向相同）。
- 0 表示两个向量正交（无关）。
- -1 表示两个向量完全相反（方向相反）。

### 2. **欧几里得距离 (Euclidean Distance)**

欧几里得距离用于计算两个向量之间的直线距离，衡量它们在空间中的"相离"程度。其计算方式为：

$
\text{Euclidean Distance}(A, B) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}
$

其中：

- \($A_i$\) 和 \($B_i$\) 是向量 \($A$\) 和 \($B$\) 在第 \(i\) 个维度上的分量。

**范围**：欧几里得距离的值从 0 到正无穷大：

- 0 表示两个向量完全相同。
- 越大的值表示向量之间的差异越大。

### 3. **点积 (Dot Product)**

点积是一种简单的相似度度量，表示两个向量在某种程度上的"投影"关系。其计算方式为：

$\text{Dot Product}(A, B) = \sum_{i=1}^{n} A_i B_i$

其中：

- \($A_i$\) 和 \($B_i$\) 是向量 \($A$\) 和 \($B$\) 在第 \($i$\) 个维度上的分量。

**范围**：点积的值范围是从负无穷大到正无穷大：

- 正值表示两个向量的方向相似（指向相同或相似方向）。
- 0 表示两个向量正交（方向无关）。
- 负值表示两个向量方向相反。

### 总结：

- **余弦相似度**：衡量向量之间的角度差异，范围从 -1 到 1。
- **欧几里得距离**：衡量向量之间的直线距离，范围从 0 到正无穷。
- **点积**：衡量向量在同一方向上的投影大小，范围从负无穷大到正无穷。

不同的相似度计算方法适用于不同的应用场景，例如，余弦相似度适用于文本数据中的向量相似性比较，欧几里得距离常用于几何计算，而点积则常用于机器学习中的特征向量相似度计算。

32. 也可以通过向量的元数据进行过滤，然后再进行匹配

33. **随机投影（Random Projection）**  
    随机投影的基本思想是使用随机投影矩阵将高维向量投影到低维空间。

34. **（Product Quantization）**  
    这种方法将原始向量分解为更小的块，通过为每个块创建代表性的“代码”来简化每个块的表示，然后将所有块重新组合在一起。

35. **局部敏感哈希（Locality-Sensitive Hashing, LSH）**  
    局部敏感哈希（LSH）是一种在近似最近邻搜索中用于索引的技术。

36. **（Hierarchical Navigable Small World, HNSW）**  
    （HNSW）创建一个层次化的树状结构，每个树的节点代表一组向量。节点之间的边表示向量之间的相似性。

37. **元数据（Metadata）**  
    每个存储在数据库中的向量还包括元数据。除了能够查询相似向量外，向量数据库还可以根据元数据查询来过滤结果。

38. **向量索引和元数据索引**  
    为了实现基于元数据的过滤，向量数据库通常维护两个索引：一个是向量索引，另一个是元数据索引。

## 18 Timeseries Database

1.  云数据库走 serverless 的模式

2.  时序数据库的特点

    1. 时序数据库里的数据是有一定的存活时间的，比如一个小时之内电脑的状态

    2. 数据的格式简单，数值在一定范围内波动

    3. 所以不一定需要存原值、可以存差值（difference），好处是当数值很大的时候、差值会比较少、可以节省空间

    4. 所有的数据都带 timestamp（和 sstable 一样，存的时候可以带一个前缀，比如前几个 time 是一样的、就可以合并）（也可以存差值）

3.  时序数据库的数据叫做度量，是通过监控和下采样得到的

    - 特点是数据会源源不断地来
    - 需要做数据的生命周期管理
    - 关心的不是单点的数据、而是一段时间的总结性的数据（比如平均利用率，总和、标准差）
    - 所以也需要扫描一大堆的数据（经常做的操作）

4.  serverless 的会涉及到冷启动的 docker 的问题、一开始的体验不会很好

5.  influxdb 用 telegraf 来采集数据

    - flux 是类似于 sql 的查询语言

6.  课上监控了 cpu 的状态

7.  bucket 相当于数据库表的名字，但是没有库的概念

    - \_time 是时间戳
    - \_measurement 是一个 bucket 里面，在度量什么
    - \_field 是字段的名字，key
    - \_value 是值
    - 然后也会有 tag key 和 tag value，用来表示后面的值的特征

8.  时序数据库的索引

    - \_field 是不参与索引的，因为只能在 value 上做索引、但是没有意义，这个是搜索的结果，而且取值不是唯一的，所以建索引没有意义
    - \_tag 是参与索引的，因为我们经常在 tag 这一列上去做搜索和筛选，而且 tag 的值是固定的，所以建索引有意义（写时有额外开销、但是搜索快）
    - 所以其实 tag-field 之间在设计的时候是可以互相转化的，如果操作上不需要做全表的 scan、可以在根据搜索过滤的条件、把对应的数据转化成 tag，然后再去做搜索

9.  series

    - 有时候查询出来的数据是一系列的，这个就是 series
    - A series key is a collection of points that share a measurement, tag set, and field key

10. point：A point includes the series key, a field value, and a timestamp.

11. bucket：A bucket combines the concept of a database and a retention period (the duration of time that each data point persists). A bucket belongs to an organization.

    - 超过一定时间会删掉数据

12. organization：An InfluxDB organization is a workspace for a group of users. All dashboards, tasks, buckets, and users belong to an organization.

13. InfluxDB design principle

    1. 为了提升性能、这个数据是按照时间升序排列的、所以是可以 append 来做插入的
    2. 数据是一次性写入、不允许再修改（而且不存在新版本的数据、只存在新数据，每个数据时间戳都是不一样的）（这个不能修改和日志合并树很类似）
    3. 因为有可能会在做 query 的时候、不断地有数据进来、Therefore, if the ingest rate is high (multiple writes per ms), query results may not include the most recent data.
    4. Schemaless design：Time series data are often ephemeral, meaning the data appears for a few hours and then goes away.For example, a new host that gets started and reports for a while and then gets shut down.
    5. 没有 ID（和关系型数据库不同）Points are differentiated by timestamp and series, so don’t have IDs in the traditional sense.
    6. 对于 Duplicate data，InfluxDB assumes data sent multiple times is duplicate data. Identical points aren’t stored twice.（好处就是记录的数据会进一步减少）

14. The storage engine includes the following components:

    - Write Ahead Log (WAL)
    - Cache
    - Time-Structed Merge Tree (TSM)
    - Time Series Index (TSI)

15. Writing data from API to disk

    - 数据点如果非常多地进来、会需要先进行压缩，否则 WAL 会很快地变得很大
    - points 写进来会先写到 cache 里面
    - 然后 cache 会周期性地通过 TSM 文件写到磁盘里面
    - （和 LSM 很像） As TSM files accumulate, the storage engine combines and compacts accumulated them into higher level TSM files.

16. cache

    - 通过组织一组值的 key (measurement, tag set, and unique field) （series）

17. TSM

    - Column-oriented storage lets the engine read by series key and omit extraneous data.
    - 用的是列存、因为经常要做 scan、所以用列存会更快
    - 也分为 shared key 和 non-shared key

18. TSI：time series index

    - 基于 series key 做的索引
    - The TSI stores series keys grouped by measurement, tag, and field

19. binlog 不应该和 MySQL 放在一起、因为如果遇到的是硬盘损坏、那么数据和 binlog 都会丢失、所以应该放在不同的地方

20. InfluxDB file structure

    - blotdb 用来存小的用户数据
    - Configs path 用来存 token 之类的内容

21. InfluxDB shards and shard groups

    - 大量的数据进来、存不下、就用 shard
    - A shard contains encoded and compressed time series data for a given time range defined by the shard group duration.（压缩过的数据）
    - 会根据不同的时间段来存不同的 Shard group （设置 duration）
    - 只有这个时序数据库会自动帮你删除、其他种类的数据库都需要自己去删除（主要是对于原始的数据点不太关心、关心的是一段时间的总结性数据）

22. shard life circle

    - 可以预先做 Shard precreation，然后之后写的时候就会快一点
    - InfluxDB writes time series data to un-compacted or “hot” shards. When a shard is no longer actively written to, InfluxDB compacts shard data, resulting in a “cold” shard.
    - Shard compaction（Level 1 (L1)（在内存里）: InfluxDB flushes all newly written data held in an in-memory cache to disk. Level 2 (L2): InfluxDB compacts up to eight L1-compacted files into one or more L2 files ）
    - 也可以设置 retention forever、然后就永远不会删除数据

23. 这里的代码例子上课的时候没听、复习的时候可以听一下

## 19 GaussDB

1. 不是自己写的、是基于 postgresql 等数据库的，主要是为了做分布式（比如分布式里会有 2PC 之类的）

2. GaussDB 技术架构

   - 分布式优化

   1. 分布式近数计算：数据的计算的位置应该尽量靠近数据节点的位置，效率能够提高

   2. 全链路编译执行：类似于 prepared statement，可以提前编译好、然后把参数传进去再执行、这样可以提高效率（用的是 JIT）

   3. 大规模并发事务处理

   4. sql 解析：类似于编译器的优化（query plan），

   5. RBO：比如可以基于一些数据库的规则、把一些查询优化成更好的查询（基于规则的优化）根据预定义的启发式规则对 SQL 语句进行优化（比如贪心算法，有一个全局最优解、但是算不出来）（还有一个 meta-heristic，可以跳出局部最优解找出全局最优解）

   6. CBO：基于查询的代价来做优化

   7. Hint：辅助优化器的工具

   8. postgres 没有多线程、gaussdb 做了线程池

   9. 向量化：提高多核效率（充分利用多核的效率、比如处理 tensor 的时候，一次一批做（batch））

   - 多地容灾

   1. 多地多活容灾（两地三中心，另一个城市用于容灾）

   - 云原生弹性伸缩

   1. 纵向：增加进程可用的内存、cpu

   2. 横向：增加计算节点服务器

   3. 多租户（重要）

   - 智能优化

   1. ABO：AI based optimization（基于怎么样的 ai 模型能做出更好的优化）

   - 安全隐私

   1. 防篡改：比如用了区块链技术

3. coordinator node 用来协调（比如分布式事务），data node 用来存数据和执行计算

4. 优化器优化出来的查询可能对某些数据库表现好、有些数据库表现不好

5. 会有缓存层和存储层之间的 RDMA（Remote Direct Memory Access）（不需要 CPU 的参与、直接在内存和硬盘之间传输数据）

6. 多租户：主要问题在于维护后期的差异、比如不同的租户会有不同的数据表的要求，有四个方案

   - 可以用两个分别的小的数据库
   - 可以用一个大的数据库，然后在一个大数据库里面维护两张表
   - 可以在一张表里加一个字段是租户 id，然后在查询的时候加一个条件，然后增加修改字段的时候可以用外键关联（相同的列存在一张表里）
   - 可以把列转成行去做，这样就可以维护多租户（主键+列名+取值），缺点是这张表可能会很大
   - GaussDB 实现了多租户、而且可以选择方式（但是不同的方式代价不一样）
   - 可能形式上有 6 个数据库、但是实际上只有 5 个
   - 相同的数据库最好只有一个实例

7. 查询重写中的谓词下推：

   - 有两个 data node、一种方法是通过把数据表上推到 coordinator node 做计算，另一种方法是把计算下推到 data node 上去做（比如把 t1 和 t2 表的连接操作推到 data node 上去做）

8. GaussDB 计划生成：GaussDB 主要采用自底向上与随机搜索相结合的方式

9. 全局计划缓存：每个 session 可以复用全局其他 session 已经执行过的缓存、提高效率

10. 页面结构：行存储以页面为单位

    - 因为可能会有 varchar 这样每个元素不一样的字段、所以需要 pointer 的槽位去指向这些元组的位置
    - 元组信息前面四个比较重要，需要做 MVCC（因为每个元组的版本号是不一样的）
    - xmin 和 xmax 决定了元组的生命期，亦即该版本的可见性窗口。（ppt 上有一个表格）

11. ppt 例子是在同一个事务里做两次更新，同一个事务里对一个记录改两次、就会改动 t_cid。而 t_ctid 指向的是最新的记录，页面中的第几个元组，有新的变动的时候就会更新这个值。

    - 保存历史数据：为了做回滚和做快照

12. CSNLog：记录 XID 与 CSN 的映射关系，为每个事务生成一个唯一递增的 CSN，用于将事务与其可见性进行关联

    - Clog：记录事务 ID 的运行状态：运行中/提交/回滚

    - 当事务结束后，使用 CLOG 记录是否提交，使用 CSNLOG 记录该事务提交的序列

13. 有一个 Ring Buffer？环形缓冲区

14. 列存里面可以用 min 和 max 做 index

15. 预分片：预先分配好 shard，然后在写入的时候就不用再去分配 shard 了

16. 热备：请求同时发到两个节点上，然后一个节点挂了，另一个节点可以继续工作，用户无感知（能实现 RPO=0，但是是有资源的浪费的代价）

17. 暖备：请求只发到一个节点上，然后另一个节点挂了，用户会感知到（fail 了，需要重发）

18. 定期切换冷备中的主从节点：为了知道这个冷备的节点是不是崩了

19. 讲了一个 sql 注入的例子和一个跨站脚本攻击的例子

## 20 Data Lake

1. 如果有很多数据分布在不同种类的数据库中、需要抽取到一个统一的地方然后，访问的时候对这个地方做 OLAP。但是会有很多 ETL 的操作（抽取、转换、加载）

2. 所以数据湖就是为了解决上面的问题，直接存储 raw data（各种不同的数据源的不同格式的数据）

3. 所以数据湖需要很强的数据接入能力（通过 flink 工具）

4. 可以通过 metadata 来发现需要操作哪些类的数据库，然后通过 calcite 来做针对不同的查询语言的转换

5. 数据湖和数据仓库有不同功能和特点的对比

6. filter 需要进行实时的流式处理（比如推文的立即过滤）、原来的 hdfs （批处理）对于这种实时处理支持不太好，因此产生了 lambda 架构（batch 和 stream 两种方式）

7. 也可以控制流计算的时间窗口、比如 5s 一个窗口，然后统一处理（流批一体）

8. 更新的时候不做 ETL、萃取、沉淀的时候做 ETL

9. datalake 的意义还在于可以支撑更多的数据分析、比如机器学习、数据挖掘，而不仅仅是直接做完 etl 之后进入 data warehouse 的数据分析

10. incremental ETL：按需去做对应数据的 ETL，而不是全部的数据都做 ETL

11. 边缘计算就是基站里的这些服务器里也有 cpu 和资源，可以利用这些资源做一些计算（但是计算能力有限）

12. 比如一辆车开、它会不断地把数据发到相邻的边缘服务器里面（因为总是在连接到边缘节点里，压缩后传输可以减少 edge 到 cloud 的带宽占用），然后定期地从边缘节点同步数据到云中心节点里面（所以有些 query 既需要在中心 cloud server 里面找、也要在 edge server 里面找）

13. 边缘存储架构比较简单、是 tag 或者是 key-value、因为计算资源有限

14. 边缘节点会不断地发数据流元数据到云数据中心，然后之后的查询就会知道在哪些边缘节点去查找了

15. Rosetta 过滤器：多层 bloom 过滤器，减少假阳性

16. 这里的下推是指把计算下推到边缘节点去做、比如先在边缘节点做过滤和聚合（比如做 group by），然后再把数据传到云中心节点去做 union 和 aggregate（这里再做 count）

    - 好处就是原来是把整表拿上来、造成数据传输很多，现在是过滤了一下，所以数据传输就会少
    - 然后还有一个好处就是利用了节点的计算能力，减少了云中心节点的压力，整体性能也会提高

17. 核心问题两个：

    - 定位数据
    - 并行处理

## 21 Clustering

1. 集群的目的就是提高系统的可靠性和可扩展性

2. 负载均衡的至少 3 种不同策略

   - 轮询策略：依次选择一个节点
   - 选择最少连接的节点
   - hash 策略：根据请求的 ip 地址或者请求的 url 去 hash，然后把请求发到对应的节点上去

3. session stickiness 会话粘滞性问题：比如用户登录之后、会话会一直保持在一个节点上，这样就会造成负载不均衡

   - 可以用 ip-hash 的方法，把同一个 ip 的请求都发到同一个节点上去

4. Request-level failover 和 Session failover

   - 因为服务器会崩、需要重做（容错就是重做）（所以操作需要幂等性）
   - 一种方法是广播
   - 还有一种方法是专门的节点是设置一个 session 服务器（缺点是这个服务器就变成了单一故障节点）（好处是其他节点都变成了无状态的节点）

5. 正向代理：相当于服务器视所有使用代理的客户端请求为同一个客户端的请求

6. 反向代理：相当于一个 load balancer、把请求分发到不同的服务器上去

7. 还可以加 Weighted load balancing

8. mysql 的 innoDB 也可以配置集群（mysql shell + router）

   - 使用的时候、是前端连接到 router
   - 就可以做到写请求只写一个、然后读请求会做负载均衡

9. 12 月 9 日这里还讲了一个配置 mysql 集群的操作，不知道在干什么，可以再回看一下

10. 做 configurationInstance 的时候、几个实例需要完全一样（mysql 版本、配置、数据），而且需要至少 3 个实例

11. 返回结果的时候、是通过 nginx 进行返回的，还是不通过 nginx 进行返回的？（如果是，还能不能那么高效？可以通过 nginx 集群增加数量）

    - 是经过 nginx 统一中转再返回到前端的，客户端访问和看到的是 Nginx 而不是 实际 server(自己看一下 HTTP Header 中的 Server 头就能确认，确实是 nginx)
    - 屏蔽了后端的真实 IP，增加了安全性，也避免了差异性
    - nginx 做了优化
    - Nginx 的非阻塞事件驱动模型：Nginx 采用的是基于事件驱动的非阻塞模型（即单线程异步 I/O），能够高效地处理大量并发请求。这使得 Nginx 即使在高并发环境下也能保持高效的响应速度。
    - Nginx 内存效率高：Nginx 在处理请求时，使用的是少量的内存，并且它的处理模型使得它可以在不占用过多资源的情况下，处理大量的并发连接。

## 22 Cloud Computing

1. 网格计算：每个人可以把自己的计算资源共享，然后使用计算资源的时候不用在意是谁的计算资源

2. 云计算的特点

   - 弹性伸缩
   - 按需付费
   - 服务化
   - 无限扩展性
   - 快速虚拟化

3. 也会涉及到多租户的情况

   - 最简单的相当于在门户网站上定制自己的 ui，但是本质上是一个服务
   - 然后云服务把所有软件和硬件都提供为服务

4. 会有三种情况：

   - long-term
   - short-term
   - onspot（拍卖）：灵活定价
   - 可以根据需求去选择不同的云服务商（课上的曲线）以降低成本

5. 垂直扩展和水平扩展

   - 垂直扩展就是增加单个节点的资源
   - 水平扩展就是增加节点的数量
   - 云提供商应该都支持这些操作

6. google 提出了 mapreduce

   - 使得云上的操作系统可以进行 job scheduling
   - 首先输入
   - map phase：把原文映射成了很多个键值对（然后每次一个 worker 会处理一个 split）
   - 然后存在了一个 intermediate file 里面（等到所有 split 都被处理完之后、就会进行 reduce）
   - reduce phase：把相同 key 的 value 进行合并（可以会有不同的区间范围规则。比如 A-N 由 worker1 处理，O-Z 由 worker2 处理）
   - 然后输出
   - 这个是一个批式处理的过程

7. 然后需要一个分布式的文件系统 DFS，谷歌提出了 GFS

   - 有一个 master 节点，然后有很多 chunk server
   - master 节点负责管理 chunk server，chunk server 负责存储 chunk
   - master 节点会记录 chunk 的 metadata
   - 有一个 shadow master 节点，用来备份 master 节点的信息
   - 有一个 chunk server 会有很多个 chunk，但是一个 chunk 只会存在于一个 chunk server 上
   - 返回的是 handler

8. 文件系统在多个用户使用的时候、需要考虑并发问题

   - 并发控制
   - 还需要应对错误
   - 分离的服务器越多、可靠性越差
   - 需要使用 Write Control and Data Flow 增加副本，提高可靠性（解决一致性问题）
   - 增加 replica 的数量、可以提高系统的可用性，但是会增加系统的负担（所以写操作的时候性能会下降）（如果不一致、这个写操作就失败）（也是必须要做的）

9. 数据存储的时候需要用到 bigtable

   - 如果用传统的关系型数据库、会有很多的外键关联和检查（而且会需要很多不同机器上的 join 操作和不同进程间通信）
   - 如果横着存、很多时候会实现不了（如果采用切开等价类，采用语义等价的情况）
   - bigtable 就把很多表之间的关系关联都切除、就可以把数据存储在不同的机器上
   - 可以通过再加一列提高它的表现性（比如把几个列合并加成列族，然后再动态加一列新的，允许字段为空，所以对结构化的要更灵活）

10. 所有 table 和 tablet 都可以进行切分

    - 然后数据库的入口就在了 root tablet
    - chubby file 能够维护一个锁，保证并行安全

11. OS：

    - FS：GFS
    - job scheduling：mapreduce
    - DB：Bigtable
    - mem（内存管理）：这个就没有统一架构了、每个系统有自己的实现（可以像 gaussdb 一样看作是一个大的内存整体）

12. hadoop 就是上面这些内容的开源实现

13. 云原生：cloud native

    - microservice -> serverless
    - VM -> container
    - CI/CD（持续开发、持续集成、持续部署，增量式开发）

14. 边缘计算

    - 比如有很多基站，里面其实也是有服务器的
    - 边缘设备和云不能持续性连接
    - 所以需要节省带宽，可以间歇性地把数据发到最近的基站上，然后选择性地把数据发到云上去

15. 边缘计算的实际场景（云边端融合的场景）：

    - 比如说视频分析，会在摄像头里直接使用计算资源进行计算
    - 如果超过了摄像头的处理能力、那么就发到云里去做处理（动态变化的）
    - 或者说智能家居（边缘操作系统实现互联）
    - 或者说如果实现手机的 ai 计算、可以把一些计算放到附近的 MEC（Mobile Edge Computing）上去做
    - 然后逐层地动态决定在哪一层上进行处理

16. 不同的模式名称

    - Local Execution （全部本地处理）
    - Full offloading （全部发送）
    - Partial offloading （部分本地处理，部分发送）

17. 下面讲了 graphql

    - 能不能对客户端更友好，客户端可以根据查询的语句只获取自己需要的数据、而不是整个数据，比如搜索 id 返回 id
    - graphql 在后端直接根据前端的 query 进行组装，然后返回结果给前端（和 restful 不同）

18. 也需要写 resource 里的 schema

## 23 docker

1. 一开始讲了一个例子、就是在一个表上如果有一个复合主键（a 和 b）、会自动地先在这个复合主键上面做索引，然后再新建了一个 b 和 c 两个键的索引（默认都是升序的）

   - 查询的时候、如果在最前面加一个 explain、会解释这个查询是怎么执行的
   - 比如说如果做 EXPLAIN SELECT \* FROM table WHERE a = 1 AND b = 2 AND c > 1 ，会返回 possible_keys 是这两个都可以，就是 PRIMARY 和 b_c，然后这里会先用 PRIMARY（然后 key_len 是 8，因为是两个 int，a 和 b，所以是 4+4=8）
   - EXPLAIN SELECT \* FROM table WHERE a = 1 AND b > 2 AND c < 1 也用 PRIMARY , key_len 是 8
   - EXPLAIN SELECT \* FROM table WHERE b = 2 AND c > 1 也用 b_c（先用 b 搜然后搜出来之后再用 c 去筛选, key_len 是 8）
   - 但是有一个情况不一样、就是 EXPLAIN SELECT \* FROM table WHERE b > 2 AND c > 1， key_len 是 4，虽然仍然是在用这个索引，但是只用了 b 这个索引，没有用到 c 这个索引因为相当于是在 b=8 和 b=9 上、分别有不同的关于 c 的记录，所以找到对应的大于的 b 之后、需要进一步地在子树 c 里面去做遍历、而且它的 c 的值可能不连续
   - 所以在范围查找的时候、只有在第一个碰到的时候才会用到这个索引、之后就不会用到了，会在这个索引上做遍历 scan （b > 2 的时候、就会用到这个索引，但是 c > 1 的时候就会遍历）
   - 所以设计索引的时候需要考虑到很多是不是会有范围查找的情况，因为碰到第一个范围查找之后、这个索引就不会再被使用了

2. image 就是对整个运行环境进行了打包。docker 是在一个 linux 的 os 环境的上层进行打包运行的

   - 分层的目的是什么、比如我们用 spring boot 开发一个项目、别人用 c#开发、其中有一个共同的层、那么这个共同的层就可以抽取出来做复用。（pull 的时候可以节省空间）
   - 就是不同的容器可能会共享相同的层
   - 用的是 cgroup 和 namespace 将进程隔离开
   - 通过 cgroup 保证不同容器的文件系统是隔离开的
   - 虚拟机是每一个虚拟机都有一个独立的操作系统，而容器是共享一个操作系统

3. dockerfile 主要负责怎么打包这个 image

4. 能不能不要每次修改原始文件的时候都重新打包这个镜像呢？

   - 也可以在环境中动态添加文件
   - 可以用 volume，把本地的文件映射到容器里面去
   - 这个卷的名称好像也可以指定固定的路径+名称、而不用每次都用路径
   - 有差别、一个是容器虚拟机内部管理、还有一个是本地管理（挂载）

5. 下面的每一行代码的意思是

```shell
$ docker run -dp 127.0.0.1:3000:3000 \
    -w /app --mount type=bind,src="$(pwd)",target=/app \
    node:18-alpine \
    sh -c "yarn install && yarn run dev"
```

    - -d 是后台运行
    - -p 是端口映射
    - -w 是工作目录
    - --mount 是挂载,这里挂载了当前目录到 /app
    - node:18-alpine 是镜像

6. 容器间要能互相通信、需要加入共同的网络

7. docker compose 一次起多个容器

## 24 Hadoop

1. 一个操作系统需要

   - 文件系统 GFS
   - job scheduling mapreduce
   - memory management 各个不一样
   - database bigtable
   - hadoop 包含了上面的所有

2. 在配置文件里面、一定要有下面的内容，指定了 hdfs 的地址和端口

```xml
etc/hadoop/core-site.xml:
     <configuration>
         <property>
             <name>fs.defaultFS</name>
             <value>hdfs://localhost:9000</value>
         </property>
      </configuration>
```

3. 下面配置副本数

```xml
etc/hadoop/hdfs-site.xml:
     <configuration>
         <property>
             <name>dfs.replication</name>
             <value>1</value>
         </property>
     </configuration>
```

3. hdfs 是基于 linux fs 上面运行的、而不是纯基于底层硬件的

4. 文件分区表存在下面的 namenode 里面

```shell
Format the filesystem:
$ bin/hdfs namenode -format
```

5. namenode 是客户端每次访问的必经之路

   - 存储了每个 file 的所有 block 底层上存在了 linuxFS 中的哪里

6. 可以在 localhost:9870 查看 namenode 的状态

7. 实际的运行流程就是先启动 hdfs 集群、然后在上面指定地运行 mapreduce 操作、比如丢一个 jar 包，然后等待结果

8. 和云计算那一节和 cse 中提到的差不多

9. 例子里的时候、是会对两个 string 1 合并成 string [1,1]，然后统计总数

10. 第一次做 reduce、会在 mapper worker 结束之后的本地去做（这次操作叫做 combine，把本地的次数合并），第二次做 reduce 是在 reduce worker 里面去做（是对多个本地合并后的结果再去做 reduce）

11. 然后这个 intermediate file、图里有两列、那么第一列永远是 reducer 1 的 partition、第二列是 reducer 2 的 partition

12. 需要进行 shuffle（就是 sort 排序），然后再进行合并（为了减少随机 io）

13. combine 和 reduce 可以用不同的函数、也可以用相同的函数

14. 还有一个例子是找不同年份中温度的最大值

15. combiner 也可以让在网络上传递的数据变少

16. 有些时候也不需要 combiner、比如传输的数据不多的时候

17. 但是有时候不能用 combiner、比如求平均值的时候、除非每个本地的数目都相同

18. 所以要具体问题具体分析

19. 导入这个库后、貌似这个进程会跑一个 hadoop 集群

20. 一种常见的情况是、一个 outputfile 包含了 A-N 的、还有一个包含了 O-Z 的、那么就需要两个 reduce worker 去处理，最后存到不同的 replica 里面

21. 也可以没有 reduce、做完 map 之后就存

22. 首先是 job client 根据程序生成一个 job、然后给 jobtracker

23. jobtracker 会做资源的管理、他会记录每个机器的状态和任务、然后记录心跳包、然后如果发现某个节点挂了、那么就会重新分配任务到节点

24. jobtracker 会是一个瓶颈

25. 给定一个时间段里、会需要接收到固定数量的心跳、不然就会认为这个节点挂了（因为 job 的时长可能会不同）

26. reducer 的结果可能不是有序的

27. 这个排序的比较器也可以自己指定

28. 然后讲了一个问题、就是因为每个 split 可能也不是足够小的、所以每个 mapper 在做 map 的过程中、如果内存满了、会把内容以 spill 的形式写到硬盘上（overflow 了）

    - mapReduce 之中会使用大量的磁盘 IO、在所有的步骤中都会涉及到（都是在磁盘中实现的）
    - 而且会涉及到可靠性的问题、如果机器崩了、那么前面的任务就白做了
    - 可以做一个优化、比如直接把 spill 放到待 reduce 的队列中

29. 由于 job tracker 是把资源管理和 job scheduling 都放在一起、所以可能会有瓶颈

30. yarn 就是 mapreduce 的升级版，每个应用都有一个 applicationmaster（每一个 job 都有一个 tracker 相当于），然后有一个全局的 resource manager

31. 相当于管理一个 job 的所有容器的状态都丢给了 application master，然后请求新资源的时候会通过向给 resource manager 去请求，这就实现了解耦、职责分离了，然后一个 job 崩了、另外的 job 还可以正常进行

32. 这样就没有了性能瓶颈和单一故障节点

33. 但是都是对用户无感知的

## 25 Spark

1. 也是对大规模的数据做分析、但是是放在内存里做的

2. hadoop 会涉及到很多 IO、所以性能比较差

3. 例子的目的是在一个很大的 log 中、过滤出来一个小部分的报错的信息

4. 这个 lines 就是 RDD 的一个实例、一旦读到了内存里之后这个值就不能被修改了、这也意味着这个值可以被复用（多线程的竞争就可以免除掉）

5. filter 可以使得一个 RDD lines 变成另一个 RDD errors

6. 同样地，map 也是一样的、可以把一个 RDD 变成另一个 RDD

7. 为什么要 cache 住呢，因为内存可能会满，所以需要涉及 LRU，除了 LRU、还有 swap、就是把内存里的内容写到硬盘上。但是 cache 住了之后、这个值就不会被释放掉了（无论做 LRU 还是 swap 都不会被释放，一直在内存里，但是这只是一个愿望，如果全都满了，还是会进行替换）

8. 然后其实本质上等到 cache 这行的时候、才会反推执行前面这三步一起执行、内存里会先将它们进入计算图再执行。（通过 stage 执行，惰性）（所以抛错误是在 cache 这一行才报的错）

9. 同样地直到执行.count 的时候才会执行前面的.filter。这些操作叫做 action（比如.count，.cache），才会反推进行前面的计算（前面的叫做 transformation）。然后这个 filter 和 count 都是在多个节点上进行的、然后再合并成一个的。

10. spark 也是分为 worker 和 manager 的

11. 先起一个 spark 集群、然后使用 spark-submit 提交一个脚本、然后就能执行任务了。

12. 这里需要用 spark submit exe 去运行一个 jar 包才能运行对应的 java 代码

13. scala 也可以变成一个 jar 包去运行

14. RDD 是一个弹性分布式数据集（Resilient Distributed Dataset），可以并行地去操作。也可以把内存里不是 RDD 的内容通过 sparkcontent 变成 RDD

15. RDD 一般 load 之后都是分 partition 的

16. 所有 transformation 操作都是 lazy 的。比如上面的这些 filter 操作

17. 中间可以看一下回放

18. 这个 An Easy Example 的例子、当没有 partition 的时候。如果要生成一个结果、那么这个 join 就需要读所有的 userDate 和所有的 events

19. 如果已经用 hash 的方式把 userData 做了分区、那么久可以每个 join 对应一个 userData 的块去做操作

20. 窄依赖就意味着性能会比较高、宽依赖就意味着性能会比较差

21. Narrow Dependencies VS. Wide Dependencies

    - 宽依赖往往意味着 Shuffle 操作，可能涉及多个节点的数据传输
    - 当 RDD 分区丢失时，Spark 会对数据进行重算
    - 窄依赖只需计算丢失 RDD 的父分区，不同节点间可以并行计算，能更有效地进行节点的恢复
    - 宽依赖中，重算的子 RDD 分区往往来源自多个父 RDD 分区，其中只有一部分数据用于恢复，造成了不必要的冗余，甚至需要整体重新计算

22. 下面这个例子是只有当发生宽依赖的时候、才进行计算、然后再进行前面还没有进行的计算

    - 好处是可以节省很多中间结果的 RDD、可以节省内存，而且可以并行化
    - 而且每个 stage 内部的计算会很快、因为可能这个数据和计算发生在同一台机器上（lazy）
    - 就是一个 stage 结束之后、才回推做 stage 前面还没做的很多操作
    - 每个阶段 stage 内部尽可能多地包含一组具有窄依赖关系的 transformations 操作，以便将它们流水线并行化（pipeline）
    - 边界有两种情况：一是宽依赖上的 Shuffle 操作；二是已缓存分区

23. 复习的时候感觉也可以重听一下、这里貌似有点绕

24. cache 也有不同的等级、比如 MEMORY_ONLY、MEMORY_AND_DISK、MEMORY_AND_DISK_SER、DISK_ONLY

25. dataframe 是有列名的数据、结构化更好，可以用语句去查询，也可以用 sql 语句去查询

26. 微观上是批处理、宏观上是流处理。这个例子是处理增量进入的数据、进行数据的统计。（就是 spark streaming）（可以通过 kafka 里读出来）

27. 也可以做机器学习和图计算（把顶点和边分别放在两个 RDD 里）（不是最优选择）

28. spark 可以做流批一体的计算

## 26 Storm

1. 这个 Storm 就是专门基于流数据的处理做的框架

2. 处理的中间结果叫做 Bolt（后面的都是 bolt）

3. 产生数据的源是 Spout

4. 一样是在微观上是批处理、宏观上是流处理

5. 同样也需要集群管理、默认用的是 zookeeper

6. 一个 worker 可以有多个 slots、就是多个进程用于运行多个不同的任务

7. 在 storm 上会运行 topology，但是这个任务是不会终止的（和 mapreduce 不同）

8. nimbus 管理机器节点的重启和工作等

9. 执行的时候和 spark 类似、需要通过 storm 命令运行一个 jar 包

10. 这里的例子里到底是实例数还是其他的？

11. 后面的 zookeeper 都没怎么再讲

## 27 hdfs

1. 适合 Very large files “Very large” in this context means files that are hundreds of megabytes, gigabytes, or terabytes in size. 因为存的 block 大

2. Hadoop doesn’t require expensive, highly reliable hardware to run on.所以不适合并行写

3. write-once, read-many-times pattern， the most efficient data processing pattern

4. HDFS is not a good fit when:

   - Low-latency data access. Remember, HDFS is optimized for delivering a high throughput of data, and this may be at the expense of latency.
   - Lots of small files . Since the namenode holds file system metadata in memory, the limit to the number of files in a file system is governed by the amount of memory on the namenode. （节省内存空间）
   - Multiple writers, arbitrary file modifications

5. 所以比如说存很多视频的时候、就适合用 HDFS，因为视频文件很大，而且不会经常修改

6. metadata 存在 namenode 里面

7. datanode 存储实际的数据

8. 所有的数据都不经过 namenode、只有 metadata 经过 namenode（减少瓶颈）

9. 副本数为 3 的话、可能在一个机架上放两个副本、一个在另一个机架上

10. 写操作是流水线式地写入每个 replica，写完每个之后才完成

11. 集群起：先起 namenode、然后 datanode、datanode 会给 namenode 发心跳包，然后就能加入集群、可扩展性就会变强、不需要重启

12. 通过发 blocks report 告诉 namenode 自己有哪些 block，然后要保证每个 block 都有 3 个 replica，然后每台机器上的 block 数量都要均衡（？）A Blockreport contains a list of all blocks on a DataNode.（因为 The DataNode has no knowledge about HDFS files. ）

13. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes.

14. namenode 专门用一台机器跑，和 datanode 分离、因为每次都是从 namenode 读取 metadata，所以要保证 namenode 的高可用性，要干净

15. 一个机器上跑多个 datanode 没有意义（因为两个 datanode 可能会在同一台机器上的时候、做冗余就没有意义了）

16. 可以创建目录、不支持硬/软链接

17. have strictly one writer at any time. （不能并行地写多个 replica，否则不能保证数据一致性）

18. 会有一个 rack-aware replica placement policy 机架感知策略，但是会减少性能（具体是根据网络延迟的距离来判别的，距离越大、写入的代价越大）（示意图）

19. (replicas - 1) / racks + 2 公式保证了每个 rack 上的 block 数量都是均衡的

20. 多个副本的时候、读操作可以做负载均衡

21. On startup, the NameNode enters a special state called Safemode. （在等 datanode 给它发心跳包，因为没有 datanode 的时候，它是不能工作的，所以处于安全模式）

22. EditLog 记录了所有操作，写完日志才会做文件操作

23. 这个日志文件不是 hdfs 的一部分、而是本地文件系统的一部分

24. 同样地、fsimage 也是本地文件系统的一部分，因为是启动 hdfs 必须的文件

25. 启动的时候也需要做对齐，有些 editlog 里的操作还没写入 fsimage 里面。

26. 这个会有一个 checkpoint、可以检查 fsimage 和 editlog 的一致性。也可以设置 checkpoint 的时间间隔，但是不能太大也不能太小，太小会影响性能，太大会影响空间开销

27. 如果一个收不到一个 datanode 的心跳包，那么就不会再往这个 datanode 上做 io 了、而且会给其他 datanode 发需要产生其他 replica 的请求

28. 如果是网络问题、也是不可用的

29. 如果存的数据太多、就会需要做数据迁移 Cluster Rebalancing

30. 会对所有的生成的文件生成一个校验码 checksum，保证 Data Integrity（不被篡改）

31. FsImage and the EditLog 也有多个副本、保证能够启动 hdfs，而且这两个文件的副本必须要实时同步

32. 数据块大小是 128MB，写一次多次读

33. 如果没有 ack 的话、就是会相当于没有写成功。但是还是有一定优化、比如如果就差 1MB 没写的话、还是会有快照之类的机制让它不要重试的时候重复再写一次

## 28 HBase

1. 在云计算那讲中的 Bigtable 的 hadoop 中的实现

2. 本身是一个大表、可以切开用 hdfs 存在不同的机器上（分布式）

3. 通过 root 找到要找的表的 metadata、然后根据 metadata 找到对应的 tablet

4. 像一个立方体的数据？

5. 它是按列存的

6. mysql 不做垂直分区、是因为 mysql 默认按行存、而且每个切开的行都要带主键，很麻烦

7. hbase 是按列存、所以可以做垂直分区，而且还支持列族的概念。

8. 典型的应用是 webtable、可以记住在不同时间点上的某个字段的不同的数据

9. Features

   - Linear and modular scalability.
   - Strictly consistent reads and writes.
   - Automatic and configurable sharding of tables
   - Automatic failover support between RegionServers.
   - Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.
   - Easy to use Java API for client access.
   - Block cache and Bloom Filters for real-time queries.
   - Query predicate push down via server side Filters （query 下推）
   - Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options
   - Extensible jruby-based (JIRB) shell
   - Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX

10. Tables are made of rows and columns.

    - Table cells—the intersection of row and column coordinates—are versioned. （是带版本的）
    - Hbase 里面的主键是完全有序地排列的，因为要记住每个 tablet 里面的键的最小和最大值（和 mysql 的多副本时不一样）

11. 列族的意义是：可以动态地加入一个表里（就是垂直分区的一个依据）（列族的数量不适合超过 3 个，因为会减少性能）

12. 要通过 namesever 改 metadata，需要先通过 chubby lock 获取锁 （重听？）

13. Tables are automatically partitioned horizontally by HBase into regions.

14. 这个图要重听一下

15. A common row key pattern is a website domain.

16. 这个会一直往时间戳之前读、直到读到一个不为空的值（所以 hbase 是稀疏的表）

17. 所以用一个类似 json 的结构去存（时间戳是倒序的、保证直接读就是最新值）

18. 如果在一个先前的时间戳去读的话、有时候会读到一个空值

19. 版本在读取的时候可以指定（时间戳是倒序的、保证直接读就是最新值）

20. 要先 disable 然后再能做增加列族的操作

21. 所以是三维立方体、有不同版本的维度，更灵活、没有要求每个表有严格一直的 schema

22. 关注的是大量的数据如何存储、而不是关注关系

## 29 Hive

1. hive 是在 hadoop 基础上发展起来的数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的 sql 查询功能，可以将 sql 语句转换为 MapReduce 任务进行运行。

2. 要在低版本的 java8 上运行

3. 很多数据仓库仍然用的是 hive 去做 metadata 的 store

4. 数据仓库和数据库最大的区别就是它是 schema on read 的状态。大量的数据仍然是以原始文件的状态存储在 hdfs 上面、然后 hive 在上面去读取这个 hdfs 里的内容。

5. 然后 sql 不可能直接执行在 csv 这种文件格式上、所以，hive 会先将这些文件做处理、然后转成一种中间格式、最常见的是 parquet 格式。

6. csv 加载进来的 dataframe 之后、有些内容字段会为空、所以会在进行 LOAD DATA 操作的时候、不会先做预处理（不做 ETL 操作、不做校验、数据清洗转换和加载）、直到 SQL（做在这个数据上面的分析时）的时候才会执行变成 parquet 的操作。这个是 schema on read 的特性。

7. 初始化 hive 的时候、可以填进去一个<db type>，默认是 derby、存的是 metadata 的信息。如果是 derby 的话、就是存储在内存里面、也可以存储在 mysql 里面，但是要告诉具体的位置在哪。

8. 也可以做分区、之后做搜索的时候就可以定位到一个分区里了

9. parquet 为什么是列存的？因为做的是 OLAP、就是在线的分析处理、都是会对同一列的数据进行操作、所以列存储的效率会更高。

10. 同样是 sql、为什么要存到 hive 里而不是 mysql 的？因为 hive 支持很多数据格式。而且 hive 执行 sql 表格的来源是多样的、不仅仅是 mysql table，还可以是 csv 等等。

11. Metastore 是 hive 的元数据存储的地方、hive 的元数据存储在 metastore 里面、metastore 是一个数据库、存储了 hive 的元数据信息、比如表的信息、表的结构、表的列、表的分区等等。可以分成独立的进程、也可以和 hive 在一起。

12. 是 schema on read 的状态、所以在 hive 里面、不会对数据做任何的校验和处理、只有在 query 查询的时候才会做处理。（好处就是一开始只需要直接拷过来、然后就很快）（而且场景是大量的数据、不可能一开始就做处理，多种数据源情况下的一个选择）

13. schema on write 的状态、就是在写入的时候就会做处理、比如 mysql、写入的时候就会做校验、数据清洗、转换等等。（查询的数据块，因为查询的时候就不用校验了）

14. （会把 tables 或者 partitions 进一步地划分成小的 buckets）用 buckets、查询方便，进一步减少查询的数据量。而且会做分布存储、不同的 bucket 会存储在不同的地方、这样查询的时候就会更快。也可以保证机器学习中采样的效率、保证整体的采样和部分的采样的一致性。（如果是按照性别区分的 buckets）

15. 存储的时候、会把表拆成不同的列族，变成 HBase。然后 RCFile 是会先吧几个行组织成 row group、然后在这个 row group 里面再按照列进行存储。就有助于 OLAP 的查询。

16. ORCFile 就是优化过的 RCFile。会有 Index data、row data、stripe footer。这样就可以快速定位到数据的位置、然后直接读取数据。（index data 存的就是 row data 中的 block 的 offset）

17. 重复数据多的时候、同样会有优化、比如只存变化量

18. 压缩率高、存储的时候会压缩、查询的时候会解压缩，所以会需要做权衡

## 30 Flink

1. 事实上的流式数据处理的框架。

2. 在 spark 里、在微观上会把时间轴切成很小的时间窗、然后宏观上是流处理、微观上是批处理。

3. flink 要支持有状态的处理、就是处理后面事件的时候、要知道前面的状态是什么。比如统计一个小时内的数据、就要知道前面的数据是什么。

4. 它可以支持有界或者无界的流式数据

5. 几个概念：streams

6. state

7. time

   - 所有的事件都有一个时间戳。
   - 有一个产生事件的时间、有一个处理的时间。

8. 有 layered api

   - 如果关心业务、可以用高层的 table api
   - 如果需要从流的角度去处理、可以用中间的 datastream api
   - 如果需要更底层的操作、可以用底层的 process function（流是由一堆事件组成的、可以对这些事件进行操作）

9. 先演示了一个最底层的 api、process function，这个例子是说要计算一个事件的开始时间和结束时间之间的时间差，需要在 start 的时候开始记录并等待、然后在 end 的时候计算时间差。如果等待超过了 4h、就会停止等待这个事件。

10. 这个 datastream api 的例子、就是分一段固定的时间时、做 map reduce

11. 状态是怎么保存的、是放在内存或硬盘上、处理的时候会要读这个状态、内存不大的时候、需要存到硬盘上、会影响性能。每个程序只能访问自己的本地的 state、不能访问其他设备上的 state。为了保证一致性、需要做 state 的 snapshot 快照，那么核心问题就是要给哪些东西做快照。

12. flink 在做 word count 的时候、会有两个流程、就是先 input、然后再做 sink（count）。为了保证不发乱，需要把同样的单词发到同样的地方去。所以就根据 hash 发到不同的机器上去。

13. 这个就是 keyed state、就是根据 key 来存储 state。这个 key 是根据 hash 来生成的、所以相同的 key 会发到同一个地方去。这个 counter 必须是有状态的、这个 counter 是 1 还是 2，是有状态的、基于之前的状态来做的。source 发送的时候就需要做指定目标的发送。

14. 为了提高可靠性、需要每隔一段时间做一个 checkpoint，只会丢掉最后一个 checkpoint 之后的数据。记下来之后、已经做过 checkpoinnt 的事件、如果还没来得及处理、可以通过 replay checkpoint 来处理。

15. 这个 checkpoint barrier 是什么呢、也是相当于流里的一个事件。比如一个 operator 会接受有两个流、每个流都会接受到一个 checkpoint barrier，然后这个 operator 会等待两个流都接受到 checkpoint barrier 之后、才会继续往下走。（这里就在做对齐操作了，然后写到硬盘上之后就记录完了）

16. 能不能不对齐也能做呢？也是可以的、就是要把这个栅栏之后还没处理完的内容、也都算在这个 checkpoint 的 state 里面、好处就是没有这个等的过程，性能更高。坏处就是记录的状态会更多。

17. 这个存这个 snapshot 状态的时候用的是 RocksDB、所以新的状态会更快地被拿到、老的数据就沉下去了。

18. 为了做同步、还有用 watermarks 水印来表示的时间。意义表达了、如果这个流是按照顺序来的、那么 watermarks 前面的数据都是小的、watermarks 之后的时间都是大的、那么就可以去除掉这个不符合 watermarks 规则的事件

19. 后面的示意图里、黄色的事件时间就代表当前这个 operator 实际处理过的时间（就是黄色的时间戳只是记录经过的 watermark 的时间、可能会出现虽然处理了 35 的事件、但是还是黄色的 33 时间戳、因为只过去了 33 的 watermark）、白色的就是事件的时间。就是它经过一个 watermark 的时候、代表这个时间点之前的所有事件都已经处理完了、可能处理不是顺序处理、但是都处理了

20. 对于这个例子、如果一个 operator 有两个输入、它还是会受限于那个时间戳慢的输入事件。水位线的目的是就是、因为这个事件可能会来得比较乱、所以要有一个规则来保证这个事件是大致有序的。（就是在这个 watermark 之后不会出现比这个数字更小的时间了、但是在这个 watermark 之前、可能会出现比这个数字更大的数字）

21. 单挑水位线出来、保证了严格有序。但是之间的值可能是乱序（相当于一个上界）

22. 这个时间窗 window 应该如何去确定？

    - 第一种是按照相同的时间段（缺陷：当一个时间段内的数据量很大的时候，会导致一个时间段内的数据量很大，或者当一个时间段内的数据量很小的时候，会导致一个时间段内的数据量很小）
    - 第二种是按照相同的数据量（缺陷：会需要等待时间）

23. 管理和 hadoop、yarn 和 spark、storm 差不多、就是有 worker 和 job manager 管理。每一个 worker 上面有一个 task manager、然后 job manager 会把任务分配到 task manager 上去。（也可以多线程地去跑等多个任务）
