## 17 Log-Structured Database & Vector Database

1.  先讲了需要给 redis 的信息加入一个超时的时间，如果一开始的时候、就把所有书都放入到 redis 里，当 3600s 到了之后，那么这些 book 对象全部失效了，那么大量的访问书的请求就需要重新去数据库里拿。这个情况就是雪崩的情况。如何解决呢、比如说可以把这个 10K 的分成 10 个 1K 的，然后让他们逐个失效。第二个情况是、如果 10w 个请求同时读一个不在 redis 里的数据、不加处理的话、会导致 10w 个全部往数据库里去拿、这个就是击穿的情况。如何解决呢、可以加一个锁，当所有请求都访问一个数据的时候、只有一个请求去数据库里拿。第三个情况是，大量访问数据库中不存在的数据，那么全都会访问数据库，这个就是缓存的穿透现象。如何解决呢，可以在前面做一个过滤、比如用布隆过滤器，可以马上告诉你这个数据不在数据库里。（穿透其实是一种攻击）

2.  日志结构数据库：先前的 mysql、mongo 和 neo4j，都有一个隐含的前提、就是所有的数据访问的次数都是差不多的（体现不出新订单和旧订单访问次数的差异（没法处理热点数据的处理情况）（可以用 partition 做、但是还是需要用 timestamp 做、再根据其他字段做子分区也不方便）），所以就有了日志结构数据库。

3.  日志结构数据库用了 LSM-tree

    - Log 需要提高写入性能
    - Log 以 Append 的模式追加写入，不存在删除和修改
    - 这种结构虽然大大提升了数据的写入能力，却是以牺牲部分读取性能为代价，故此这种结构通常适合于写多读少的场景
    - C0 树（常驻内存）
    - C1-N 树(位于磁盘)

4.  因为要落盘的时间、所以会有 immutable memtable （rockdb 做了优化、这个 immutable table 可能会有多个）

5.  要先写到 WAL log （write ahead log）里面、才能再写到内存里面

6.  优点

    - 大幅度提高插入（修改、删除）性能
    - 空间放大率降低（可能会产生不同版本的数据，一个数据多个版本在里面的话、空间放大率就会变大）
    - 访问新数据更快，适合时序、实时存储（适合存双十一的订单）
    - 数据热度分布和 level 相关（经常不被访问到的数据就会沉到底层）

7.  缺点

    - 牺牲了读性能（一次可能访问多个层）
    - 读、写放大率提升

8.  ppt p6 有 sstable 的定义

9.  会有一个 shared key 的概念、就是 abcd、abcd、abcf 前面共同的前缀 abc 就是 shared key（一个同样的 shared key 会存在一个 record group 里面）

10. 写放大：一直要合并到最后一层的时候，会涉及到很多层的合并

11. L0 是无序的，L1-Ln 是有序的

12. 读放大：比如读一个 key=61 的，但是不存在这个数据库里，如果每一层都找不到、就要一路找到最底层（如果不存在布隆过滤器的时候）

13. 老的数据也会在 compaction 的时候被 remove 掉（所有的 sstable 都不可更改、都是在 compaction 的时候重新写进行处理的）

14. 给出的 rocksdb 的例子是嵌入式执行的

15. 热点数据不是一个大量的数据，只需要尽可能快地访问到热点数据就可以了。如果热点数据不再成为热点数据之后、应该把高层的数据放到其他的数据库里（因为会有读放大和写放大）

16. HTAP Hybrid Transactional/Analytical Processing 混合事务分析处理 （这里提到了数据是应该按行存储还是按列存储）（因为在一个系统里面，下面这两种需求都会有，最好不要只有一种、应该混合，或者也可以按行存一轮、按列存一轮（但是也会有缺点、就是空间的浪费和维护一致性的两个问题））

17. Online Transaction Processing(OLTP) （要对随机的读写做支持，要做索引的维护、随机访问数据，可以按行存）（这里就不能按列存、因为需要在不同列地方存同一个订单的信息）

    - 在线事务处理（订单之类的）

    - 低延迟
    - 高并发
    - 数据量小

18. Online Analytical Processing(OLAP)

    - 在线分析处理（统计之类的）（这个如果按行存储的话、导致每一行的长度不一样、会造成很多的随机访问、所以可以按列存储，这样按顺序读就很快）

    - 高延迟
    - 低并发
    - 大量数据

19. LSM-Tree 抽象结构

    - 分层结构
    - 提供写优化
    - TP 事务友好

20. 这里有一个列族的概念（相当于垂直分区，但是管理起来比较麻烦、但是 LSM-tree 在 rocksDB 里面可以支持（可以共用一个 WAL 日志和 manifest files）

    - 比如说可以对和 sale 相关的信息用列存、然后做 OLAP、放在一个列族里面，用 lsm-tree 来存储列信息
    - 然后其他的数据用 row 存、然后支持 OLTP、存在另一个列族里面，用 lsm-tree 来存储行信息

21. 写阻塞问题降低了 TP 事务的可用性，读放大问题限制了 AP 查询的性能

22. 解决写阻塞问题，在 RocksDB 和数据源之间增加一个收集分发层，写阻塞时缓存数据（collector）（基于 apache flink）

    - 采用中心化设计：
      Master 采用主从备份
      监控集群负载，
      调控负载均衡。

    - 负载均衡策略基于剩余内存大小，
      即分配到某个节点的概率与剩余内存大小正比

23. 解决读放大问题：在 RocksDB 中增加列式存储，列式存储在访问少量列时磁盘读取量更小，可以减少读放大的开销（列式存储也是按照数据块的）

24. 提出混合存储策略

    - 常做事务的数据以行式存储，
    - 常做查询的数据以列式存储

25. 然后讲向量数据库

    - 会有 one-hot 编码（所有的单词的点积为 0，每个单词都无关、但是这个缺点就是需要的维数太大）
    - 也会有一个对一个人的各种特征做向量化的处理（降维，然后可以计算两个向量之间的相似性，最后得到的数字都是数值型的，然后找最相似的时候，就去找和向量最相似的（最 naive 的方法，其实需要做更多预处理））
    - 所以向量数据库就是先存、然后做搜索（范围内的搜索，区别就是不是精确匹配、找的是最相似的，和 mongo 的 2d 的区别就是向量数据库支持更高维的）

26. 数据插入向量数据库之后也会需要做索引

27. 随机投影：为了降低数据的维数

    - 用一个随机的矩阵 random matrix generator，然后把原来的数据乘上这个矩阵，就可以得到一个降维或者升维的数据

28. 建立索引的时候、还可以做一个 product quantization，就是把向量分成几个部分，然后每个部分再做一个聚类（通过 codebook generator 转化成 code，然后查找的时候也把查找的 vector 变成多个 code 进行匹配）（比如就是把 500 种组合压缩成 10 种 code，这样就可以做近似匹配（因为不需要做精确匹配））

29. 还可以做位置敏感哈希 locality sensitive hashing，通过 hash 值决定它在哪个 bucket 里面，然后 query 的时候还是找最近的

30. HNSW（hierarchical navigable small world）就是多层的索引

31. 相似度计算：

    - 余弦相似度 Cosine similarity：It ranges from -1 to 1, where 1 represents identical vectors, 0 represents orthogonal vectors, and -1 represents vectors that are diametrically opposed. $\text{Cosine Similarity}(A, B) = \frac{A \cdot B}{|A| |B|}$

    - Euclidean distance: 
      It ranges from 0 to infinity, where 0 represents identical vectors, and larger values represent increasingly dissimilar vectors.

    - Dot product: 
      It ranges from -∞ to ∞, where a positive value represents vectors that point in the same direction, 0 represents orthogonal vectors, and a negative value represents vectors that point in opposite directions.

32. 向量数据库中的相似度计算方法用于衡量向量之间的相似度，常见的计算方式包括**余弦相似度 (Cosine Similarity)**、**欧几里得距离 (Euclidean Distance)** 和**点积 (Dot Product)**，每种方法采用不同的计算方式。以下是它们的具体计算方式：

### 1. **余弦相似度 (Cosine Similarity)**

余弦相似度衡量的是两个向量在角度上的相似度，忽略它们的大小，仅关注方向。它的计算方式如下：

$
\text{Cosine Similarity}(A, B) = \frac{A \cdot B}{\|A\| \|B\|}
$

其中：

- \(A $\cdot$ B\) 是向量 \(A\) 和 \(B\) 的点积。
- \(\|A\|\) 和 \(\|B\|\) 分别是向量 \(A\) 和 \(B\) 的模（即它们的长度）。

**范围**：余弦相似度的值范围是从 -1 到 1：

- 1 表示两个向量完全相同（方向相同）。
- 0 表示两个向量正交（无关）。
- -1 表示两个向量完全相反（方向相反）。

### 2. **欧几里得距离 (Euclidean Distance)**

欧几里得距离用于计算两个向量之间的直线距离，衡量它们在空间中的"相离"程度。其计算方式为：

$
\text{Euclidean Distance}(A, B) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}
$

其中：

- \($A_i$\) 和 \($B_i$\) 是向量 \($A$\) 和 \($B$\) 在第 \(i\) 个维度上的分量。

**范围**：欧几里得距离的值从 0 到正无穷大：

- 0 表示两个向量完全相同。
- 越大的值表示向量之间的差异越大。

### 3. **点积 (Dot Product)**

点积是一种简单的相似度度量，表示两个向量在某种程度上的"投影"关系。其计算方式为：

$\text{Dot Product}(A, B) = \sum_{i=1}^{n} A_i B_i$

其中：

- \($A_i$\) 和 \($B_i$\) 是向量 \($A$\) 和 \($B$\) 在第 \($i$\) 个维度上的分量。

**范围**：点积的值范围是从负无穷大到正无穷大：

- 正值表示两个向量的方向相似（指向相同或相似方向）。
- 0 表示两个向量正交（方向无关）。
- 负值表示两个向量方向相反。

### 总结：

- **余弦相似度**：衡量向量之间的角度差异，范围从 -1 到 1。
- **欧几里得距离**：衡量向量之间的直线距离，范围从 0 到正无穷。
- **点积**：衡量向量在同一方向上的投影大小，范围从负无穷大到正无穷。

不同的相似度计算方法适用于不同的应用场景，例如，余弦相似度适用于文本数据中的向量相似性比较，欧几里得距离常用于几何计算，而点积则常用于机器学习中的特征向量相似度计算。

32. 也可以通过向量的元数据进行过滤，然后再进行匹配

## 18 Timeseries Database

1.  云数据库走 serverless 的模式

2.  时序数据库的特点

    1. 时序数据库里的数据是有一定的存活时间的，比如一个小时之内电脑的状态

    2. 数据的格式简单，数值在一定范围内波动

    3. 所以不一定需要存原值、可以存差值（difference），好处是当数值很大的时候、差值会比较少、可以节省空间

    4. 所有的数据都带 timestamp（和 sstable 一样，存的时候可以带一个前缀，比如前几个 time 是一样的、就可以合并）（也可以存差值）

3.  时序数据库的数据叫做度量，是通过监控和下采样得到的

    - 特点是数据会源源不断地来
    - 需要做数据的生命周期管理
    - 关心的不是单点的数据、而是一段时间的总结性的数据（比如平均利用率，总和、标准差）
    - 所以也需要扫描一大堆的数据（经常做的操作）

4.  serverless 的会涉及到冷启动的 docker 的问题、一开始的体验不会很好

5.  influxdb 用 telegraf 来采集数据

    - flux 是类似于 sql 的查询语言

6.  课上监控了 cpu 的状态

7.  bucket 相当于数据库表的名字，但是没有库的概念

    - \_time 是时间戳
    - \_measurement 是一个 bucket 里面，在度量什么
    - \_field 是字段的名字，key
    - \_value 是值
    - 然后也会有 tag key 和 tag value，用来表示后面的值的特征

8.  时序数据库的索引

    - \_field 是不参与索引的，因为只能在 value 上做索引、但是没有意义，这个是搜索的结果，而且取值不是唯一的，所以建索引没有意义
    - \_tag 是参与索引的，因为我们经常在 tag 这一列上去做搜索和筛选，而且 tag 的值是固定的，所以建索引有意义（写时有额外开销、但是搜索快）
    - 所以其实 tag-field 之间在设计的时候是可以互相转化的，如果操作上不需要做全表的 scan、可以在根据搜索过滤的条件、把对应的数据转化成 tag，然后再去做搜索

9.  series

    - 有时候查询出来的数据是一系列的，这个就是 series
    - A series key is a collection of points that share a measurement, tag set, and field key

10. point：A point includes the series key, a field value, and a timestamp.

11. bucket：A bucket combines the concept of a database and a retention period (the duration of time that each data point persists). A bucket belongs to an organization.

    - 超过一定时间会删掉数据

12. organization：An InfluxDB organization is a workspace for a group of users. All dashboards, tasks, buckets, and users belong to an organization.

13. InfluxDB design principle

    1. 为了提升性能、这个数据是按照时间升序排列的、所以是可以 append 来做插入的
    2. 数据是一次性写入、不允许再修改（而且不存在新版本的数据、只存在新数据，每个数据时间戳都是不一样的）（这个不能修改和日志合并树很类似）
    3. 因为有可能会在做 query 的时候、不断地有数据进来、Therefore, if the ingest rate is high (multiple writes per ms), query results may not include the most recent data.
    4. Schemaless design：Time series data are often ephemeral, meaning the data appears for a few hours and then goes away.For example, a new host that gets started and reports for a while and then gets shut down.
    5. 没有 ID（和关系型数据库不同）Points are differentiated by timestamp and series, so don’t have IDs in the traditional sense.
    6. 对于 Duplicate data，InfluxDB assumes data sent multiple times is duplicate data. Identical points aren’t stored twice.（好处就是记录的数据会进一步减少）

14. The storage engine includes the following components:

    - Write Ahead Log (WAL)
    - Cache
    - Time-Structed Merge Tree (TSM)
    - Time Series Index (TSI)

15. Writing data from API to disk

    - 数据点如果非常多地进来、会需要先进行压缩，否则 WAL 会很快地变得很大
    - points 写进来会先写到 cache 里面
    - 然后 cache 会周期性地通过 TSM 文件写到磁盘里面
    - （和 LSM 很像） As TSM files accumulate, the storage engine combines and compacts accumulated them into higher level TSM files.

16. cache

    - 通过组织一组值的 key (measurement, tag set, and unique field) （series）

17. TSM

    - Column-oriented storage lets the engine read by series key and omit extraneous data.
    - 用的是列存、因为经常要做 scan、所以用列存会更快
    - 也分为 shared key 和 non-shared key

18. TSI：time series index

    - 基于 series key 做的索引
    - The TSI stores series keys grouped by measurement, tag, and field

19. binlog 不应该和 MySQL 放在一起、因为如果遇到的是硬盘损坏、那么数据和 binlog 都会丢失、所以应该放在不同的地方

20. InfluxDB file structure

    - blotdb 用来存小的用户数据
    - Configs path 用来存 token 之类的内容

21. InfluxDB shards and shard groups

    - 大量的数据进来、存不下、就用 shard
    - A shard contains encoded and compressed time series data for a given time range defined by the shard group duration.（压缩过的数据）
    - 会根据不同的时间段来存不同的 Shard group （设置 duration）
    - 只有这个时序数据库会自动帮你删除、其他种类的数据库都需要自己去删除（主要是对于原始的数据点不太关心、关心的是一段时间的总结性数据）

22. shard life circle

    - 可以预先做 Shard precreation，然后之后写的时候就会快一点
    - InfluxDB writes time series data to un-compacted or “hot” shards. When a shard is no longer actively written to, InfluxDB compacts shard data, resulting in a “cold” shard.
    - Shard compaction（Level 1 (L1)（在内存里）: InfluxDB flushes all newly written data held in an in-memory cache to disk. Level 2 (L2): InfluxDB compacts up to eight L1-compacted files into one or more L2 files ）
    - 也可以设置 retention forever、然后就永远不会删除数据

23. 这里的代码例子上课的时候没听、复习的时候可以听一下

## 19 GaussDB

1. 不是自己写的、是基于 postgresql 等数据库的，主要是为了做分布式（比如分布式里会有 2PC 之类的）

2. GaussDB 技术架构

   - 分布式优化

   1. 分布式近数计算：数据的计算的位置应该尽量靠近数据节点的位置，效率能够提高

   2. 全链路编译执行：类似于 prepared statement，可以提前编译好、然后把参数传进去再执行、这样可以提高效率（用的是 JIT）

   3. 大规模并发事务处理

   4. sql 解析：类似于编译器的优化（query plan），

   5. RBO：比如可以基于一些数据库的规则、把一些查询优化成更好的查询（基于规则的优化）根据预定义的启发式规则对 SQL 语句进行优化（比如贪心算法，有一个全局最优解、但是算不出来）（还有一个 meta-heristic，可以跳出局部最优解找出全局最优解）

   6. CBO：基于查询的代价来做优化

   7. Hint：辅助优化器的工具

   8. postgres 没有多线程、gaussdb 做了线程池

   9. 向量化：提高多核效率（充分利用多核的效率、比如处理 tensor 的时候，一次一批做（batch））

   - 多地容灾

   1. 多地多活容灾（两地三中心，另一个城市用于容灾）

   - 云原生弹性伸缩

   1. 纵向：增加进程可用的内存、cpu

   2. 横向：增加计算节点服务器

   3. 多租户（重要）

   - 智能优化

   1. ABO：AI based optimization（基于怎么样的 ai 模型能做出更好的优化）

   - 安全隐私

   1. 防篡改：比如用了区块链技术

3. coordinator node 用来协调（比如分布式事务），data node 用来存数据和执行计算

4. 优化器优化出来的查询可能对某些数据库表现好、有些数据库表现不好

5. 会有缓存层和存储层之间的 RDMA（Remote Direct Memory Access）（不需要 CPU 的参与、直接在内存和硬盘之间传输数据）

6. 多租户：主要问题在于维护后期的差异、比如不同的租户会有不同的数据表的要求，有四个方案

   - 可以用两个分别的小的数据库
   - 可以用一个大的数据库，然后在一个大数据库里面维护两张表
   - 可以在一张表里加一个字段是租户 id，然后在查询的时候加一个条件，然后增加修改字段的时候可以用外键关联（相同的列存在一张表里）
   - 可以把列转成行去做，这样就可以维护多租户（主键+列名+取值），缺点是这张表可能会很大
   - GaussDB 实现了多租户、而且可以选择方式（但是不同的方式代价不一样）
   - 可能形式上有 6 个数据库、但是实际上只有 5 个
   - 相同的数据库最好只有一个实例

7. 查询重写中的谓词下推：

   - 有两个 data node、一种方法是通过把数据表上推到 coordinator node 做计算，另一种方法是把计算下推到 data node 上去做（比如把 t1 和 t2 表的连接操作推到 data node 上去做）

8. GaussDB 计划生成：GaussDB 主要采用自底向上与随机搜索相结合的方式

9. 全局计划缓存：每个 session 可以复用全局其他 session 已经执行过的缓存、提高效率

10. 页面结构：行存储以页面为单位

    - 因为可能会有 varchar 这样每个元素不一样的字段、所以需要 pointer 的槽位去指向这些元组的位置
    - 元组信息前面四个比较重要，需要做 MVCC（因为每个元组的版本号是不一样的）
    - xmin 和 xmax 决定了元组的生命期，亦即该版本的可见性窗口。（ppt 上有一个表格）

11. ppt 例子是在同一个事务里做两次更新，同一个事务里对一个记录改两次、就会改动 t_cid。而 t_ctid 指向的是最新的记录，页面中的第几个元组，有新的变动的时候就会更新这个值。

    - 保存历史数据：为了做回滚和做快照

12. CSNLog：记录 XID 与 CSN 的映射关系，为每个事务生成一个唯一递增的 CSN，用于将事务与其可见性进行关联

    - Clog：记录事务 ID 的运行状态：运行中/提交/回滚

    - 当事务结束后，使用 CLOG 记录是否提交，使用 CSNLOG 记录该事务提交的序列

13. 有一个 Ring Buffer？

14. 列存里面可以用 min 和 max 做 index

15. 预分片：预先分配好 shard，然后在写入的时候就不用再去分配 shard 了

16. 热备：请求同时发到两个节点上，然后一个节点挂了，另一个节点可以继续工作，用户无感知（能实现 RPO=0，但是是有资源的浪费的代价）

17. 暖备：请求只发到一个节点上，然后另一个节点挂了，用户会感知到（fail 了，需要重发）

18. 定期切换冷备中的主从节点：为了知道这个冷备的节点是不是崩了

19. 讲了一个 sql 注入的例子和一个跨站脚本攻击的例子

## 20 Data Lake

1. 如果有很多数据分布在不同种类的数据库中、需要抽取到一个统一的地方然后，访问的时候对这个地方做 OLAP。但是会有很多 ETL 的操作（抽取、转换、加载）

2. 所以数据湖就是为了解决上面的问题，直接存储 raw data（各种不同的数据源的不同格式的数据）

3. 所以数据湖需要很强的数据接入能力（通过 flink 工具）

4. 可以通过 metadata 来发现需要操作哪些类的数据库，然后通过 calcite 来做针对不同的查询语言的转换

5. 数据湖和数据仓库有不同功能和特点的对比

6. filter 需要进行实时的流式处理（比如推文的立即过滤）、原来的 hdfs （批处理）对于这种实时处理支持不太好，因此产生了 lambda 架构（batch 和 stream 两种方式）

7. 也可以控制流计算的时间窗口、比如 5s 一个窗口，然后统一处理（流批一体）

8. 更新的时候不做 ETL、萃取、沉淀的时候做 ETL

9. datalake 的意义还在于可以支撑更多的数据分析、比如机器学习、数据挖掘，而不仅仅是直接做完 etl 之后进入 data warehouse 的数据分析

10. incremental ETL：按需去做对应数据的 ETL，而不是全部的数据都做 ETL

11. 边缘计算就是基站里的这些服务器里也有 cpu 和资源，可以利用这些资源做一些计算（但是计算能力有限）

12. 比如一辆车开、它会不断地把数据发到相邻的边缘服务器里面（因为总是在连接到边缘节点里，压缩后传输可以减少 edge 到 cloud 的带宽占用），然后定期地从边缘节点同步数据到云中心节点里面（所以有些 query 既需要在中心 cloud server 里面找、也要在 edge server 里面找）

13. 边缘存储架构比较简单、是 tag 或者是 key-value、因为计算资源有限

14. 边缘节点会不断地发数据流元数据到云数据中心，然后之后的查询就会知道在哪些边缘节点去查找了

15. Rosetta 过滤器：多层 bloom 过滤器，减少假阳性

16. 这里的下推是指把计算下推到边缘节点去做、比如先在边缘节点做过滤和聚合（比如做 group by），然后再把数据传到云中心节点去做 union 和 aggregate（这里再做 count）

    - 好处就是原来是把整表拿上来、造成数据传输很多，现在是过滤了一下，所以数据传输就会少
    - 然后还有一个好处就是利用了节点的计算能力，减少了云中心节点的压力，整体性能也会提高

17. 核心问题两个：

    - 定位数据
    - 并行处理

## 21 Clustering

1. 集群的目的就是提高系统的可靠性和可扩展性

2. 负载均衡的至少 3 种不同策略

   - 轮询策略：依次选择一个节点
   - 选择最少连接的节点
   - hash 策略：根据请求的 ip 地址或者请求的 url 去 hash，然后把请求发到对应的节点上去

3. session stickiness 会话粘滞性问题：比如用户登录之后、会话会一直保持在一个节点上，这样就会造成负载不均衡

   - 可以用 ip-hash 的方法，把同一个 ip 的请求都发到同一个节点上去

4. Request-level failover 和 Session failover

   - 因为服务器会崩、需要重做（容错就是重做）（所以操作需要幂等性）
   - 一种方法是广播
   - 还有一种方法是专门的节点是设置一个 session 服务器（缺点是这个服务器就变成了单一故障节点）（好处是其他节点都变成了无状态的节点）

5. 正向代理：相当于服务器视所有使用代理的客户端请求为同一个客户端的请求

6. 反向代理：相当于一个 load balancer、把请求分发到不同的服务器上去

7. 还可以加 Weighted load balancing

8. mysql 的 innoDB 也可以配置集群（mysql shell + router）

   - 使用的时候、是前端连接到 router
   - 就可以做到写请求只写一个、然后读请求会做负载均衡

9. 12 月 9 日这里还讲了一个配置 mysql 集群的操作，不知道在干什么，可以再回看一下

10. 做 configurationInstance 的时候、几个实例需要完全一样（mysql 版本、配置、数据），而且需要至少 3 个实例

11. 返回结果的时候、是通过 nginx 进行返回的，还是不通过 nginx 进行返回的？（如果是，还能不能那么高效？可以通过 nginx 集群增加数量）

    - 是经过 nginx 统一中转再返回到前端的，客户端访问和看到的是 Nginx 而不是 实际 server(自己看一下 HTTP Header 中的 Server 头就能确认，确实是 nginx)
    - 屏蔽了后端的真实 IP，增加了安全性，也避免了差异性
    - nginx 做了优化
    - Nginx 的非阻塞事件驱动模型：Nginx 采用的是基于事件驱动的非阻塞模型（即单线程异步 I/O），能够高效地处理大量并发请求。这使得 Nginx 即使在高并发环境下也能保持高效的响应速度。
    - Nginx 内存效率高：Nginx 在处理请求时，使用的是少量的内存，并且它的处理模型使得它可以在不占用过多资源的情况下，处理大量的并发连接。

## 22 Cloud Computing

1. 网格计算：每个人可以把自己的计算资源共享，然后使用计算资源的时候不用在意是谁的计算资源

2. 云计算的特点

   - 弹性伸缩
   - 按需付费
   - 服务化
   - 无限扩展性
   - 快速虚拟化

3. 也会涉及到多租户的情况

   - 最简单的相当于在门户网站上定制自己的 ui，但是本质上是一个服务
   - 然后云服务把所有软件和硬件都提供为服务

4. 会有三种情况：

   - long-term
   - short-term
   - onspot（拍卖）：灵活定价
   - 可以根据需求去选择不同的云服务商（课上的曲线）以降低成本

5. 垂直扩展和水平扩展

   - 垂直扩展就是增加单个节点的资源
   - 水平扩展就是增加节点的数量
   - 云提供商应该都支持这些操作

6. google 提出了 mapreduce

   - 使得云上的操作系统可以进行 job scheduling
   - 首先输入
   - map phase：把原文映射成了很多个键值对（然后每次一个 worker 会处理一个 split）
   - 然后存在了一个 intermediate file 里面（等到所有 split 都被处理完之后、就会进行 reduce）
   - reduce phase：把相同 key 的 value 进行合并（可以会有不同的区间范围规则。比如 A-N 由 worker1 处理，O-Z 由 worker2 处理）
   - 然后输出
   - 这个是一个批式处理的过程

7. 然后需要一个分布式的文件系统 DFS，谷歌提出了 GFS

   - 有一个 master 节点，然后有很多 chunk server
   - master 节点负责管理 chunk server，chunk server 负责存储 chunk
   - master 节点会记录 chunk 的 metadata
   - 有一个 shadow master 节点，用来备份 master 节点的信息
   - 有一个 chunk server 会有很多个 chunk，但是一个 chunk 只会存在于一个 chunk server 上
   - 返回的是 handler

8. 文件系统在多个用户使用的时候、需要考虑并发问题

   - 并发控制
   - 还需要应对错误
   - 分离的服务器越多、可靠性越差
   - 需要使用 Write Control and Data Flow 增加副本，提高可靠性（解决一致性问题）
   - 增加 replica 的数量、可以提高系统的可用性，但是会增加系统的负担（所以写操作的时候性能会下降）（如果不一致、这个写操作就失败）（也是必须要做的）

9. 数据存储的时候需要用到 bigtable

   - 如果用传统的关系型数据库、会有很多的外键关联和检查（而且会需要很多不同机器上的 join 操作和不同进程间通信）
   - 如果横着存、很多时候会实现不了（如果采用切开等价类，采用语义等价的情况）
   - bigtable 就把很多表之间的关系关联都切除、就可以把数据存储在不同的机器上
   - 可以通过再加一列提高它的表现性（比如把几个列合并加成列族，然后再动态加一列新的，允许字段为空，所以对结构化的要更灵活）

10. 所有 table 和 tablet 都可以进行切分

    - 然后数据库的入口就在了 root tablet
    - chubby file 能够维护一个锁，保证并行安全

11. OS：

    - FS：GFS
    - job scheduling：mapreduce
    - DB：Bigtable
    - mem（内存管理）：这个就没有统一架构了、每个系统有自己的实现（可以像 gaussdb 一样看作是一个大的内存整体）

12. hadoop 就是上面这些内容的开源实现

13. 云原生：cloud native

    - microservice -> serverless
    - VM -> container
    - CI/CD（持续开发、持续集成、持续部署，增量式开发）

14. 边缘计算

    - 比如有很多基站，里面其实也是有服务器的
    - 边缘设备和云不能持续性连接
    - 所以需要节省带宽，可以间歇性地把数据发到最近的基站上，然后选择性地把数据发到云上去

15. 边缘计算的实际场景（云边端融合的场景）：

    - 比如说视频分析，会在摄像头里直接使用计算资源进行计算
    - 如果超过了摄像头的处理能力、那么就发到云里去做处理（动态变化的）
    - 或者说智能家居（边缘操作系统实现互联）
    - 或者说如果实现手机的 ai 计算、可以把一些计算放到附近的 MEC（Mobile Edge Computing）上去做
    - 然后逐层地动态决定在哪一层上进行处理

16. 不同的模式名称

    - Local Execution （全部本地处理）
    - Full offloading （全部发送）
    - Partial offloading （部分本地处理，部分发送）

17. 下面讲了 graphql

    - 能不能对客户端更友好，客户端可以根据查询的语句只获取自己需要的数据、而不是整个数据，比如搜索 id 返回 id
    - graphql 在后端直接根据前端的 query 进行组装，然后返回结果给前端（和 restful 不同）

18. 也需要写 resource 里的 schema

## 23 docker

1. 一开始讲了一个例子、就是在一个表上如果有一个复合主键（a 和 b）、会自动地先在这个复合主键上面做索引，然后再新建了一个 b 和 c 两个键的索引（默认都是升序的）

   - 查询的时候、如果在最前面加一个 explain、会解释这个查询是怎么执行的
   - 比如说如果做 EXPLAIN SELECT \* FROM table WHERE a = 1 AND b = 2 AND c > 1 ，会返回 possible_keys 是这两个都可以，就是 PRIMARY 和 b_c，然后这里会先用 PRIMARY（然后 key_len 是 8，因为是两个 int，a 和 b，所以是 4+4=8）
   - EXPLAIN SELECT \* FROM table WHERE a = 1 AND b > 2 AND c < 1 也用 PRIMARY , key_len 是 8
   - EXPLAIN SELECT \* FROM table WHERE b = 2 AND c > 1 也用 b_c（先用 b 搜然后搜出来之后再用 c 去筛选, key_len 是 8）
   - 但是有一个情况不一样、就是 EXPLAIN SELECT \* FROM table WHERE b > 2 AND c > 1， key_len 是 4，虽然仍然是在用这个索引，但是只用了 b 这个索引，没有用到 c 这个索引因为相当于是在 b=8 和 b=9 上、分别有不同的关于 c 的记录，所以找到对应的大于的 b 之后、需要进一步地在子树 c 里面去做遍历、而且它的 c 的值可能不连续
   - 所以在范围查找的时候、只有在第一个碰到的时候才会用到这个索引、之后就不会用到了，会在这个索引上做遍历 scan （b > 2 的时候、就会用到这个索引，但是 c > 1 的时候就会遍历）
   - 所以设计索引的时候需要考虑到很多是不是会有范围查找的情况，因为碰到第一个范围查找之后、这个索引就不会再被使用了

2. image 就是对整个运行环境进行了打包。docker 是在一个 linux 的 os 环境的上层进行打包运行的

   - 分层的目的是什么、比如我们用 spring boot 开发一个项目、别人用 c#开发、其中有一个共同的层、那么这个共同的层就可以抽取出来做复用。（pull 的时候可以节省空间）
   - 就是不同的容器可能会共享相同的层
   - 用的是 cgroup 和 namespace 将进程隔离开
   - 通过 cgroup 保证不同容器的文件系统是隔离开的
   - 虚拟机是每一个虚拟机都有一个独立的操作系统，而容器是共享一个操作系统

3. dockerfile 主要负责怎么打包这个 image

4. 能不能不要每次修改原始文件的时候都重新打包这个镜像呢？

   - 也可以在环境中动态添加文件
   - 可以用 volume，把本地的文件映射到容器里面去
   - 这个卷的名称好像也可以指定固定的路径+名称、而不用每次都用路径
   - 有差别、一个是容器虚拟机内部管理、还有一个是本地管理（挂载）

5. 下面的每一行代码的意思是

```shell
$ docker run -dp 127.0.0.1:3000:3000 \
    -w /app --mount type=bind,src="$(pwd)",target=/app \
    node:18-alpine \
    sh -c "yarn install && yarn run dev"
```

    - -d 是后台运行
    - -p 是端口映射
    - -w 是工作目录
    - --mount 是挂载,这里挂载了当前目录到 /app
    - node:18-alpine 是镜像

6. 容器间要能互相通信、需要加入共同的网络

7. docker compose 一次起多个容器

## 24 Hadoop

1. 一个操作系统需要

   - 文件系统 GFS
   - job scheduling mapreduce
   - memory management 各个不一样
   - database bigtable
   - hadoop 包含了上面的所有

2. 在配置文件里面、一定要有下面的内容，指定了 hdfs 的地址和端口

```xml
etc/hadoop/core-site.xml:
     <configuration>
         <property>
             <name>fs.defaultFS</name>
             <value>hdfs://localhost:9000</value>
         </property>
      </configuration>
```

3. 下面配置副本数

```xml
etc/hadoop/hdfs-site.xml:
     <configuration>
         <property>
             <name>dfs.replication</name>
             <value>1</value>
         </property>
     </configuration>
```

3. hdfs 是基于 linux fs 上面运行的、而不是纯基于底层硬件的

4. 文件分区表存在下面的 namenode 里面

```shell
Format the filesystem:
$ bin/hdfs namenode -format
```

5. namenode 是客户端每次访问的必经之路

   - 存储了每个 file 的所有 block 底层上存在了 linuxFS 中的哪里

6. 可以在 localhost:9870 查看 namenode 的状态

7. 实际的运行流程就是先启动 hdfs 集群、然后在上面指定地运行 mapreduce 操作、比如丢一个 jar 包，然后等待结果

8. 和云计算那一节和 cse 中提到的差不多

9. 例子里的时候、是会对两个 string 1 合并成 string [1,1]，然后统计总数

10. 第一次做 reduce、会在 mapper worker 结束之后的本地去做（这次操作叫做 combine，把本地的次数合并），第二次做 reduce 是在 reduce worker 里面去做（是对多个本地合并后的结果再去做 reduce）

11. 然后这个 intermediate file、图里有两列、那么第一列永远是 reducer 1 的 partition、第二列是 reducer 2 的 partition

12. 需要进行 shuffle（就是 sort 排序），然后再进行合并（为了减少随机 io）

13. combine 和 reduce 可以用不同的函数、也可以用相同的函数

14. 还有一个例子是找不同年份中温度的最大值

15. combiner 也可以让在网络上传递的数据变少

16. 有些时候也不需要 combiner、比如传输的数据不多的时候

17. 但是有时候不能用 combiner、比如求平均值的时候、除非每个本地的数目都相同

18. 所以要具体问题具体分析

19. 导入这个库后、貌似这个进程会跑一个 hadoop 集群

20. 一种常见的情况是、一个 outputfile 包含了 A-N 的、还有一个包含了 O-Z 的、那么就需要两个 reduce worker 去处理，最后存到不同的 replica 里面

21. 也可以没有 reduce、做完 map 之后就存

22. 首先是 job client 根据程序生成一个 job、然后给 jobtracker

23. jobtracker 会做资源的管理、他会记录每个机器的状态和任务、然后记录心跳包、然后如果发现某个节点挂了、那么就会重新分配任务到节点

24. jobtracker 会是一个瓶颈

25. 给定一个时间段里、会需要接收到固定数量的心跳、不然就会认为这个节点挂了（因为 job 的时长可能会不同）

26. reducer 的结果可能不是有序的

27. 这个排序的比较器也可以自己指定

28. 然后讲了一个问题、就是因为每个 split 可能也不是足够小的、所以每个 mapper 在做 map 的过程中、如果内存满了、会把内容以 spill 的形式写到硬盘上（overflow 了）

    - mapReduce 之中会使用大量的磁盘 IO、在所有的步骤中都会涉及到（都是在磁盘中实现的）
    - 而且会涉及到可靠性的问题、如果机器崩了、那么前面的任务就白做了
    - 可以做一个优化、比如直接把 spill 放到待 reduce 的队列中

29. 由于 job tracker 是把资源管理和 job scheduling 都放在一起、所以可能会有瓶颈

30. yarn 就是 mapreduce 的升级版，每个应用都有一个 applicationmaster（每一个 job 都有一个 tracker 相当于），然后有一个全局的 resource manager

31. 相当于管理一个 job 的所有容器的状态都丢给了 application master，然后请求新资源的时候会通过向给 resource manager 去请求，这就实现了解耦、职责分离了，然后一个 job 崩了、另外的 job 还可以正常进行

32. 这样就没有了性能瓶颈和单一故障节点

33. 但是都是对用户无感知的

## 25 Spark

1. 也是对大规模的数据做分析、但是是放在内存里做的

2. hadoop 会涉及到很多 IO、所以性能比较差

3. 例子的目的是在一个很大的 log 中、过滤出来一个小部分的报错的信息

4. 这个 lines 就是 RDD 的一个实例、一旦读到了内存里之后这个值就不能被修改了、这也意味着这个值可以被复用（多线程的竞争就可以免除掉）

5. filter 可以使得一个 RDD lines 变成另一个 RDD errors

6. 同样地，map 也是一样的、可以把一个 RDD 变成另一个 RDD

7. 为什么要 cache 住呢，因为内存可能会满，所以需要涉及 LRU，除了 LRU、还有 swap、就是把内存里的内容写到硬盘上。但是 cache 住了之后、这个值就不会被释放掉了（无论做 LRU 还是 swap 都不会被释放，一直在内存里，但是这只是一个愿望，如果全都满了，还是会进行替换）

8. 然后其实本质上等到 cache 这行的时候、才会反推执行前面这三步一起执行、内存里会先将它们进入计算图再执行。（通过 stage 执行，惰性）（所以抛错误是在 cache 这一行才报的错）

9. 同样地直到执行.count 的时候才会执行前面的.filter。这些操作叫做 action（比如.count，.cache），才会反推进行前面的计算（前面的叫做transformation）。然后这个 filter 和 count 都是在多个节点上进行的、然后再合并成一个的。

10. spark 也是分为 worker 和 manager 的

11. 先起一个 spark 集群、然后使用 spark-submit 提交一个脚本、然后就能执行任务了。

12. 这里需要用spark submit exe去运行一个jar包才能运行对应的java代码

13. scala也可以变成一个jar包去运行

14. RDD是一个弹性分布式数据集（Resilient Distributed Dataset），可以并行地去操作。也可以把内存里不是RDD的内容通过sparkcontent变成RDD

15. RDD一般load之后都是分partition的

16. 所有transformation操作都是lazy的。比如上面的这些filter操作

17. 中间可以看一下回放

18. 这个An Easy Example的例子、当没有partition的时候。如果要生成一个结果、那么这个join就需要读所有的userDate和所有的events

19. 如果已经用hash的方式把userData做了分区、那么久可以每个join对应一个userData的块去做操作

20. 窄依赖就意味着性能会比较高、宽依赖就意味着性能会比较差

21. Narrow Dependencies VS. Wide Dependencies

    - 宽依赖往往意味着Shuffle操作，可能涉及多个节点的数据传输
    - 当RDD分区丢失时，Spark会对数据进行重算
    - 窄依赖只需计算丢失RDD的父分区，不同节点间可以并行计算，能更有效地进行节点的恢复
    - 宽依赖中，重算的子RDD分区往往来源自多个父RDD分区，其中只有一部分数据用于恢复，造成了不必要的冗余，甚至需要整体重新计算

22. 下面这个例子是只有当发生宽依赖的时候、才进行计算、然后再进行前面还没有进行的计算

    - 好处是可以节省很多中间结果的RDD、可以节省内存，而且可以并行化
    - 而且每个stage内部的计算会很快、因为可能这个数据和计算发生在同一台机器上（lazy）
    - 就是一个stage结束之后、才回推做stage前面还没做的很多操作
    - 每个阶段stage内部尽可能多地包含一组具有窄依赖关系的transformations操作，以便将它们流水线并行化（pipeline）
    - 边界有两种情况：一是宽依赖上的Shuffle操作；二是已缓存分区

23. 复习的时候感觉也可以重听一下、这里貌似有点绕

24. cache也有不同的等级、比如MEMORY_ONLY、MEMORY_AND_DISK、MEMORY_AND_DISK_SER、DISK_ONLY

25. dataframe是有列名的数据、结构化更好，可以用语句去查询，也可以用sql语句去查询

26. 微观上是批处理、宏观上是流处理。这个例子是处理增量进入的数据、进行数据的统计。（就是spark streaming）（可以通过kafka里读出来）

27. 也可以做机器学习和图计算（把顶点和边分别放在两个RDD里）（不是最优选择）

28. spark可以做流批一体的计算

## 26 Storm

1. 这个Storm就是专门基于流数据的处理做的框架

2. 处理的中间结果叫做Bolt（后面的都是bolt）

3. 产生数据的源是Spout

4. 一样是在微观上是批处理、宏观上是流处理

5. 同样也需要集群管理、默认用的是zookeeper

6. 一个worker可以有多个slots、就是多个进程用于运行多个不同的任务

7. 在storm上会运行topology，但是这个任务是不会终止的（和mapreduce不同）

8. nimbus管理机器节点的重启和工作等

9. 执行的时候和spark类似、需要通过storm命令运行一个jar包

10. 这里的例子里到底是实例数还是其他的？

11. 后面的zookeeper都没怎么再讲

## 27 hdfs

1. 适合Very large files “Very large” in this context means files that are hundreds of megabytes, gigabytes, or terabytes in size. 因为存的block大 

2. Hadoop doesn’t require expensive, highly reliable hardware to run on.所以不适合并行写

3. write-once, read-many-times pattern， the most efficient data processing pattern 

4. HDFS is not a good fit when:
    - Low-latency data access.  Remember, HDFS is optimized for delivering a  high  throughput  of  data,  and  this  may  be  at  the  expense  of  latency. 
    - Lots of small files . Since the namenode holds file system metadata in memory, the limit to the number of files in a file system is governed by the amount of memory on the namenode. （节省内存空间）
    - Multiple writers, arbitrary file modifications

5. 所以比如说存很多视频的时候、就适合用HDFS，因为视频文件很大，而且不会经常修改

6. metadata存在namenode里面

7. datanode存储实际的数据

8. 所有的数据都不经过namenode、只有metadata经过namenode（减少瓶颈）

9. 副本数为3的话、可能在一个机架上放两个副本、一个在另一个机架上

10. 写操作是流水线式地写入每个replica，写完每个之后才完成

11. 集群起：先起namenode、然后datanode、datanode会给namenode发心跳包，然后就能加入集群、可扩展性就会变强、不需要重启

12. 通过发blocks report告诉namenode自己有哪些block，然后要保证每个block都有3个replica，然后每台机器上的block数量都要均衡（？）A Blockreport contains a list of all blocks on a DataNode.（因为The DataNode has no knowledge about HDFS files. ）

13. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. 

14. namenode专门用一台机器跑，和datanode分离、因为每次都是从namenode读取metadata，所以要保证namenode的高可用性，要干净

15. 一个机器上跑多个datanode没有意义（因为两个datanode可能会在同一台机器上的时候、做冗余就没有意义了）

16. 可以创建目录、不支持硬/软链接

17. have strictly one writer at any time. （不能并行地写多个replica，否则不能保证数据一致性）

18. 会有一个rack-aware replica placement policy 机架感知策略，但是会减少性能（具体是根据网络延迟的距离来判别的，距离越大、写入的代价越大）（示意图）

19. (replicas - 1) / racks + 2公式保证了每个rack上的block数量都是均衡的

20. 多个副本的时候、读操作可以做负载均衡

21. On startup, the NameNode enters a special state called Safemode. （在等datanode给它发心跳包，因为没有datanode的时候，它是不能工作的，所以处于安全模式）

22. EditLog记录了所有操作，写完日志才会做文件操作

23. 这个日志文件不是hdfs的一部分、而是本地文件系统的一部分

24. 同样地、fsimage也是本地文件系统的一部分，因为是启动hdfs必须的文件

25. 启动的时候也需要做对齐，有些editlog里的操作还没写入fsimage里面。

26. 这个会有一个checkpoint、可以检查fsimage和editlog的一致性。也可以设置checkpoint的时间间隔，但是不能太大也不能太小，太小会影响性能，太大会影响空间开销

27. 如果一个收不到一个datanode的心跳包，那么就不会再往这个datanode上做io了、而且会给其他datanode发需要产生其他replica的请求

28. 如果是网络问题、也是不可用的

29. 如果存的数据太多、就会需要做数据迁移 Cluster Rebalancing

30. 会对所有的生成的文件生成一个校验码checksum，保证Data Integrity（不被篡改）

31. FsImage and the EditLog 也有多个副本、保证能够启动hdfs，而且这两个文件的副本必须要实时同步

32. 数据块大小是128MB，写一次多次读

33. 如果没有ack的话、就是会相当于没有写成功。但是还是有一定优化、比如如果就差1MB没写的话、还是会有快照之类的机制让它不要重试的时候重复再写一次

## 28 HBase

1. 在云计算那讲中的Bigtable的hadoop中的实现

2. 本身是一个大表、可以切开用hdfs存在不同的机器上（分布式）

3. 通过root找到要找的表的metadata、然后根据metadata找到对应的tablet

3. 像一个立方体的数据？

4. 它是按列存的

5. mysql不做垂直分区、是因为mysql默认按行存、而且每个切开的行都要带主键，很麻烦

6. hbase是按列存、所以可以做垂直分区，而且还支持列族的概念。

7. 典型的应用是webtable、可以记住在不同时间点上的某个字段的不同的数据

8. Features

    - Linear and modular scalability.
    - Strictly consistent reads and writes.
    - Automatic and configurable sharding of tables
    - Automatic failover support between RegionServers.
    - Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.
    - Easy to use Java API for client access.
    - Block cache and Bloom Filters for real-time queries.
    - Query predicate push down via server side Filters （query下推）
    - Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options
    - Extensible jruby-based (JIRB) shell
    - Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX

9. Tables are made of rows and columns. 
    - Table cells—the intersection of row and column coordinates—are versioned. （是带版本的）
    - Hbase里面的主键是完全有序地排列的，因为要记住每个tablet里面的键的最小和最大值（和mysql的多副本时不一样）

10. 列族的意义是：可以动态地加入一个表里（就是垂直分区的一个依据）（列族的数量不适合超过3个，因为会减少性能）

11. 要通过namesever改metadata，需要先通过chubby lock获取锁 （重听？）

12. Tables are automatically partitioned horizontally by HBase into regions. 

13. 这个图要重听一下

14. A common row key pattern is a website domain. 

15. 这个会一直往时间戳之前读、直到读到一个不为空的值（所以hbase是稀疏的表）

16. 所以用一个类似json的结构去存（时间戳是倒序的、保证直接读就是最新值）

17. 如果在一个先前的时间戳去读的话、有时候会读到一个空值

18. 版本在读取的时候可以指定（时间戳是倒序的、保证直接读就是最新值）

19. 要先disable然后再能做增加列族的操作

20. 所以是三维立方体、有不同版本的维度，更灵活、没有要求每个表有严格一直的schema

21. 关注的是大量的数据如何存储、而不是关注关系