## deep learning

1. 本质上就是通过一系列函数的线性组合去模拟出最终数值变化的函数

2. 欠拟合：神经元太少，模型太简单，不能很好地拟合数据

3. 过拟合：神经元太多，模型太复杂，对训练数据过度拟合，泛化能力差

4. 所以根据问题的规模、选择合适的模型大小

5. sigmod 的缺点就是梯度消失，（因为最大的一层的梯度为 0.25，然后再往前传的时候，梯度就会越来越小，导致前面的神经元的权重更新很慢）

6. 最后的模式里面、蓝色的是权重小的、红色的是权重大的（好像这个图不太对，因为有很多层，如果把隐藏层减掉就是差不多的了）

7. 卷积几次之后、特征已经变得更复杂了（模式的组合），所以需要池化降低维度

8. 填充：为了结果不要太小

9. 通道数：会影响到特征的提取，如果通道数太少，可能会丢失一些特征，如果通道数太多，可能会增加一些噪声，但是也能提取更多特征

10. 池化：最大值池化、平均值池化

    - 最大值池化：保留最大值，减少噪声，保留特征

11. 卷积神经网络：参数更少，适合图像识别任务

## NLP

1. NLP的六个场景：

    - 信息获取
    - 文本分类
    - 文档相似度推荐
    - 机器翻译
    - 聊天机器人/问答系统
    - 文本生成
        
2. NLP 研究领域  ≈  应用领域

3. 如果一个word用一个one-hot vector表示，那么这个向量的维度就是词典的大小，这样的向量维度太大，不利于计算，而且会有很多个0，浪费空间

3. 第一个问题是怎么把一个 word 变成一个 vector（因为上面的问题、所以需要降维）

4. 第二个问题可以用一个 BOW（bag of words）来解决（这个单词间的顺序问题如何去解决？）

5. 上面就是一句话怎么把它数值化的一个方法，数值化之后，每个单词都是一个向量

6. CNN就是可以用于NLP中的文本分类

7. 然后用卷积核去卷的时候、就能发现三个单词之间有什么关系、然后再池化、就能找到 5 个单词之间有什么关系、所以 CNN 是可以去做 NLP 的、只不过需要进行修改

8. 有CNN和TextCNN、TextCNN是用卷积核去卷的时候、拼接了短、中、长的三个不同范围的单词的特征、再去做训练，效果会更好（简单的CNN一定要卷到一个长的句子为止才终止）

## RNN，LSTM

1. 可以做时序预测，文本的生成（NLP）

2. 相当于每次输入的时候都拿到了前面一段时间的信息

3. 缺点是这个程序是不能并行的、因为一个点算之前需要算之前的点，只能串行

4. 如果这个时间的依赖非常长期的话、那么它这个网络就会很弱、学习不到这个特征

5. 让这个时间序列变得灵活的话、就有了LSTM（长短期记忆网络）

6. cell状态就是它的状态，h就是输出

6. LSTM 有三个门：遗忘门、输入门、输出门

7. 遗忘门：决定哪些信息是需要遗忘的（就是用一个输入的数据、sigmoid之后乘以上一个时间的输出）

8. 输入门：决定哪些信息是需要输入的（就是用一个输入的数据、sigmoid之后乘以上一个时间的输出，然后再用一个 tanh 之后乘上一个时间的输出）

9. 输出门：决定哪些信息是需要输出的（就是用一个输入的数据、sigmoid之后乘以上一个时间的输出，然后再用一个 tanh 之后传给下一个时间），输出了h

10. ppt上有具体的LSTM的公式

## chatGPT

1. 由于先前的计算效率不够高、所以需要transformer这些加快运算

2. 为什么每次问相同的问题的时候、会给出不同的答案。因为其中会有一个温度的概念、当温度不一样的时候、这个就有一定的概率去选择概率不是那么高的预测结果、这就显示出了很多的灵活性。（然后是每次选择一个单词、然后再去预测下一个单词，所以得到的结果不是千篇一律的而且会有差异）（所以如果温度为0的话，那么每次内容都会一样）

3. 这个n-gram就是对n个字母进行预测、n越高的时候它越像一个单词了

4. 然后这个从字母推广到词的时候、就能产生类似的效果。

5. 但是词的组合会有很多个、所以会有很多的组合、所以这个模型的参数会很大

6. 所以需要去涉及到做降维、就是做embeddings（相当于把一个one-hot编码变成一个词的vector）

7. 然后再经过transformer（和attention机制）、会得到后续结果