## 01 Stateful and Stateless

1. 本地缓存比较小、所以需要再有分布式缓存

2. 大部分数据是只读数据、所以可以使用缓存，而不是每次都去数据库查询

3. 主从复制的服务器可以每隔一段时间切换一下角色，保证多机器的冷热均衡

4. CDN 服务器：内容分发网络，可以缓存静态资源，减少服务器压力，在不同地区有不同的服务器节点，可以加速访问

5. 反向代理服务器：用于分发请求。“反向”这个词强调的是代理的作用是代表后端服务器来接收客户端的请求。反向代理则是指客户端直接向代理服务器发送请求，而代理服务器再将请求转发到后端的目标服务器。（让音视频可以在线点播）

6. 数据访问网关：封装数据访问的细节，提供统一的数据访问接口，对外提供服务（比如数据到底是从数据库还是缓存中获取）

7. 消息服务器：把服务变成基础设施

8. 微服务：抽离出来公用的服务，比如 A 应用和 B 应用都需要用到支付服务，那么就可以把支付服务抽离出来；或者比如说登录服务、注册服务等等

9. http 是无状态的协议，如果有状态，内存会爆

10. 实例池：限制每种实例的数量，防止内存爆炸，然后用 LRU 策略来淘汰，但是会有 swap out，硬盘读写，会影响性能

11. springboot 中也可以通过配置@Scope("prototype")，@Scope("session")等来控制实例的创建。prototype 即每次请求都会创建一个新的实例（任意情况下都是新的），session 即每个会话创建一个实例（见 ppt 表格）。singleton 即单例模式，即整个应用只有一个实例，这个是默认情况下的。request 即每个请求创建一个实例。

12. 如果需要不需要维护多个用户间独立的状态的话、就用成 singleton，如果需要维护多个用户间独立的状态的话、就用成 prototype。（状态即程序变量，比如在前端显示用户已经在线了多久，这个时间就是状态，需要在后端维护，而且每个用户的状态是不一样的）

13. 最好是有状态的内容尽量少（压缩、但是有些东西必须要靠状态实现），因为有状态的内容会占用内存，而且会有并发问题。无状态的内容尽量多。极致就是函数式编程，没有状态，只有函数。好处就是后端不用维护那么多对象，消耗资源就很少。

14. 连接的本质是一个线程，线程之间需要切换。所以连接池不是越大越好，因为线程上下文切换也是有开销的。所以连接池的大小要根据服务器的性能来调整。连接池的大小设置和用户并发数没有关系。如果高并发之后性能不行，那么就是服务器性能不行，不是连接池参数设置的问题。

15. connections = (2 \* core_count) + effective_spindle_count; （线程去做 IO 的时候，cpu 是空闲的，所以可以\*2 作为等待线程的数量）[链接](https://blog.csdn.net/Receptive2WE/article/details/122421718) 【遗留问题】effective_spindle_count 是什么？

首先可能形成瓶颈的是硬盘，网络IO和内存。然后MySQL是半双工的网络模型，半双工就是在同一时刻只能进行收发的其中一个动作。所以有了core_count*2的说法，一半在写数据，一半在收数据。effective_spindle_count 是在不能命中的时候需要硬盘去寻址，读页数据，比较耗时，这时候CPU不会一直等着硬盘，可以去处理其他，也就是说如果只有一个机械硬盘，我们需要多加1根线程来处理等待硬盘读的事务。

那为什么文中会说是在这个值附近呢？而不是说很精确的就是这个值呢？我的理解是这个也和其他一些比如事务类型，缓存，内存计算有关，抛开硬盘部分不说，如果事务类型都是很简单的计算，同时缓存能够100命中的话，那这个连接池数量还可以加，因为内存这时候可能网络IO成了瓶颈，可以多加一些链接来满足内存的运算量。如果本身事务比较复杂，内存计算不及时，那么这时候再加链接数量反倒增加了上下文切换的压力。

## 02 Messaging (JMS, Kafka)

1. 同步通信的问题：因为我们是面对接口编程的。有大量实例，可能调用不成功。或者网络不佳，也会失败。（没有送达保证）同步调用也没有请求缓冲（忙了也会失败）。同步还是紧耦合的，即调用函数的时候会需要根据函数名重新修改代码。同时会比较难维护。即过于强调了发送了一个 request，就要等待 response，这样就会导致请求的阻塞。所以同步通信不适合高并发的场景。（例子 打电话和发短信的例子）

2. 如果不用消息队列，就没有 request buffer，会在 controller 或者 service 或者 dao 里面阻塞，导致可能请求会丢掉。而且前端也不知道请求的状态。这几层之间都可以通过消息中间件进行交互。比如，kafka 中有很多 Topic 的缓冲。在消息队列的场景下，client 发送请求，然后 controller 直接返回一个确认收到的响应给 client。那怎么 client 如何最终确定这个请求处理完了呢：一种是再发一次 ajax 请求，一种是 websocket，一种是把结果重新发给另外一个 B 的消息队列，然后 client 从这个 B 的消息队列里面去拿结果。（类似于发邮箱）对应于 ajax ws 和 messaging （消息队列解决一段时间内请求高峰，请求波动的问题，服务器资源不够）

3. 问题在于异步的返回值应该是什么，service.m()还可能抛出一个异常。所以也要把这个异常组成一个消息发给 B 这个结果的消息队列。

4. 消息系统是 peer to peer 的通信方式。是松耦合。在这个 topic 里面的是数据。这个是 data-driven 的，即不指明调用的方法。 SOAP 和 HTML。

5. jms 是标准的 java 里的消息队列。让通信变得异步和可靠。

6. 什么时候要用消息通信？比如下订单。（可以在闲时处理）。

7. 消息的格式：1.header（必有） 2.body（可选） 3.property （可选）

8. 消息队列的 S，C 和 P 是什么意思：S 是 send ，C 是 consumer（client 设定的），P 是 producer（server 设定的）

9. 有时候 A - Kafka - B 之间，会根据设置的 Destination 或者 DeliveryMode 来决定消息的发送和接收。比如说，如果设置了 Destination 为持久化，那么即使 B 没有启动，A 发送的消息也会被保存在磁盘上，等 B 启动之后，会把这个消息发送给 B。如果设置了 DeliveryMode 为持久化，那么即使 Kafka 挂了，消息也不会丢失。如果设置了 DeliveryMode 为非持久化，那么 Kafka 挂了，消息也会丢失，不会发到 B。（比如股市的例子，要保证时序的顺序）

10. 消息头的例子 Clients can not extend the fields

    - JMSDestination (S)
    - JMSDeliveryMode (S)
    - JMSMessageID (S)
    - JMSTimestamp (S)
    - JMSCorrelationID (C)
    - JMSReplyTo (C)
    - JMSRedelivered (P)
    - JMSType (C)
    - JMSExpiration (S)
    - JMSPriority (S)

11. 消息的属性（Property） Clients can extend the fields。比如，扩展属性，给消息加上 selector：name=“B”，就能起到筛选的作用。只能加简单类型。Property name: follows the rule of naming selectors；Property value: boolean, byte, short, int, long, float, double, String

12. 消息体：见 ppt 表格

13. java 对象能够被序列化的意思：实现一个 java 类的接口，使得能变成适合在网络上传播的数据。

14. jms：destination 分为 topic 和 queue（那个图是什么意思）都需要通过链接工厂建立一个到 jms server 的链接，然后找到数据源 datasource。（JNDI namespace）

15. 点对点的消息：client 1 的消息只会被 client 2 接收到，而不会被 client 3 接收到（queue）。而发布订阅的消息：client 1 的消息会被 client 2 和 client 3 接收到。即点对点是一对一的，发布订阅是一对多的（广播 topic）。

16. Durable 和 Non-Durable 的区别：Durable 是持久化的，即消息不会丢失。（consumer 不在线的时候，消息不会丢失）Non-Durable 是非持久化的，即消息会丢失。（不在线的时候，消息会丢失）

17. browser 只浏览

18. 重要的是类的 toString 方法的重写

19. kafka 基于日志通信，只追加（append-only），顺序写，速度快。

20. 对每个用户维护一个 offset，即每个用户都有一个 offset，这个 offset 会记录这个用户读到了哪里。这个 offset 会被保存在 zookeeper 中。如果 kafka 挂了，那么 zookeeper 会把这个 offset 保存下来，等 kafka 恢复之后，会把这个 offset 读取出来，然后继续读取。（用 zookeeper 来管理）

21. 一个 topic 里面可以放相同类型的对象（长度易于管理），也可以放不同类型的对象（缺点是要记录多个偏移量，好处是只用一个队列）。

22. topic 可以分区，比如按照用户 id 来分区，这样可以保证一个用户的消息是有序的。但是这样会导致一个用户的消息只能被一个消费者消费，所以要根据业务需求来决定是否要分区。

23. 分区的目的：可以并发，同时可以集群部署。也可以加上分区备份，保证数据不丢失（在不同机器上，保证可靠性）。

24. 会发送心跳包，如果一个 consumer 挂了，那么会把这个分区的消息发送给另外一个 consumer。

25. 最好在事务里发送消息，配置 transaction 的原因：如果一个消息发送失败，那么就会导致这个消息丢失。所以要配置 transaction，保证消息的可靠性。

26. broker 是一个 kafka server，一个集群里面有多个 broker，每个 broker 里面有多个 topic，每个 topic 里面有多个 partition，每个 partition 里面有多个 offset。

27. 在 Kafka 中，`groupId` 用于将多个消费者组织成一个消费组。将两个 `@KafkaListener` 的 `groupId` 设置为相同的含义如下：

    - **同一组中的多个消费者**：当多个消费者属于同一个消费组时，Kafka 会在这些消费者之间分配消息。这意味着在这个消费组中的每个消费者只能处理自己分配到的消息，而不会重复处理。

    - **负载均衡**：如果你有多个实例的应用程序运行并且它们都使用相同的 `groupId`，Kafka 将会自动在这些实例之间分配消息，从而实现负载均衡。

    - **消息确认**：同一消费组的消费者共同维护消息的偏移量（offset），这意味着当组内的某个消费者成功处理了消息后，偏移量会更新，其他消费者就不会再处理这些消息。

    - 在例子中，虽然两个监听器的 `groupId` 相同，但它们监听的是不同的主题（`topic1` 和 `topic2`），因此它们不会相互影响。 `topic1Listener` 处理来自 `topic1` 的消息，而 `topic2Listener` 处理来自 `topic2` 的消息。不过，它们共享相同的 `groupId` 并没有实际意义，因为它们不在同一个主题上工作。

## 03 Websocket

1. 全双工的通信，即客户端和服务端都可以发送消息（双边都在工作）。没有响应 请求的概念了。

2. 底层仍然是 TCP 协议。（一定要保证收到，会重发）（UDP 是不保证收到的，但是速度快）

3. 前端和后端都运行着 websocket 程序

4. 称为一个 websocket 的 endpoint，有统一定位符

5. 所以需要握手（即注册）。websocket 由握手和数据传输两部分组成。握手是基于 http 的，数据传输是基于 tcp 的。

6. 握手时，如果两个 key 能匹配上、即握手成功。

7. ws://是不加密的，wss://是加密的 443 端口。（ws+ssl）

8. 握手握上了、server 会专门为每一个客户端开一个 session。（server 端维护了一个 list）

9. session.getOpenSessions() 可以获取所有的 session 的 list （当规模大了之后，单纯的 for 循环会有性能问题）

10. ConcurrentLinkedQueue 是线程安全的 （注意要用）

11. 可以增加 websocket 的编码器和解码器，来处理消息的编码和解码

12. 前端要用一个 STOMP 的库

13. 下面的代码是一个简单的例子，即前端发送一个消息给后端，后端再把这个消息发送给前端。其中，`@MessageMapping("/hello")` 是前端发送消息的目标路径，`@SendTo("/topic/greetings")` 是后端发送消息的目标路径。

```java
@MessageMapping("/hello")
@SendTo("/topic/greetings")
```

14. 也可以设置 broker 代理。

15. ajax 通过 callback 函数来实现前端的异步通信。消息队列处理前端请求过多的问题（返回给前端订单已接受的响应）。（后端异步，如果是同步的话、数据在后端就直接丢掉了）。websocket 解决订单什么时候处理完的问题。（主动推回给前端）

16. 也不是什么都要用消息队列。比如 login（高优先级的任务）就不用消息队列。

17. service 下的应该是同一层逻辑，不然会有很多代码重复。

18. http session 和 websocket session 是不同的 session

19. react 使用 需要先 npm install websocket ; npm install sockjs-client ; npm install stompjs ; npm install --save-dev @types/sockjs-client ; npm i --save-dev @types/stompjs

```javascript
//vite.config.ts
import { defineConfig } from "vite";
import react from "@vitejs/plugin-react";

export default defineConfig({
  plugins: [react()],
  define: {
    // Some libraries use the global object, even though it doesn't exist in the browser.
    // Alternatively, we could add `<script>window.global = window;</script>` to index.html.
    // https://github.com/vitejs/vite/discussions/5912
    global: {},
  },
});
// 这里有一个问题、如果在vite中像上面这样使用、尝试使用sockjs的时候、会导致前端控制台没有报错信息，所以也可以用下面的方法，在index.html中加入
<script>const global = globalThis;</script>;
```

20. SockJS 需要使用 http 或 https 协议进行连接，内部会自动转化为 ws 或 wss 协议

21. npm 设置代理

```shell
npm config set proxy http://127.0.0.1:7890
npm config set https-proxy http://127.0.0.1:7890
npm config list
```

22. 使用 sockjs 之后、还可能需要配置跨域访问和拦截器的设置，因为是通过 http 协议连接的。

23. 找到一个问题、如果在 vite 里面设置 global 变量、会导致前端 sockjs 没有报错信息。

24. 但是后端需要额外设置 sockjs 的 CORS 策略，而且要分清楚是 patterns 还是 origins。如果是 patterns 的话，那么就是正则表达式，
    如果是 origins 的话，那么就是一个字符串数组。

25. 前端 react 使用严格模式的时候、在调试环境的时候、会自动触发两次 useEffect ，关掉 strict mode 就不会触发两次了。

26. @ServerEndpoint 注解好像不能用于 sockjs 的情况，只能用于 websocket 的情况。自定义 sockjs 的 endpoint 需要其他方式。

27. 注意下面的 patterns ...

```java
registry.addEndpoint("/ws").setAllowedOriginPatterns("*").withSockJS();
```

## 04 transaction

1. OrderDao 和 OrderItemDao ?

2. 问题例子 1：如果两个用户同时对只剩 1 本书的库存下单。（并发情况下需要保证库存的一致性）

3. 问题例子 2：在转账操作中，分批写数据库，如果第一次写成功，第二次写失败，那么就会导致数据不一致。（要看起来像原子性的）

4. 所以有了事务，要么全部执行，要么回滚。（注意是只在数据库（资源管理器）中进行回滚，而不是在代码中进行回滚）

5. EJB 是 JavaEE 的一部分，是一种企业级的 JavaBean。EJB 是一种服务器端组件模型，
   用于构建分布式应用程序。EJB 有三种类型：Session Bean、Entity Bean 和 Message-Driven Bean。
   Session Bean 又分为 Stateful Session Bean 和 Stateless Session Bean。
   Stateful Session Bean 是有状态的，Stateless Session Bean 是无状态的。

6. 注解@TransactionAttribute 是用来控制事务的属性的。

   - TransactionAttributeType.REQUIRED：如果目前进程中有事务，那么就加入到这个事务中，如果没有事务，那么就新建一个事务。（默认）
     这里的有和没有事务是相对于父子函数的 txattribute 来说的（见 ppt 上 methodA 和 methodB 的例子）
   - TransactionAttributeType.REQUIRES_NEW：不管有没有事务，都新建一个事务。（即使外层有事务，也会新建一个事务）
     即新事务 TX2 的成功或者回滚都不会影响到外层事务 TX1 的成功或者回滚。（比如 TX2 是 log，TX1 是转账，TX2 不稳定，TX1 稳定）
   - TransactionAttributeType.SUPPORTS：如果有事务，那么就加入到这个事务中，如果没有事务，那么就不用事务。
     （比如 A 和 B 都是 support，那么两个都没有事务）
   - TransactionAttributeType.NOT_SUPPORTED：不管有没有事务，都不用事务。（非事务状态下运行，会挂起父事务）
   - TransactionAttributeType.MANDATORY：必须父函数有事务，如果没有事务，就会抛出异常。（必须有事务，否则就会抛异常）
   - TransactionAttributeType.NEVER：必须父函数没有事务，如果有事务，就会抛出异常。（必须没有事务，否则就会抛异常）

7. Spring 里面用@Transactional 来控制事务的属性。即事务传播行为。

8. String... 是可变参数，可以传入多个参数，但是要放在最后一个参数。（就是可以相当于是数组）

9. ACID

   - Atomicity：原子性，要么全部执行，要么全部回滚
   - Consistency：一致性，事务执行前后，数据的完整性约束没有被破坏
   - Isolation：隔离性，多个事务并发执行的时候，一个事务的执行不应该影响到其他事务的执行
   - Durability：持久性，事务执行成功之后，数据的改变是持久的

10. dirty reads：脏读，一个事务读取到了另一个事务未提交的数据（进行中的状态，可能会被 rollback）（没有任何隔离）
    non-repeatable reads：不可重复读，切断了事务间内存的交流，但是可能会对数据库同时写。
    phantom reads：幻读，在上面的基础上再对数据库加锁，但是会有不断有新的满足条件的记录进去。（通过对表的加锁定来解决）

11. 所以有了@Transactional isolation 注释

    - Isolation.DEFAULT：使用数据库的默认隔离级别
    - Isolation.READ_UNCOMMITTED：读未提交，会出现脏读、不可重复读、幻读
    - Isolation.READ_COMMITTED：读已提交，会出现不可重复读、幻读
    - Isolation.REPEATABLE_READ：可重复读，会出现幻读
    - Isolation.SERIALIZABLE：串行化，不会出现脏读、不可重复读、幻读

12. read locks：读锁，多个事务可以同时读取数据，但是不能同时写入数据

13. write locks：写锁，只有一个事务可以写入数据，其他事务不能读取和写入数据

14. 这个注释是通过连接在数据库中完成的，而不是在应用程序中完成的。即数据库管理锁。（即不同的隔离级别）

15. 管理分布式的事务：因为多个数据库之间是互相隔离的（spring 在监测到多数据源时会自动管理，不需要代码上的额外操作）

    - 实现：2 phase commit（2PC）
    - Stage 1：prepare（准备阶段）：每个数据库都会把事务的结果写到一个 log 里面，然后返回给协调者
    - Stage 2：commit（提交阶段）：如果所有的数据库都返回了 prepare，那么就会提交，否则就会回滚

16. 但是也有问题、如果 rollback 的时候、到其中一个数据库的网络不通。会导致这个数据库进行启发式的操作，即自己决定是 commit 还是 rollback。
    所以这里只能人工介入了。（就是不能避免第二阶段的问题）

17. 能支持这种事务的，都叫 Resource Manager。比如说，数据库、消息队列、邮件等等。（见 ppt 图）

18. optimistic offline lock：多线程可以读取，但是只有一个线程可以写入。即读取的时候不加锁，写入的时候检查版本号 version。
    （即乐观锁，判断是基于旧数据进行的操作就行，预计并发写的情况不多，即读的概率很高，写的概率不高）

19. pessimistic offline lock：读取的时候加锁，写入的时候也加锁。即悲观锁，判断是基于新数据进行的操作就行，预计并发写的情况很多。

20. Coarse-grained lock：粗粒度锁，即把和一张表关联的所有数据都锁住。（比如说，一个订单的所有商品都锁住）
    也存在乐观锁和悲观锁的区别。（通过 version 表，这里的悲观锁需要对 version 表，版本号加锁）

21. mysql 的默认隔离级别是 REPEATABLE READ。作业中发现一个问题、如果让 orderItem 的 Save 变成 REQUIRES_NEW，会导致死锁，报错如下。SQL 错误代码 1205 和 SQLState 40001 通常表示一个 死锁（Deadlock）错误。注意这里抛出异常的是父事务、可能是因为父事务先加入等待序列、而子事务后加入等待序列、其定时器先到期，所以父事务先抛出异常。

```shell
2024-10-09T16:46:32.239+08:00  WARN 36108 --- [ntainer#0-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 1205, SQLState: 40001
2024-10-09T16:46:32.239+08:00 ERROR 36108 --- [ntainer#0-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : Lock wait timeout exceeded; try restarting transaction
Order processing failed !
```

## 05 事务管理

1.  做 websocket 连接的时候 其实需要用到 JWT 鉴权。

2.  回滚：rollback 的时候、需要用到 undo log，即把之前的操作反过来执行一遍。

3.  redo log：解决从内存写到磁盘时，数据还没落盘就挂了的问题。即把数据先写到 redo log 里面，然后再写到磁盘里面。数据库重启的时候，会把 redo log 里面的数据重新写到磁盘里面。

4.  两种写硬盘方式，一种是直接落盘，一种是先写 dirty page，然后再写到磁盘里面。后者的好处是速度快，但是有可能会丢失数据。

5.  日志分为两种，一种是记录 sql 语句的，一种是记录 bit 位的。

6.  隔离性：

    - 写读冲突：导致脏读（dirty read）
    - 读写冲突：导致不可重复读（non-repeatable read）（在同一个事务中，读取到的数据不一样，因为在事务中，数据可能被修改了）（可以通过加锁来解决）
    - 写写冲突：导致脏写（dirty write）

7.  幻读：读到其他事务插入或者删除的数据。（和不可重复读的区别是，不可重复读是读到了其他事务修改的数据，而幻读是读到了其他事务插入或者删除的数据）

8.  调度：即多个事务的读写顺序安排。会有串行调度和并发调度。（串行调度一定是正确的）

9.  可串行化调度：一个并发调度 S 如果存在一个串行调度 S'，在任何数据库状态下，S'和 S 产生的结果是一样的，那么就称 S 是可串行化的。

10. 实现方法：找到一个解就行（因为一共的解是很多的）

11. 冲突可串行化调度：：冲突可串行化调度，是从冲突的角度出发，针对一个调度 S 去发现其等价的串行调度 S’ 来确定 S 是一个可串行化调度。

12. 一次操作交换定义为交换事务调度序列中相邻的两个操作，一个交换操作可以将一个调度 A 变成另外一个调度 B。

13. 当交换不会影响两个调度的一致性时，定义该交换得到的两个调度是等价的，该交换为等价交换。

14. 等价操作

    - 交换连续两个相同数据读取操作的顺序:RT1(A) RT2(A)
    - 交换连续两个不同数据读写操作的顺序:RT1(A) WT2(B)；WT1(A) RT2(B)；WT1(A) WT2(B)

15. 非等价操作– 交换连续两个相同数据的读写、写写操作: RT1(A) WT2(A)；WT1(A) RT2(A)；WT1(A) WT2(A)

16. 优先图：利用冲突关系，可以将并发的事务构建成一个图来验证事务的并发调度 S’是否是冲突可串行化调度。

    - 若 Ti 与事务 Tj 存在冲突（读写冲突、写写冲突、写读冲突）（注意是这三种冲突，没有读读冲突），且 Ti 的冲突操作在 Tj 冲突操作之前，则 Ti 向 Tj 连一条有向边（Ti，Tj）。

17. 冲突可串行化的充分必要条件：优先图无环

18. 原子性：运行期间刷盘的优势：内存占用少、但是重启的时候会慢一点。运行期间不刷盘的优势：重启的速度快，但是内存占用大。

19. 持久性： 事务完成（commit、abort）时刷盘，故障系统重启后自动保证持久性；事务完成时不刷盘，故障系统重启后需重做该事务

20. 原子性：撤销未结束（不带 Commit、Abort 标记）的事务 – 持久性：重做已经结束（带 Commit 或 Abort 标记）的事务

21. ppt 上有例子。

22. 保持脏页的缺点就是内存占用大。写入的缺点就是 IO 操作多，速度慢。

23. FORCE 条件：事务完成强制刷脏 vs 不强制刷脏

    - Force 缺点：每次事务提交都必须刷新脏页，消耗大量 IO 读写资源
    - 解决方法：使用 NO-FORCE 模式，利用 Redo 日志重做事务
    - Redo 日志：记录事务对数据库的所有影响

24. NO-STEAL 条件：事务中间可以刷脏 vs 不可以刷脏

    - No-Steal 缺点：事务执行过程中不能刷新磁盘，必须占有较大的缓冲区空间，不利于多个事务的并发执行
    - 解决方法：使用 STEAL 模式，利用 Undo 日志撤销事务
    - Undo 日志：记录撤销事务所需的内

25. 要解决原子性和持久性问题，各分别存在两种方法，两两组合起来产生四种数据库恢复算法

26. 日志的内容在写入磁盘以后是不会被修改的，，因此所有的日志内容可以顺序写入磁盘，这保证了高效的写入速度。该特性也是建立高可用恢复机制的前提。即数据的随机读写转换为日志的连续读写。

27. 逻辑日志和物理日志的区别：逻辑日志是记录的是 sql 语句，物理日志是记录的是 bit 位。物理日志可以是幂等的，即多次执行不会有影响。（比如一个在逻辑日志中的 insert 操作，物理日志满足幂等性；逻辑日志不满足）物理日志通常还小一点。

28. 有关数据库日志的三个重要性质（见 ppt）

    - 幂等性：一条日志记录无论执行一次或多次，得到的结果都是一致的。物理日志满足幂等性；逻辑日志不满足
    - 失败可重做性：：一条日志执行失败后，是否可以重做一遍达成恢复目的。物理日志满足失败可重做性；逻辑日志不满足：例如插入数据页面成功，而插入索引失败，重做插入这个逻辑日志失败。
    - 操作可逆性：逆向执行日志记录的操作，可以恢复原来状态（未执行这批操作时的状态）物理日志不可逆（页面偏移量位置可能被后续记录修改），逻辑日志可逆。

29. 所以重做 redo 日志是物理日志（因为逻辑日志不具有幂等性，逻辑日志不具有失败可重做性，一条逻辑日志记录可能对应多项数据修改（表、索引），崩溃时 X 对索引 造成影响，但没有影响数据页面。），而撤销 undo 日志是逻辑日志或者物理逻辑日志（ 物理日志不具有可逆性，无法处理数据项位置变化的情况，页面分裂后 A 的地址发生了变化，撤销的时候无法定位数据项，因此物理日志一般不能用于回滚，Undo 日志须用逻辑日志）。

30. 有一个影子拷贝的方法，事务修改在拷贝数据库上(或者影子拷贝页面上)

    - 优点：影子页面，仅拷贝修改的页面
    - 缺点：效率低，难并发

31. 补偿日志（Undo 日志的 redo 日志，基于 undo/redo 日志的恢复，每次执行 undo 日志记录后，数据库需要向日志中写入一条补偿日志记录 （compensation log record，CLR），记录撤销的动作，记录已经 undo 的日志，保证 undo 不被重复执行）、还有 redo 日志和 undo 日志的实际流程在对应的 ppt 中。

32. undo 里面就是每次操作完的旧值（因为要撤销）、redo 里面就是每次操作完的新值（因为要重做）

## 06 Multi-threading

1. 问题：进程间的内存是隔离的。可以通过 RMI 来实现进程间的通信。RMI：Remote Method Invocation，即远程方法调用。

2. 但是整个 JVM 是一个进程，里面的所有比如 spring 都是线程。

3. 有两种方法，一个是实现 Runnable object，一个是当继承 thread 的子类。但是推荐是前者，因为 java 是单继承的。（即每个类只能有一个子继承的类）

4. sleep 可能会有在多台机器上精度不一致的问题。

5. 多线程下会存在同步性和内存一致性的问题，比如操作一个类里的同一个对象

6. 所以有了 synchronized 关键字，即同步锁。synchronized 修饰的方法，只能有一个线程进入。synchronized 修饰的代码块，只能有一个线程进入。（而且是多个函数之间会共享一把锁，每一个对象会有一个内部锁，用 1bit，这个对象里面所有的 synchronized 方法都会共享这个锁）（static 的也需要这个锁）

7. 也可以用 synchronized(this) 来锁代码块

8. 也可以用一个对象当作一个锁，即 synchronized(object) 来锁代码块

9. reentrant synchronization 可重入锁：即一个线程可以多次进入同一个 synchronized 方法，但是要注意，如果是多个 synchronized 方法，那么就会有问题。（即一个线程可以多次进入同一个 synchronized 方法，但是不能进入不同的 synchronized 方法）

10. java 中的原子性操作：Reads and writes are atomic for reference variables and for most primitive variables (all types except long and double). Reads and writes are atomic for all variables declared volatile (including long and double variables).

    - 读和写是原子性的，即一个线程读到的是另一个线程写的值，而不是自己的缓存值。（除了 long 和 double 之外的所有类型）
    - 如果设置为 volatile，那么读和写都是原子性的。（包括 long 和 double）

11. 会有死锁、活锁和 starvation 的问题。死锁解决方法可以是超时。

12. 可以会有 spin lock，会阻塞，但是不会有线程切换。但是会有问题，比如一个线程一直占用 CPU。（如果线程切换的开销大于了 spin lock 的开销，那么就适合用 spin lock）

13. 所以也可以用 wait 和 notify 来解决。wait 会释放锁，notify 会唤醒一个线程。notifyAll 会唤醒所有线程。

14. java 的自动垃圾回收通过 generation 来实现的，即新生代和老年代。新生代又分为 Eden 和两个 Survivor。老年代是存活时间长的对象。（就是一个 generation 满了之后，就会被移到下一个 generation，然后做垃圾回收）

15. Immutable Objects 就是不可变对象，即一旦创建就不能被修改。比如 String 类。这样可以避免多线程的问题。如果要修改、就要重新创建一个对象然后赋值。

16. 单单对函数进行 sync 还是会有问题，因为可能会有多个线程按不同的时序进入 sync 函数。（ppt 例子）

17. 所以需要写 final private 的方法。然后不提供 setter，因为不存在多线程写的情况，所以一定是线程安全的。

18. 有一个锁对象 Lock，分为可重入和不可重入。

19. 可重入的锁：ReentrantLock，即一个线程可以多次进入同一个锁。（即一个线程可以多次进入同一个锁，但是不能进入不同的锁）

20. 也有一个 executor，就相当于有一个线程池给所有的线程使用，不然的话就是会造成对象的垃圾回收。线程池会保持内存存在，只是值恢复初始化

21. executor interface 分为 3 种，即 Executor、ExecutorService 和 ScheduledExecutorService。Executor 是最基本的接口，接收 Runnable 类。ExecutorService 可以接受 runnable 和 callback。ScheduledExecutorService 可以延迟执行任务。

22. blur 操作，不停地将任务分解，直到任务可以被执行。即任务分解的过程。（fork / join task），在 java 里面叫做 RecursiveAction 和 RecursiveTask。这样就可以切成多线程多处理器并发执行了。

23. 这个 recursivetask 通过 override 的 compute 方法来实现任务的分解和合并。即分解成子任务，然后合并子任务。

24. 上面会涉及到 invokeAll 方法，把新建的线程通过线程池进行管理，即通过线程池的管理来执行任务。（已经封装好了）

25. 为什么要并发安全、因为不知道同时会有多少客户端连接，所以需要用线程安全的集合，同时写有问题。

26. atomic object 这种对象也可以通过非基本类型的原子性操作

27. 用途：每个线程生成不同的伪随机数

28. java 新特性，virtual thread，即轻量级线程，不需要操作系统的线程，而是在 JVM 里面实现的。这样就可以避免线程的切换开销。（相当于 n 个虚拟线程可以在一个真实线程里面运行）（本质就是要复用）（例子：服务器给每一个客户端分配一个虚拟线程）（有一个 try-resolve block，即先获取，再执行，try（）{}catch（）{}）

```
 Thread.Builder builder =
    Thread.ofVirtual().name("worker-", 0);
```

29. 与之对应的是 platform thread，即绑定着操作系统的线程（平台线程）。会存在线程栈，持有很多资源，开销大。（操作系统里面的线程数是有限的）（linux 线程调用用的是红黑树，线程调度性能受树高影响）

## 07 cache

1. memcached，就是以键值对的形式存储数据在内存里面。

2. DBMS 是数据库管理系统。比如 mysql 里面也有 cache；spring 这里 JPA 里面也带有 cache，为什么还需要缓存 memcache 呢？

3. mysql 的 cache 是根据 sql 操作来进行存储 cache 的，他做不到。MySQL 内部的缓存主要包括多个不同层次和类型的缓存，旨在提高数据库性能和响应速度。比如查询缓存、查询结果缓存、InnoDB 缓冲池、表缓存、键缓存等。

4. 进程间通信要 martial（即序列化），因为内存是隔离的。（也会耗时）

5. 也可能 spring，memcache 和 db server 在不同的机器上。所以需要单独分离出一个 memcache。（memcache 甚至可以既缓存 mysql 的，又缓存 mongodb 的，而且存的是一个处理后的结果，而不是原始数据）

6. InnoDB 是 MySQL 的一个存储引擎，专门设计用于支持高性能和高可靠性的数据库应用。它是 MySQL 默认的存储引擎，并具有多个关键特性，使其适用于需要事务处理和高并发访问的应用场景。

7. InnoDB 的聚簇索引（Clustered Index）是该存储引擎的一种特殊索引类型，它将表的数据存储与索引结构结合在一起，优化了数据的检索效率。聚簇索引是将表中的数据行与索引存储在一起，数据的物理顺序与索引的顺序一致。换句话说，表的实际数据按照聚簇索引的顺序存储在磁盘上。聚簇索引使用 B+树结构，这意味着所有的叶子节点（leaf node）都包含了实际的数据行，而非仅仅是指向数据行的指针。这样可以通过索引直接访问数据行，提高查询性能。

8. 经常要写的数据、可以不用放在 memcache 里面

9. 书本的 isbn 和 title 可以放在 cache 里面，因为不经常变化。

10. 这个 cacheable 的注解可以写在 repository 的方法上。比如 getByIsbn，会先在 cache 里面看有没有对应的 isbn，如果有的话，就直接返回，如果没有的话，就去数据库里面找。（springcache，也要在 main 函数里面加上 @EnableCaching）

11. 比如 book 只有 1k，一个 person 因为含有 img，所以有 5M，怎么在 memcache 里面存储高效？在 memcache 里面分成了很多种大小的 chunks。然后这个注解的名字空间可以辅助剪枝，减少找到内存块的时间。

12. 不同的 cache server 间的 node 不需要通信。（因为有哈希）

13. 如果有了多个 server 节点，需要做一致性哈希来存储和 get（解决增减服务器后数据位置的问题）（会涉及到一个环中的 chord 的概念，环中可能还有环）（好处、总体浪费空间不多，而且可以增加搜索性能）

14. 如果 server 上的 cache 满了，那么通过 LRU 来写到硬盘上。（然后这个硬盘是 memcache server 同一个的硬盘上）（然后硬盘也可能被写满，所以要限制硬盘写的空间大小）

15. redis：redis 也可以做主从备份，redis 里面的 list 是 link list，因为避免复制（所以顺序查找比较慢）

16. 为什么需要数据访问网关？data access gateway。（减少冗余代码，多种 daoimpl 调用同一个模板类）

17. redis 除了放类，还可以放消息（不需要 kafka，因为运行在 spring 外，所以实现异步通信，也不一定要用消息队列中间件，可以直接种 redis 缓存中间件）

18. 可以有不同的 redis 操作，比如 rightpush 和 set，可以用不同的操作来实现不同的功能（数据结构）

19. opsForList()是一个操作，可以用来操作 list，比如 rightpush 和 leftpop。opsForValue()是一个操作，可以用来操作 value（普通的 key-value），比如 set 和 get。

20. 但是这个 list 上面提到是链表、所以不用

## 08 Full-text Searching

1. 场景：搜索书评（非结构化数据）（remark 用 varchar 放、可变长，char 是定长）（或者用表外部的 text/blob 文件进行存储，用指针指向）

2. 如果用 sql 的 like 加%%，本质还是一个一个读出来再对比，效率低，会很慢。

3. 还有一种方法是结构化部分的关键词、作为列或者行。（但是看似能做结构支撑、但是还需要做结构化）

4. 所以需要做一个反向定位的索引，比如搜关键词然后返回评论的位置。

5. 所以有四个问题，一个是 keyword 进去之后、如何返回哪些 doc；还有一个是返回的 doc 如何排序。（相似度和频率）；还有一个是返回的数据为什么是 title+url+部分内容+author，而不是全部（即索引维护的内容究竟是哪些部分，可以是只对 title 进行索引，其他文本内容只是为了快速返回内容）；这个索引在做维护的时候、能不能 incremental 地做维护。

6. Lucene 是一个全文检索引擎，可以用来做搜索。（主要做非结构化数据文本的搜索，比如可以是来自文件、数据库、网页等等）

7. 因为有各种各样的数据、所以需要一个适配器 adapter，来解析各种数据。

8. 为了变快，需要将索引建成支持快速扫描的结构。

9. .fnm 文件：有一个索引的域（field），可以是（支持 index 的） subject、content、author 等等。（或者是支持向量化的 vectored 的 subject，用余弦相似度计算）

10. .tis 文件：维护域里的 value 在几个文档里面出现过。

11. .frq 文件：维护域里的 value 在每个文档里面出现的次数。

12. .prx 文件：维护.frq 文件中的 frequency 在文档中的位置。

13. 以上这个就是一种反向索引的结构。即不需要通过文档的一行行匹配来对比。可以直接通过关键词定位到文档的位置。

14. 会涉及到 precision 和 recall 的 balance 的问题，即精确度和召回率。精确度是指检索出来的结果中有多少是正确的，召回率是指所有正确的结果中有多少被检索出来了。

15. 有些内容是不需要参与索引的构建的（优化）

16. lucene analyzer 是用来分词的，比如中文分词器、英文分词器等等，把 text 分成一个个的 token。其中也有 stop analyzer，即停用词分析器，可以把一些无意义的词去掉。

17. lucene codecs ：是用来对索引进行编码的，比如可以用来压缩索引。

18. lucene docuemnt 是用来存储索引的，即一个 document 里面有很多个 field，每个 field 里面有一个 value。

19. 给的示例代码的例子、即对应概念中的域

20. 其中的数值索引、可以支持范围搜索或者精确匹配

21. 建完索引之后会有一个 segment 文件生成，相当于就把 lucene 的索引分成了很多建好的索引段（索引的索引）（因为索引可能会比较大、所以就支持了在不同的机器上面存）

22. document number 是 lucene 里面的一个概念，即每个 document 有一个编号，可以通过这个编号来找到 document。（这个编号是唯一的）

23. 相似度是为了最终进行排序

24. Similarity 是用来计算相似度的，即计算文档和查询的相似度。（比如 BM25Similarity）

25. TF-IDF 是一种计算相似度的方法，即 term frequency 和 inverse document frequency。即一个词在文档中出现的次数和在所有文档中出现的次数的比值。(即出现多的次数可能是在很多文档中出现了很多次、说明是一个常用词，所以权重就会降低)

26. 由于 lucene 使用比较复杂，所以有了 solr，即一个基于 lucene 的搜索服务器，可以用来搜索、存储、索引文档。

27. 除此之外、还有 elasticsearch。是近实时的。（不用写 sql，建立反向索引）

28. cache 里面的对象可以被多个进程读
