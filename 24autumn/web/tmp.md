## Hive

1. hive是在hadoop基础上发展起来的数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。

2. 要在低版本的java8上运行

3. 很多数据仓库仍然用的是hive去做metadata的store

4. 数据仓库和数据库最大的区别就是它是schema on read的状态。大量的数据仍然是以原始文件的状态存储在hdfs上面、然后hive在上面去读取这个hdfs里的内容。

5. 然后sql不可能直接执行在csv这种文件格式上、所以，hive会先将这些文件做处理、然后转成一种中间格式、最常见的是parquet格式。

6. csv加载进来的dataframe之后、有些内容字段会为空、所以会在进行LOAD DATA操作的时候、不会先做预处理（不做ETL操作、不做校验、数据清洗转换和加载）、直到SQL（做在这个数据上面的分析时）的时候才会执行变成parquet的操作。这个是schema on read的特性。

7. 初始化hive的时候、可以填进去一个<db type>，默认是derby、存的是metadata的信息。如果是derby的话、就是存储在内存里面、也可以存储在mysql里面，但是要告诉具体的位置在哪。

8. 也可以做分区、之后做搜索的时候就可以定位到一个分区里了

9. parquet为什么是列存的？因为做的是OLAP、就是在线的分析处理、都是会对同一列的数据进行操作、所以列存储的效率会更高。

10. 同样是sql、为什么要存到hive里而不是mysql的？因为hive支持很多数据格式。而且hive执行sql表格的来源是多样的、不仅仅是mysql table，还可以是csv等等。

11. Metastore是hive的元数据存储的地方、hive的元数据存储在metastore里面、metastore是一个数据库、存储了hive的元数据信息、比如表的信息、表的结构、表的列、表的分区等等。可以分成独立的进程、也可以和hive在一起。

12. 是schema on read的状态、所以在hive里面、不会对数据做任何的校验和处理、只有在query查询的时候才会做处理。（好处就是一开始只需要直接拷过来、然后就很快）（而且场景是大量的数据、不可能一开始就做处理，多种数据源情况下的一个选择）

13. schema on write的状态、就是在写入的时候就会做处理、比如mysql、写入的时候就会做校验、数据清洗、转换等等。（查询的数据块，因为查询的时候就不用校验了）

14. （会把tables或者partitions进一步地划分成小的buckets）用buckets、查询方便，进一步减少查询的数据量。而且会做分布存储、不同的bucket会存储在不同的地方、这样查询的时候就会更快。也可以保证机器学习中采样的效率、保证整体的采样和部分的采样的一致性。（如果是按照性别区分的buckets）

15. 存储的时候、会把表拆成不同的列族，变成HBase。然后RCFile是会先吧几个行组织成row group、然后在这个row group 里面再按照列进行存储。就有助于OLAP的查询。

16. ORCFile就是优化过的RCFile。会有Index data、row data、stripe footer。这样就可以快速定位到数据的位置、然后直接读取数据。（index data存的就是row data中的block的offset）

17. 重复数据多的时候、同样会有优化、比如只存变化量

18. 压缩率高、存储的时候会压缩、查询的时候会解压缩，所以会需要做权衡

## Flink

1. 事实上的流式数据处理的框架。

2. 在spark里、在微观上会把时间轴切成很小的时间窗、然后宏观上是流处理、微观上是批处理。

3. flink要支持有状态的处理、就是处理后面事件的时候、要知道前面的状态是什么。比如统计一个小时内的数据、就要知道前面的数据是什么。

3. 它可以支持有界或者无界的流式数据

4. 几个概念：streams

5. state

6. time

    - 所有的事件都有一个时间戳。
    - 有一个产生事件的时间、有一个处理的时间。

7. 有 layered api

    - 如果关心业务、可以用高层的table api
    - 如果需要从流的角度去处理、可以用中间的datastream api
    - 如果需要更底层的操作、可以用底层的process function（流是由一堆事件组成的、可以对这些事件进行操作）

8. 先演示了一个最底层的api、process function，这个例子是说要计算一个事件的开始时间和结束时间之间的时间差，需要在start的时候开始记录并等待、然后在end的时候计算时间差。如果等待超过了4h、就会停止等待这个事件。

9. 这个datastream api的例子、就是分一段固定的时间时、做map reduce

10. 状态是怎么保存的、是放在内存或硬盘上、处理的时候会要读这个状态、内存不大的时候、需要存到硬盘上、会影响性能。每个程序只能访问自己的本地的state、不能访问其他设备上的state。为了保证一致性、需要做state的snapshot快照，那么核心问题就是要给哪些东西做快照。

11. flink在做word count的时候、会有两个流程、就是先input、然后再做sink（count）。为了保证不发乱，需要把同样的单词发到同样的地方去。所以就根据hash发到不同的机器上去。

12. 这个就是keyed state、就是根据key来存储state。这个key是根据hash来生成的、所以相同的key会发到同一个地方去。这个counter必须是有状态的、这个counter是1还是2，是有状态的、基于之前的状态来做的。source发送的时候就需要做指定目标的发送。

13. 为了提高可靠性、需要每隔一段时间做一个checkpoint，只会丢掉最后一个checkpoint之后的数据。记下来之后、已经做过checkpoinnt的事件、如果还没来得及处理、可以通过replay checkpoint来处理。

14. 这个checkpoint barrier是什么呢、也是相当于流里的一个事件。比如一个operator会接受有两个流、每个流都会接受到一个checkpoint barrier，然后这个operator会等待两个流都接受到checkpoint barrier之后、才会继续往下走。（这里就在做对齐操作了，然后写到硬盘上之后就记录完了）

15. 能不能不对齐也能做呢？也是可以的、就是要把这个栅栏之后还没处理完的内容、也都算在这个checkpoint的state里面、好处就是没有这个等的过程，性能更高。坏处就是记录的状态会更多。

16. 这个存这个snapshot状态的时候用的是RocksDB、所以新的状态会更快地被拿到、老的数据就沉下去了。

17. 为了做同步、还有用watermarks水印来表示的时间。意义表达了、如果这个流是按照顺序来的、那么watermarks前面的数据都是小的、watermarks之后的时间都是大的、那么就可以去除掉这个不符合watermarks规则的事件

18. 后面的示意图里、黄色的事件时间就代表当前这个operator实际处理过的时间（就是黄色的时间戳只是记录经过的watermark的时间、可能会出现虽然处理了35的事件、但是还是黄色的33时间戳、因为只过去了33的watermark）、白色的就是事件的时间。就是它经过一个watermark的时候、代表这个时间点之前的所有事件都已经处理完了、可能处理不是顺序处理、但是都处理了

18. 对于这个例子、如果一个operator有两个输入、它还是会受限于那个时间戳慢的输入事件。水位线的目的是就是、因为这个事件可能会来得比较乱、所以要有一个规则来保证这个事件是大致有序的。（就是在这个watermark之后不会出现比这个数字更小的时间了、但是在这个watermark之前、可能会出现比这个数字更大的数字）

19. 单挑水位线出来、保证了严格有序。但是之间的值可能是乱序（相当于一个上界）

20. 这个时间窗 window应该如何去确定？

    - 第一种是按照相同的时间段（缺陷：当一个时间段内的数据量很大的时候，会导致一个时间段内的数据量很大，或者当一个时间段内的数据量很小的时候，会导致一个时间段内的数据量很小）
    - 第二种是按照相同的数据量（缺陷：会需要等待时间）

21. 管理和hadoop、yarn和spark、storm差不多、就是有worker和job manager管理。每一个worker上面有一个task manager、然后job manager会把任务分配到task manager上去。（也可以多线程地去跑等多个任务）

## Transformer

1. TextCNN、卷积本身是在找相同的特征（用相同的模式）（有小范围、中范围和大范围的，最后把这个特征给合并了，准确率比单一的好）（但是也有缺点、如果三个词的顺序发生了变化、卷积神经网络也会找不到这个特征）（而且前面的词都会对后面的词产生影响、这个初始模型太简单了）

2. 所以有了RNN。RNN的问题是很难学到长时间的依赖

3. 所以有了LSTM、会有记忆和遗忘的机制（但是还是有缺点、计算的时候必须拿到上一个的结果、就是计算效率低，只能串行）

4. 所以有了transformer、分为编码器和解码器。编码器：找到特征图，解码器，翻译这个编码内容然后输出内容结果

5. multi head attention、就是把attention的结果拆成多个头、然后再合并起来（因为自然语言处理的时候、一个词可能有多个意思、所以要多个头，多学一点出来）

6. 掩码：一开始进去的时候只有句子的开始。

7. Attention：有一个用来算自注意力的公式；每一个编码器里有一个前馈网络层和多头注意力层；这个前馈层是非线性的层（线性代数里的非线性变换，因为线性变换没有意义）

8. self-attention：就是这个it为什么指的是它这个句子里的前面的狗？有三个矩阵、查询矩阵Q、键矩阵K和值矩阵V。然后这个权重W是算出来的。最后的任何一个位置上的值、就是两个单词的相关性的值。（而且还不是一个对称矩阵、因为语言的顺序会有不一样的意思）所以Q*KT就是算的是这个东西，然后要让里面的所有的值都除以下面这个dk、防止学习的时候的梯度消失问题、然后放到softmax里、做一个归一化、最后再乘一个V矩阵。最后的Z就是注意力矩阵。

9. 多头就是把多个上面的结果合并起来。

10. 重要的是再这个Self-attention的Z1、乘相关度之后、得到了一个单词在一个句子里的特征是怎么样的

11. positional encoding：位置编码(ppt里是一个矩阵)。就是每个单词、可能单词都是一样的、但是位置不一样、所以要不同。原始嵌入值输入加上位置编码之后、再做训练。

12. 解码器就是反向得出这句话、所以会有一个mask、只能看到前面的内容。然后就是把这个编码器的输出和解码器的输出做attention、然后再做一个前馈网络层、最后输出。（就是一个负无穷大）

13. 这个transformer可以并行计算（因为是矩阵计算），解码器可以不是并行计算。

14. NLP、CV都可以用transformer。