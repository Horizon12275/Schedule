## 09 Web Services

1. 原始的前后端交互通过 restcontroller、但是 CRUD 每个方法都有一个 url pattern，这样好不好？

2. 第一个问题：是否只能 http 协议？（有很多协议、比如 smtp、ftp、sftp）还是说用 method 或者用 data 驱动？

3. 在跨语言的时候应该用什么时候调用？（同 java 语言的时候用 rmi 调用）

4. 在 www 的网络之下、仍然能访问到 alipay（在广域网上，对于 RMI 完全不可能）

5. service 不仅仅是一个 method 或者是一个 function（实现解决跨语言、操作系统、数据库的问题、通过 service 进行屏蔽）

6. REST：传递数据

7. SOAP：传递方法的调用（方法的签名、方法的参数列表）Simple Object Access Protocol（基于 xml）

8. 主要的内容在 body 里面、包含了对一个方法的描述

```java
  public interface TravelAgent {
     public String makeReservation(int cruiseID,
           int cabinID, int customerId, double price);
  }
```

9. WSDL: Web Service Description Language（描述一个 web service 的语言）, 通过 wsdl 可以生成客户端代码 （可以自动生成）

10. port 可以看作一个接口类、然后可以暴露出不同协议的方法

```xml
<message name="RequestMessage">
        <part name="cruiseId" type="xsd:int" />
        <part name="cabinId" type="xsd:int" />
        <part name="customerId" type="xsd:int" />
        <part name="price" type="xsd:double" />
</message>
<message name="ResponseMessage">
        <part name="reservationId" type="xsd:string" />
</message>
<portType name="TravelAgent">
   <operation name="makeReservation">
      <input message="titan:RequestMessage"/>
      <output message="titan:ResponseMessage"/>
   </operation>
</portType>
<!--binding element tells us which protocols and encoding styles are used -->
<binding name="TravelAgentBinding" type="titan:TravelAgent">
 <soap:binding style="rpc" transport="http://schemas.xmlsoap.org/soap/http"/>
 <operation name="makeReservation">
     <soap:operation soapAction="" />
     <input>
      <soap:body use="literal" namespace="http://www.titan.com/TravelAgent"/>
     </input>
     <output>
      <soap:body use="literal" namespace="http://www.titan.com/TravelAgent"/>
     </output>
 </operation>
</binding>
```

11. 这个 style="rpc" 也有很多种，比如 document、rpc、literal、encoded

12. 通过 binding 可以指定协议、比如 http、ftp、smtp

13. 这个 wsdl 文件有什么作用？比如说有一个接口的实现类，可以对着接口生成一个 wsdl 文件，这个文件是其他端，比如说 c# 可以访问的，然后 c# 拿到这个 wsdl 文件之后还会有一个实现类会把这个 wsdl 文件转换成一个类（这个类并不知道业务逻辑，相当于做的是封装和解析 request 和 response 消息的逻辑），然后这个类就可以调用这个接口的方法，然后通过 soap 把这个方法调用传递给 java 端，就可以通信了。

14. 但是要知道有哪些服务可用、所以会有一个第三方的注册表、比如 java 注册 wsdl、然后 c# 从注册表里面找到对应的服务。（屏蔽掉具体应用的位置）（ppt 上有图）

15. header 里面要指定为 xml type

16. 示例代码中、producer 暴露服务、consumer 调用服务

17. 有不同的方法、jaxb 和 jax-ws

18. SOAP-based Web Services （耦合度高）

- Coupling with the message format
- Coupling with the encoding of WS
- Parse and assemble SOAP （解析有开销）
- Need a WSDL to describe the details of WS（需要额外的文件去描述）
- Need a proxy generated from WSDL（需要代理去解析）

It is a time-cost way to implement Web Service with SOAP

- We should find a new way to implement WS

19. REST：REpresentational State Transfer（数据驱动）

Representational:

- All data are resources. Representation for client. （所有数据在互联网上唯一标识、可以用 uri 拿到数据）
- Each resource can have different representations （拿到的数据应该显示的是什么、怎么去呈现、由客户端决定）
- Each resource has its own unique identity(URI)

State:

- It refers to state of client. Server is stateless. （服务器端无状态）
- The representation of resource is a state of client.

Transfer: （在访问不同资源的时候、前面的状态会被传递）

- Client’s representation will be transferred when client access different resources by different URI.
- It means the states of client are also transferred.
- That is Representation State Transfer

应该叫/person/1，而不是/findPerson/1 (这个是一个动作)（怎么去改 CRUD 呢）

20. 这个 CRUD 的方法应该根据 http 的 method 来区分，比如说 get、post、put、delete

21. 所以是靠 url 做设计、然后用 method 去区分 CRUD ，这样才算是 restful，按照实例来。

22. 这个只是类似 标记，可以自定义。

23. 实现解耦、前端对修改无感知（这个和前面的 soap 作比较，前后端绑定）

24. 因为是 restful、所有返回的是 json

25. 所以对于不同语言交互的解决方法

    - 第一种是 xml 描述 request 和 response，和方法
    - 第二种是通过 http 的 crud 方法，然后返回 json

26. 对于广域网的解决方法

    - 上面两个方法就能解决。

27. alipay 是 int 到分的金额

28. Advantages:

    - Across platforms （跨平台）
      XML-based, independent of vendors
    - Self-described （自描述）
      WSDL: operations, parameters, types and return values
    - Modulization （模块化）
      Encapsulate components
    - Across Firewall （跨防火墙）
      HTTP

    Disadvantages:

    - Lower productivity
      Not suitable for stand-alone applications （单体不行）
    - Lower performance （解析有开销，同一个进程就不用 http 了）
      Parse and assembly
    - Security （安全性需要额外的机制）
      Depend on other mechanism, such as HTTP+SSL

29. 最后还讲了 ssl 的非对称加密。

## 10 Microservices & Serverless

1. 不应该将搜索结果的 key 和 result 存在 redis 里、因为会有维护一致性时的问题。

2. 前端来了 query 之后、不是马上执行的、会先把 query 变成一个 query plan、然后再执行。（可能 query1 和 query2 显式地不一样、但是生成的 query plan 是一样的，抽象语法树是一样的、才会使用 mysql cache）

3. 将 ip 和端口注册到注册中心里，然后在注册中心里通过服务名找到对应的 ip 和端口，而不是直接访问 ip 和端口。

4. 然后需要一个 gateway、通过 gateway 以 name 的方式访问服务，位置由 gateway 去查注册中心得到

5. 让 client 和其位置解耦。

6. gateway 还会有一个负载均衡的策略、比如说轮询、随机、权重、最少连接数。

7. 独立数据库可以提高容错性，但是一个流程可能需要访问到多个数据库。通信的开销变大。（性能受损）

   - 所以引入消息队列、前后端采用异步通信的方式去弥补。（只是能马上得到响应、但不是结果）
   - 也能屏蔽位置、因为通过消息队列中的 topic 通信。

8. 系统分离部件越多、越不可靠（越容易出错）

   - 所以需要事前监控、事后分析
   - 需要容错和服务降级

9. 定位问题：链路追踪

10. 其实还是从 eureka 里面拿到服务注册在里面的位置、拿到这个位置之后、再去用这个位置直接调用这个服务。

11. 任何服务默认注册进去是一个集群。

12. 前端发请求发到 gateway、gateway 通过 eureka 拿到服务的位置、然后调用服务。

13. gateway 提供统一的访问网关入口，eureka 提供和位置解耦（如果重启了，位置会变，eureka 会自动把服务注册进去，但是访问的代码不需要改动）

14. 没有 gateway 也可以、但是有了之后、只需要改写配置的 yml 文件就可以了，而不需要修改代码。

15. 函数式服务的无状态指的是什么？函数式服务为什么容易扩展？

    - 状态：一种是在内存里、比如 session、还有一种是在数据库里、比如订单状态
    - 对于有状态的、比较难维护
    - 无状态的：它的输出只与输入有关、不依赖于其他的内存里或者是数据库里的状态、所以可以很容易地扩展
    - 而且执行完一个函数之后、不会对后续的函数进行影响
    - 所以不需要知道之前执行了什么、服务器端就不需要维护状态（不需要配数据源之类的）

16. 一种方案是状态推到客户端去维护、一种是集中在服务端维护

17. 无状态的服务是事件驱动的。不需要关联服务器基础设施的状态。

18. spring cloud function 是一个函数式编程的框架、可以用来开发无状态的服务。

19. 需要包在 function 和 flux 里面、然后通过 spring cloud stream 来调用。（直接通过函数名访问）

20. 也可以通过 compose 进行函数的组合

21. 这个配置的时候、每个都需要配置一下 cloud starter eureka 包。

22. 这个参数如果是一个原生类的话、貌似在 requestbody 里面不需要用 json 格式、比如说就一个字符串、直接传就可以了。但是需要加上@requestbody

23. 这个 gateway 需要配置 cors、在配置文件里设置

24. 由于每次都会处理一次跨域、所以每一轮处理的时候会在响应头里面再加一层 cors 的头，正常情况下基于以上配置即可，但是由于目前的项目的下游微服务也配置了可以跨域的相关配置，这就导致返回的 ResponseHeader 中有多重属性，这个多重属性浏览器是不认的。所以基于此的处理方法，把下游的所有 cors 配置都取消就可以了。

25. Function 和 Flux 是来自于 Java 和 Spring 生态系统中的重要概念，特别是在响应式编程中。下面是对它们的详细解释：

    1. Function
       Function<T, R> 是 Java 8 中的一个函数式接口，它表示一个接受一个输入参数并生成一个结果的函数。在你的代码中，Function<Flux<String>, Flux<String>> 和 Function<Flux<Integer>, Flux<Integer>> 代表接受一个 Flux 类型的输入，并返回另一个 Flux 类型的输出。

    Flux<String>：表示一个异步流，可以发出 0 到 N 个字符串值。
    Flux<Integer>：表示一个异步流，可以发出 0 到 N 个整数值。 2. Flux
    Flux 是来自 Project Reactor 的一个类，表示一个响应式流，可以包含多个元素。它是响应式编程的核心概念之一，支持流式数据处理。Flux 可以处理从 0 到 N 个元素的异步序列，通常用于处理流式数据或事件。

    在你的代码中，flux.map(value -> value.toUpperCase()) 和 e.map(value -> value \* 2) 等操作是对流中每个元素进行变换的过程。

26. Flux 和 Mono 都是 Reactor 框架中的响应式类型，但它们的用途有所不同。

    主要区别：
    数据数量：

    Mono：表示零或一个元素的异步序列，适用于单一结果（如单个用户查询）。
    Flux：表示零或多个元素的异步序列，适用于多个结果（如查询多个用户）。
    使用场景：

    使用 Mono 时，你期待的是一个单一的响应，比如从数据库获取一个用户信息。
    使用 Flux 时，你可能在处理列表、流或者多项事件，比如从数据库获取所有用户或实时消息流。

27. 它这个 maven 默认是不带 web 的、所以需要加上 web 的依赖

28. 貌似这里会有一个问题、使用 redis 的时候、这个下订单的操作没法正常进行。这个错误通常表示在使用 JPA（Java Persistence API）时，您尝试删除或修改一个实体集合，但该集合的拥有者实例已经被分离（detached）或者没有正确管理。具体来说，当您使用 cascade="all-delete-orphan" 时，JPA 期望对拥有者实体（如 Book）的集合（如 cartItems）进行正确的引用和管理。

29. 这个函数计算应该在前端调用还是在后端服务里调用呢？貌似应该在前端调用啊

30. 这个函数容器在 cloud 被调用的时候、会看有没有这个容器、如果没有会马上创建一个容器、调用完之后会被马上关掉，生命周期很短

31. 或者可以把容器的操作系统层的进行保留、然后根据倒计时只更换函数层部分的代码、或者只更换语言层的环境，可以节省容器重启，做初始化环境的时间。

32. 由 spring cloud 管理需要多少个函数服务实例去处理请求（因为请求可能会有很多个）

33. 说是 gateway 和 register 不是必须的、如果重启之后位置不会变的话。

34. 因为需要通过高层的 http 协议、所以微服务的性能会受影响。

35. session 首先未必需要维护在后端、也可以维护在前端的 kvmap 和 cookie 里面。还有一种方法是通过额外的 session server 去维护

## 11 MySQL Optimization

0. 写进去的 sql 语句会通过 query plan 进行类似编译的优化、然后再进行查询，会将查询的结果存到 cache 里面。

1. 下面的一些是数据库层面的优化。

   - 存储的内容会有优化关系，比如学号应该存 Integer 还是 String，如果存 Integer，这个就会节省内存。或者是 char 和 varchar 的区别
   - 存储的索引应该建立到哪个列上面、还是建立几个列的索引？
   - InnoDB 的存储引擎是什么？不是 mysql 的原生引擎
   - 行格式的问题？比如 varchar（10），那就是 0-10 都可以，如果它里面只存了 2 bytes，那么剩下的 8 bytes 怎么处理？是应该等长的还是节省空间？或者如果学号的前缀都是相同的、是不是就可以压缩一下，是不是动态的。（空间和时间的权衡）（取决于经常发的 query 是什么样）
   - 锁级别？
   - cache 的大小？而且这里的 cache 也不是一整块的、而是分开来的。

2. 还可以做一些硬件层面的优化

   - Disk seeks. （如何寻找空闲区域）
   - Disk reading and writing. （ssd 和 hdd）
   - CPU cycles.
   - Memory bandwidth.

3. 还有 sql 语言的可移植性，需要包装

4. 索引用 B，B+树，因为读的内容大小和 pagesize 差不多，而且可以范围查询（有指向兄弟节点的指针）

5. 当更改索引指向的内容的话、需要花费额外的时间去维护索引。

6. 如果 scan 全表的时间比通过索引找到、然后再改动的时间还要少的话、那就不需要做索引，扫描一下就可以。（因为会有建索引和维护索引的开销）

7. 大多数 mysql 索引都在 B-树里面，除了：

   - 像空间索引这样的、就会用 R-trees
   - 内存表也支持哈希索引
   - 全文索引用的是反向列表

8. 聚簇索引：每个表只有一个（默认上是在主键上建立）（可以在经常做搜索的内容上面设置聚簇索引）

9. 主键优化：

   - 如果主键由多个列构成、那么会让这个索引的尺寸增长得很快。
   - 如果有空的内容作为索引、会从空的那个地方、延长出一个链表，降低索引效率，所以最好不要为空。是否为空的实现是通过最后一个 bit 的 0 和 1 来实现的、每次都会检查，会消耗时间。

10. 空间索引：

    - 把空间分为了很多个区域、对区域去做索引（八叉树）

11. 外键关联：

    - 为什么需要外键：会有多张表，之间有关联
    - 为什么会有多张表？因为一张表中每一列的详情使用频率会比其他的更高，比如 book 的信息
    - 第一个好处、可以从 hdd 上面一次读一个会被经常使用的 page
    - 第二个好处、不经常使用的数据可以存储在其他地方
    - 第三个好处、可以复用数据、冗余数据会比较少，表比较小、搜索速度就比较快（IO 更少）

12. 索引：

    - 如果这个字段的长度太长了怎么办，可以把这个前面的 n 个字符作为索引（对于 blob 和 text 是强制需要在创建索引的时候确定的）
    - 会有四种格式：
      - REDUNDANT （留下多余的空间）
      - COMPACT （压缩空闲的空间）
      - DYNAMIC
      - COMPRESSED (只存不一样的)
    - 如果 search 的内容超过了索引的长度、可以先去根据 prefix 过滤、然后扫描剩下的
    - TF 和 IDF （频率和频率的倒数）
    - LIKE 的时候也可以用到
    - 哈希索引对范围查找不太行，但是判断等或者不等很快
    - 也可以做升序降序索引，虽然只有两个字段、可以根据升降做不同的索引组合（可以有 4 种）

13. 多列索引：

    - 索引建立的顺序也会有影响，决定因素应该是到时候这个索引会被使用的方式有关
    - INDEX name (last_name,first_name)
    - 如果建立了上面这种索引，可以先 select lastname 在前面的，会利用到这种索引（ppt 上有例子，除了 lastname or firstname）

14. You can get better performance for a table and minimize storage space by using the techniques listed here:

    - Table Columns （比如 Int 和 Mediumint，或者设置为非空、因为会有额外的位和检查的开销）
    - Row Format（上面提到的四种 ROW Format，Redundant 已经不用了）（而且会有编码方式的不同、用 unicode 还是 utf8）
    - Indexes （主键应该越短越好）（一种主键是 Int 递增、8bytes，一种是 UUID，32 位 Hex。需要后面一种的原因是、因为前面保证主键递增需要额外的开销，比如再来一个线程，当用户大量的时候、就会有问题。实现是通过两个 agent，每次给 agent 分配一段的 id，比如分配 1000-1999 和 2000-2999，直到用完的时候再请求新的）（但是 UUID 是通过 IP:port:timestamp:random 的方式产生的，生成策略简单，好处是唯一的、但是坏处是太长、而且不知道插入的顺序）（维护索引节点的分裂和调整会有开销、要减少这个操作数）（分列做索引、效率不一定高、不如单列）（如果用了多列索引、那么最好把能筛选掉多的内容把它放到前面）（为什么要用 prefix 做索引、因为短的索引更快，因为读的 page 里面包含的索引更多 Shorter indexes are faster, not only because they require less disk space, but because they also give you more hits in the index cache, and thus fewer disk seeks. ）
    - Joins （如果需要对两个表做 join、而且两个表里的内容和含义和数据都是一样的、那么列名和数据类型应该相同、不然 join 之后就会慢 For example, in a table named customer, use a column name of name instead of customer_name. ）
    - Normalization（范式化、在 3 范式和 BCNF 范式里面选择，4 范式会做频繁的 join，范式低会提高维护数据一致性的开销、而且造成数据冗余）
    - 使用数值型的类型去存(Since large numeric values can be stored in fewer bytes than the corresponding strings, it is faster and **takes less memory** to transfer and compare them.If you are using numeric data, it is **faster** in many cases to access information from a database (using a live connection) than to access a text file. Information in the database is likely to be stored in a more compact format than in the text file, so accessing it involves **fewer disk accesses**. You also save code in your application because you can **avoid parsing the text file to find line and column boundaries**.)
    - 使用字符时、需要用相同的字符集（unicode、utf8）（尺寸小于 8kb，用 varchar，大于 8kb，用 blob，varchar 是存在一个 page 的相邻的内存块里、如果大于一个 page 的一半的话、就会报错，但是 blob 是存一个指针、然后在另外的地方存数据 ）（临时的内存表里面不能存 BLOB）（如果有很多记录不需要读、放到另外的表里面、保证单条记录小）

15. 对于打开的表的优化：

    - 打开的很多表里面会有默认的系统的表
    - 会有多版本的数据控制，通过 MVCC（多版本并发控制）来实现
    - table_open_cache 是打开的表的数量，根据 Max_connections 来设置(For example, for 200 concurrent running connections, specify a table cache size of at least 200 \* N, where N is the maximum number of tables per join in any of the queries which you execute. You must also reserve some extra file descriptors for temporary tables and files.)
    - If table_open_cache is set too high, MySQL may run out of file descriptors and exhibit symptoms such as refusing connections or failing to perform queries.(如果设置太高、会有文件描述符的问题)
    - 怎么做表的打开和关闭？用的不是 LRU，是把内容放到中间？（MySQL closes an unused table and removes it from the table cache under the following circumstances:When the cache is full and a thread tries to open a table that is not in the cache.When the cache contains more than table_open_cache entries and a table in the cache is no longer being used by any threads.When a table-flushing operation occurs. This happens when someone issues a FLUSH TABLES statement or executes a mysqladmin flush-tables ormysqladmin refresh command.）
    - 可以展示当前打开的表的数量（会有多版本的控制、实际没有那么多）SHOW GLOBAL STATUS LIKE 'Opened_tables’; （If the value is very large or increases rapidly, even when you have not issued many FLUSH TABLES statements, increase the table_open_cache value at server startup.）
    - 为什么会建立临时表？比如 union 的时候、会建立临时表，因为 sql 就是进行关系运算，需要额外的存储内存
    - mysql 对数据库个数和表的个数都没有上限。（但是操作系统会对目录数有限制，进而限制数据库个数）（表的大小由操作系统规定的最大文件大小限制）
    - Row Size Limits 65535 （ppt 第 51 页上的是什么意思？）（ppt p52 开始有例子，因为 text 和 blob 存的只是指针、只存 9-12 字节）（为什么 varchar 65535 直接设置是会报错的，因为还会有额外的几个 byte 去存 varchar 的它的长度信息、但是 65533 就是可以的了）（同样地、如果是可以为空的，那么也需要额外的 bit 去存储）（最后一个报错是因为 char，判断超过了页的一半，就失败了，这个貌似是规定、否则效率会低、同样的对于最后一个 row size 的例子也是一样、一行所有的不能超过半页？varchar 可以、char 不行、因为 varchar 是可以压缩的）（而且是定义的时候是可以的、但是如果真的插入 varchar 里面那么多内容、貌似也会报错）（char 是对齐的、varchar 是压缩的）
    - innodb_page_size 设置页大小、默认是 16kb。8KB 就是 65536 bits

16. Optimizing for BLOB Types
    When storing a large blob containing textual data, consider compressing it first.

For a table with several columns, to reduce memory requirements for queries that do not use the BLOB column,
consider splitting the BLOB column into a separate table and referencing it with a join query when needed.

You could put the BLOB-specific table on a different storage device or even a separate database instance.
Rather than testing for equality against a very long text string, you can store a hash of the column value in a separate column, index that column, and test the hashed value in queries.

Since hash functions can produce duplicate results for different inputs, you still include a clause AND blob_column = long_string_value in the query to guard against false matches; the performance benefit comes from the smaller, easily scanned index for the hashed values.

## 12 MySQL Optimization

1. InnoDB table 表优化:

   - OPTIMIZE TABLE : 可以让表的碎片整理（把很多小块整理成一大块），然后重建索引，提高性能。（当数据量大小稳定，而且尺寸达到一定程度很大的时候）（本质是复制碎片然后调整索引，所以会浪费时间，不能频繁做）
   - 如果主键比较长的话、那么占的磁盘空间会比较多（因为默认会在主键上面做索引）
   - 尽量用 varchar，不要用 char（因为 varchar 可以压缩）（特别是对于很多空的字段，如果空的话、就可以压缩）（除了定长和都有的字段、用 char）
   - COMPRESSED 行格式可以对有很多重复内容的数据进行压缩（比如前缀相同的、可以压缩，数值和字符串都行，字符串有重复、或者数值很接近）（但是缺点是解析会慢，所以只有当这个表的空间很大的时候才用，小的时候不用）

2. 事务管理的优化：

   - 默认每个 sql 语句都是在一个事务里面的，可以通过 autocommit 来设置多久提交一次。也可以设置 autocommmit 为 0，然后手动提交。
   - 如果一个事务里面只有类似于 select 的操作、那么 innodb 会对这个只读事务进行优化。
   - 要避免很长的事务，因为会涉及到很多的 inserting、updating 很多行，而且会有加很多锁的问题。（这个问题还不能通过重启解决、因为重启的时候会有 rollback 的操作，又加锁了）
   - 可以通过调整  buffer pool  的大小来优化性能，因为 buffer pool 是 innodb 的内存缓存，可以存储数据和索引，减少磁盘 I/O 操作。（一般是脏页到了 buffer 的 10%的时候会写、因为如果超过 10%，恢复的时候开销会很大） innodb_buffer_pool_size option.
   - 可以通过设置  innodb_change_buffering 来设置 innodb 的写入策略，可以设置为 all、inserts、none。（all 是所有的都缓存，inserts 是只缓存插入的，none 是不缓存）（而且实际业务种一般不删除，是通过标记删除的方式）
   - 避免大规模的数据操作事务，会导致数据不一致的问题
   - 也要避免长时间运行的事务、因为会有锁的问题、而且如果 non-steal、会占用很多内存（产生冲突之后不好恢复）
   - If database write performance is an issue, conduct benchmarks with the innodb_flush_method parameter set to O_DSYNC.

3. 大量数据导入的时候的优化：

   - importing data into InnoDB, turn off autocommit mode，最后统一 commit
   - 同样地、也可以关掉 UNIQUE constraints 的检查，然后最后再检查一遍
   - 外键关联也是一样、可以关掉 foreign key checks，然后最后再检查一遍
   - with auto-increment columns, set innodb_autoinc_lock_mode to 2 (interleaved 穿插) instead of 1 (consecutive 连续).（自增的数据维护在一个系统表里）（因为大量数据导入的时候、肯定不是一个线程，设置为 interveaved 的时候、当有多个线程的时候、读走的是一个区域的数值，然后分别去插入）
   - 当多行插入的时候、用 multiple-row INSERT syntax 、避免额外的通信开销

4. Queries 的优化：

   - 聚簇索引：指的是 B 树上的叶子顺序和在磁盘上的顺序是一样的，所以可以减少磁盘的读取
   - 不要让主键太长，因为会占用很多的空间在索引里面
   - 所以主键应该是经常搜索的内容，而且会涉及到批量读取的内容
   - 要根据查询的需求做复合索引（根据支持的功能），不要在每一列上做单列索引
   - 索引列本身不要包含空的值（从 null 开始一堆的链表）

5. 磁盘 IO 的优化：

   - P10 有 3 个例子
   - fsync threshold 是什么？
   - 会有不同的存储数据、如果需要随机访问、那么放在 ssd 上面、如果是顺序访问、比如 logfile、那么放在 hdd 上面（根据磁盘的特性）

   1. **识别磁盘 I/O 问题**：

      - 如果你已经按照最佳实践进行数据库设计和 SQL 操作调优，但数据库依然因磁盘 I/O 活动过重而变慢，你可能需要考虑进一步优化磁盘 I/O。
      - 使用 **Unix top** 工具或 **Windows Task Manager** 检查 CPU 使用率，如果在你的工作负载下 CPU 使用率低于 70%，说明你的数据库可能是受磁盘 I/O 限制的（即 **磁盘瓶颈**）。

   2. **增加缓冲池大小**：

      - 当表数据被缓存到 **InnoDB 缓冲池** 时，查询可以反复访问这些数据，而无需进行磁盘 I/O 操作。通过设置 **innodb_buffer_pool_size** 选项，你可以指定缓冲池的大小。
      - 增加缓冲池大小能减少磁盘 I/O，提高数据库性能。

   3. **调整刷新方法**：

      - 如果数据库的写入性能是一个问题，你可以通过将 **innodb_flush_method** 参数设置为 **O_DSYNC** 来进行性能基准测试，查看此设置是否能提升性能。

   4. **配置 fsync 阈值**：

      - 你可以使用 **innodb_fsync_threshold** 变量来定义一个阈值（以字节为单位）。
      - 指定一个阈值，强制进行较小、周期性的刷新，对于多个 MySQL 实例共享同一存储设备的情况可能特别有用。这样可以减少频繁的磁盘 I/O 操作，提高性能。

   5. **增加 I/O 容量以避免积压**：
      - 如果你注意到由于 **InnoDB 检查点操作**，吞吐量周期性下降，建议增大 **innodb_io_capacity** 配置选项的值。
      - 设置更高的 **innodb_io_capacity** 值将使得 **InnoDB** 执行更频繁的刷新操作，从而避免因工作积压而导致吞吐量下降。
      - P11 Increase I/O capacity to avoid backlogs 这个是什么，checkpoint 需要提高磁盘 io 空间？

6. DDL 操作，比如创建表、修改表结构、删除表等操作：

   - Use TRUNCATE TABLE to empty a table, not DELETE FROM tbl_name.
   - 不要修改主键、因为会涉及到很多的索引的重建

7. 内存表的优化：

   - 应该放常用的而且不重要的数据（而且只读、尽量少写）
   - 一般使用 hash index、也可以指定用 b 树，b 树对范围查找比较好，哈希对等值查找比较好

8. buffer pool 的优化：

   - 会有 chunk size 和 pool size 的区别
   - chunk size 默认是 128MB，代表 buffer pool 里面的一个块
   - 这个 buffer pool size 必须要是 chunk size 的整数倍，因为 chunk 是最小的单位
   - 可以设置多个实例、为了多个线程并行读取一个 table、减少竞争 For systems with buffer pools in the multi-gigabyte range, dividing the buffer pool into separate instances can improve concurrency, by reducing contention as different threads read and write to cached pages. （对于大的 buffer pool，可以分成多个实例，通过减少每个线程读写缓存页的竞争来提高并发性）
   - Newly read blocks are inserted into the middle of the LRU list.（新读取的块会插入到 LRU 列表的中间，一般是 3/8 的位置， 3/8 from the tail of the LRU list）
   - LRU list into two segments 也会有类似 generation 的旧代和新代的概念
   - Prefetching (Read-Ahead)来减少磁盘 io 的次数、提高性能
   - 也可以设置同时最多有几个线程去把 dirty page 写到磁盘上 innodb_page_cleaners
   - the low water mark value 10% 也可以调整
   - 缓存的内容也可以做恢复、下次再启动的时候、可以减少 warm-up 的时间（通过记录 buffer pool 里面最常用的页）

9. **多缓冲池实例的优势**：

   - 对于 **缓冲池内存大小达到多吉字节（multi-gigabyte）** 的系统，将缓冲池分成多个实例可以提高 **并发性**，即允许更多的线程同时访问缓冲池中的数据。这样做的目的是减少线程之间的 **竞争**（contention），因为多个线程可以同时访问不同的缓冲池实例，避免了在同一个缓冲池内存区域的竞争。

10. **配置多缓冲池实例**：

    - 配置多个缓冲池实例可以通过 `innodb_buffer_pool_instances` 配置选项来完成。
    - 你还可以调整 `innodb_buffer_pool_size` 来设置总的缓冲池大小。两者的配置是相辅相成的。

11. **大缓冲池的好处**：

    - 当 **InnoDB** 的缓冲池很大时，系统可以从 **内存中直接** 提供数据，减少磁盘访问。很多数据请求可以通过从内存中检索数据来得到满足，而不需要频繁地访问磁盘，从而显著提升性能。
    - 对于内存较大的系统，分割缓冲池为多个实例有助于提升并发性能。
    - 通过合理配置 `innodb_buffer_pool_instances` 和 `innodb_buffer_pool_size`，可以优化 **InnoDB** 的内存利用率和减少线程之间的竞争，进而提高数据库的处理效率。
    - 更大的缓冲池意味着更多的数据可以在内存中直接访问，减少磁盘 I/O，从而提高数据库的整体性能。

12. **启用多个缓冲池实例**：

    - 要启用多个 **InnoDB** 缓冲池实例，需要将配置选项 `innodb_buffer_pool_instances` 设置为大于 1 的值（默认为 1），最大值为 64。
    - 缓冲池实例用于管理数据库缓存，从而提高多核处理器系统的并发性能。

13. **条件要求**：

    - 该配置选项只有在你将 `innodb_buffer_pool_size` 设置为 **1GB 或更大** 时才有效。
    - `innodb_buffer_pool_size` 是指为 InnoDB 缓冲池分配的总内存大小。

14. **内存划分**：

    - 当你设置了 `innodb_buffer_pool_instances` 和 `innodb_buffer_pool_size` 后，指定的总缓冲池内存大小将会被 **平均分配** 给所有的缓冲池实例。
    - 为了实现最佳效率，建议选择 `innodb_buffer_pool_instances` 和 `innodb_buffer_pool_size` 的组合，使得每个缓冲池实例至少有 **1GB** 的内存。

15. **缓冲池扫描抗性**：  
    InnoDB 不使用严格的 LRU（最久未使用）算法，而是通过将新读入的块插入 LRU 列表的中间位置来优化缓冲池管理。这样可以确保频繁访问的“热”页面不会被替换掉，而不再访问的页面更容易被清理，从而提高缓冲池的效率。

16. **InnoDB 缓冲池预读取（Read-Ahead）**：  
    预读取是 InnoDB 通过异步 I/O 请求提前加载多个页面到缓冲池中，目的是提前准备即将需要的页面。InnoDB 使用两种预读取算法：

    - **线性预读取（Linear Read-Ahead）**：基于顺序访问的页面来预测即将需要的页面。
    - **随机预读取（Random Read-Ahead）**：基于缓冲池中已加载的页面来预测即将需要的页面，不管这些页面的访问顺序如何。  
      `SHOW ENGINE INNODB STATUS` 命令可以显示预读取算法的统计信息，帮助调优`innodb_random_read_ahead`配置。

17. **缓冲池刷新（Buffer Pool Flushing）**：  
    在 MySQL 8.0 中，InnoDB 通过后台的页面清理线程（Page Cleaner Threads）来执行缓冲池的刷新。页面清理线程将修改过的页面写回磁盘。`innodb_page_cleaners`变量控制线程数，`innodb_max_dirty_pages_pct_lwm`变量定义了脏页面达到的低水位阈值，当脏页面比例超过该阈值时，会触发刷新操作。

18. **缓冲池状态的保存和恢复**：  
    为了减少服务器重启后的热启动时间，InnoDB 会在服务器关闭时保存一部分最近使用的缓冲池页面，并在启动时恢复这些页面。通过`innodb_buffer_pool_dump_pct`配置项控制保存的页面百分比。

19. **预处理语句和存储程序的缓存**：  
    InnoDB 还会缓存预处理语句和存储程序（例如存储过程、触发器等）。`max_prepared_stmt_count`控制服务器缓存的最大预处理语句数量，而`stored_program_cache`则控制每个会话缓存的存储程序的数量。每个会话的缓存是独立的，其他会话无法访问其缓存的语句。

## 13 MySQL Backup & Recovery

1. 备份：

   - 一个是为了数据库崩溃时的恢复
   - 还有一个是防止 mysql 升级后的数据丢失

2. 备份的种类：

   - 逻辑备份：通过 sql 语句去备份（版本迁移，比较慢、适合数据量比较小的情况，可以移植到不同的机器的数据库上）
   - 物理备份：直接备份文件（适合比较大、比较重要的数据库，恢复也会比较快，不能适用版本迁移）

   - 在线备份：不停机的备份（会有不一致的问题，用户体验较好，但是要加锁）（hot）
   - 离线备份：停机的备份（会有停机的问题）（cold）
   - warm backup：只允许读，不允许写（warm）

   - 远程备份和本地备份：只是远程操控和本地操控备份操作的情况、备份的位置还是在数据库的机器上

   - 快照备份（增量式备份）：用逻辑备份备份变化的部分。（mysql 不支持快照备份）

   - incremental backup：只备份变化的部分（mysql 使用 binlog 实现）

   - Backup Scheduling, Compression, and Encryption
   - Backup scheduling is valuable for automating backup procedures.
   - Compression of backup output reduces space requirements, and
   - encryption of the output provides better security against unauthorized access of backed-up data.

3. mysql 备份方法：

   - 企业版的 mysql 有自己的备份工具
   - Innodb 用的是 hot backup，其他引擎的表用的是 warm backup
   - For InnoDB tables, it is possible to perform an online backup that takes no locks on tables using the --single-transaction option to mysqldump.
   - 也可以保存成不同格式的文件
   - 也可以关掉 binlog、但是就没法实现增量式备份了（binlog 也是 append）
   - 也可以做主从备份（然后做负载均衡、比如读操作可以到所有节点、然后写操作只到主节点）

4. **使用 MySQL Enterprise Backup 进行热备份**

   - MySQL 企业版用户可以使用 MySQL Enterprise Backup 产品对整个实例或选定的数据库、表进行物理备份。该产品支持增量备份和压缩备份功能。物理备份比 mysqldump 等逻辑备份恢复速度更快。
   - 对于 InnoDB 表，使用热备份机制进行备份。（理想情况下，InnoDB 表应占据大部分数据。）
   - 对于其他存储引擎的表，使用冷备份机制进行备份。

5. **使用 mysqldump 进行备份**

   - mysqldump 工具可以进行备份，支持备份各种表类型。对于 InnoDB 表，可以使用 `--single-transaction` 选项执行在线备份，这种方式不需要对表进行锁定。

6. **通过复制表文件进行备份**

   - 可以通过复制 MyISAM 表的文件（如 _.MYD、_.MYI 文件和相关的 \*.sdi 文件）来进行备份。为了获得一致性的备份，可以在复制表文件之前锁定并刷新相关表：
     ```
     FLUSH TABLES tbl_list WITH READ LOCK;
     ```
   - 只需要读取锁定，这样其他客户端仍然可以查询表，但在开始复制表文件之前，所有活动的索引页需要写入磁盘。
   - 也可以通过简单地复制表文件进行二进制备份，前提是服务器没有进行任何更新。但需要注意，表文件复制方法不适用于 InnoDB 表，即使服务器没有更新数据，InnoDB 可能仍然会有修改的数据缓存尚未写入磁盘。

7. **创建分隔文本文件备份**

   - 通过 `SELECT * INTO OUTFILE 'file_name' FROM tbl_name` 可以创建包含表数据的文本文件。该文件会在 MySQL 服务器主机上创建，而不是客户端主机上。需要注意，输出文件不能已存在，因为允许覆盖文件存在安全风险。
   - 这种方法适用于任何类型的数据文件，但仅保存表的数据，而不保存表的结构。
   - 另一种方法是使用 mysqldump 的 `--tab` 选项，创建包含表结构和数据的分隔文本文件。恢复时可以使用 `LOAD DATA` 或 `mysqlimport` 命令。

8. **通过启用二进制日志进行增量备份**

   - MySQL 支持使用二进制日志进行增量备份。二进制日志文件记录了自上次备份以来数据库的所有更改信息。
   - 在进行增量备份时，应使用 `FLUSH LOGS` 来轮换二进制日志。下一次进行完全备份时，您还应使用 `FLUSH LOGS` 或 `mysqldump --flush-logs` 来轮换二进制日志。

9. **使用复制服务器进行备份**

   - 如果在备份过程中服务器性能较差，您可以设置复制，并在复制服务器上进行备份，而不是在主服务器上备份。
   - 在备份复制服务器时，除了备份复制的数据库外，还应备份其连接元数据存储库和应用元数据存储库，这些信息在恢复复制服务器数据后是恢复复制所必需的。
   - 如果复制服务器正在复制 `LOAD DATA` 语句，还应备份复制服务器用于该操作的 SQL_LOAD-\* 文件，以便在恢复后继续复制任何中断的 `LOAD DATA` 操作。

10. **恢复损坏的 MyISAM 表**

    - 如果您的 MyISAM 表损坏，首先可以尝试使用 `REPAIR TABLE` 或 `myisamchk -r` 来恢复表数据。这种方法在 99.9% 的情况下都能有效恢复数据。

11. **使用文件系统快照进行备份**

    - 如果您使用的是 Veritas 文件系统，可以按照以下步骤创建备份：
      - 从客户端程序中执行 `FLUSH TABLES WITH READ LOCK`，该命令会锁定表以防止数据更新。
      - 从另一个终端执行 `mount vxfs snapshot`，挂载文件系统快照。
      - 在客户端中执行 `UNLOCK TABLES`，解除表的锁定。
      - 从快照中复制文件。
      - 卸载快照。
    - 其他文件系统（如 LVM 或 ZFS）也可能提供类似的快照功能，可以使用相似的步骤进行备份。

12. 数据库恢复方法：

    - For cases of operating system crashes or power failures, we can assume that MySQL's disk data is available after a restart.（因为有 log，可以直接恢复）
    - For the cases of file system crashes or hardware problems, we can assume that the MySQL disk data is not available after a restart.（所以会需要备份）
    - This backup operation acquires a global read lock on all tables at the beginning of the dump.（备份策略：备份的时候会加锁，这里 ppt 上有一个例子）
    - 对于没有被截断的 binlog 文件，要用 mysqlbinlog gbichot2-bin.000009 ... | mysql 里的 ...，说明还没有被截断
    - log 文件和数据文件可以分别存在不同的硬盘上、这样就避免了一个硬盘坏了之后、数据和日志都丢失的问题
    - tab-delimited text 用 tab 分隔的文本文件

13. 可以用 merkle 树来存储哈希码，然后通过对比这棵树来判断保存和恢复的数据是否一致（dump 和 recovery 的时候同时建树）

14. 下面的这种方法进行多线程地恢复是不对的，因为可能第一个 binlog 会创建一个临时表、然后第二个 binlog 会用到这个临时表，但是这个临时表在 binlog001 运行完之后、会被删除，就会在第二个 binlog 里报“unknown table”

```
shell> mysqlbinlog binlog.000001 | mysql -u root -p # DANGER!!
shell> mysqlbinlog binlog.000002 | mysql -u root -p # DANGER!!
```

应该用

```
shell> mysqlbinlog binlog.000001 binlog.000002 | mysql -u root -p
```

7. 也可以指定时间点去做 binlog 的恢复（可以跳过 file 中的一些操作）

8. 做完 full backup 时候、会把先前的所有的 binlog 都删除，然后重新开始

9. binlog 为了系统崩溃的时候恢复。

10. **备份策略总结**  
    在操作系统崩溃或电源故障的情况下，InnoDB 本身会负责数据恢复。但为了确保可以放心，建议遵循以下指南：

11. **启用二进制日志**  
    始终启用二进制日志（MySQL 8.0 的默认设置）。二进制日志有助于记录所有的数据库更改，从而支持恢复和数据复制等功能。

12. **磁盘负载平衡**  
    如果您有安全的存储介质，这种技术对于磁盘负载平衡也有帮助，从而提升性能。

13. **定期进行全备份**  
    使用 `mysqldump` 命令进行定期的全量备份，这种备份方式是在线的、非阻塞的。

14. **定期进行增量备份**  
    通过 `FLUSH LOGS` 或 `mysqladmin flush-logs` 命令定期进行增量备份。

15. **考虑使用 MySQL Shell 转储工具**  
    使用 MySQL Shell 的转储工具提供并行转储、多线程、文件压缩和进度信息显示等功能，还支持 Oracle 云基础设施对象存储流媒体以及 MySQL 数据库服务的兼容性检查和修改。  
    转储文件可以轻松导入到 MySQL Server 实例或 MySQL 数据库服务 DB 系统中。

16. **转储文件的多种用途**  
    转储文件可以用于：

    - 数据丢失时的数据恢复备份。
    - 设置复制的源数据。
    - 实验用途：
      - 创建数据库的副本，用于不改变原始数据的测试。
      - 测试潜在的升级兼容性问题。

17. **mysqldump 的输出格式**  
    `mysqldump` 产生两种类型的输出，取决于是否使用了 `--tab` 选项：

    - **没有 `--tab` 选项时**，`mysqldump` 将 SQL 语句写入标准输出。这些输出包括创建数据库、表、存储程序等的 `CREATE` 语句，以及将数据加载到表中的 `INSERT` 语句。输出可以保存在文件中，稍后使用 `mysql` 命令重新加载，以重新创建转储的对象。
    - **使用 `--tab` 选项时**，`mysqldump` 为每个转储的表产生两个输出文件。服务器会将一个文件写为制表符分隔的文本，每行一个表行，文件名为 `tbl_name.txt`，另一个文件是包含 `CREATE TABLE` 语句的 SQL 文件，文件名为 `tbl_name.sql`。

18. **如何复制数据库**
    - 如何从一个服务器复制数据库到另一个服务器。
    - 如何转储存储程序（存储过程和函数、触发器和事件）。
    - 如何分别转储数据库的定义和数据。
19. **备份和恢复单个数据库**：

    - 使用 `mysqldump` 导出数据库内容：`shell> mysqldump db1 > dump.sql`
    - 创建一个新的数据库：`shell> mysqladmin create db2`
    - 导入备份文件到新数据库：`shell> mysql db2 < dump.sql`
    - 不要在 `mysqldump` 命令中使用 `--databases`，因为这会导致在备份文件中包含 `USE db1`，会覆盖在 `mysql` 命令行中指定的数据库。

20. **从一个服务器到另一个服务器复制数据库**：

    - 在服务器 1 上：
      - 使用 `--databases` 选项导出数据库：`shell> mysqldump --databases db1 > dump.sql`
      - 将 `dump.sql` 文件从服务器 1 复制到服务器 2。
    - 在服务器 2 上：
      - 使用 `mysql < dump.sql` 导入数据。

    另一种方式是：

    - 在服务器 1 上：
      - 不使用 `--databases` 选项：`shell> mysqldump db1 > dump.sql`
    - 在服务器 2 上：
      - 创建数据库：`shell> mysqladmin create db1`
      - 导入数据：`shell> mysql db1 < dump.sql`

21. **导出存储程序（存储过程、函数、触发器、事件）**：

    - 使用以下选项来控制 `mysqldump` 如何处理存储程序：
      - `--events`：导出事件调度器事件。
      - `--routines`：导出存储过程和函数。
      - `--triggers`：导出表的触发器。
    - 默认情况下，`--triggers` 选项是启用的，其他选项需要显式指定来导出相应的对象。
    - 如果不希望导出某些内容，可以使用其跳过形式：`--skip-events`、`--skip-routines` 或 `--skip-triggers`。

22. **分别导出表结构和数据**：

    - 使用 `--no-data` 选项，只导出表的结构（不包含数据）：`shell> mysqldump --no-data test > dump-defs.sql`
    - 使用 `--no-create-info` 选项，只导出表的数据（不包含结构）：`shell> mysqldump --no-create-info test > dump-data.sql`
    - 如果需要导出表结构并包含存储程序和事件定义，可以使用以下命令：
      - `shell> mysqldump --no-data --routines --events test > dump-defs.sql`

23. **使用 `mysqldump` 测试 MySQL 升级兼容性**：

    - 在进行 MySQL 升级时，建议先在新版本的服务器上安装，并从当前生产环境中导出数据库定义，导入新服务器以检查兼容性（也适用于测试降级）。
      - 在生产服务器上：`shell> mysqldump --all-databases --no-data --routines --events > dump-defs.sql`
      - 在升级后的服务器上：`shell> mysql < dump-defs.sql`
    - 因为备份文件不包含表数据，可以很快处理，帮助快速检测潜在的不兼容问题。

24. **使用 `mysqldump` 测试升级数据加载**：

    - 确保数据库定义没有问题后，接下来可以导出数据并尝试将其加载到升级后的服务器：
      - 在生产服务器上：`shell> mysqldump --all-databases --no-create-info > dump-data.sql`
      - 在升级后的服务器上：`shell> mysql < dump-data.sql`
    - 导入数据后，可以检查表内容并运行一些测试查询。

25. **Point-in-time recovery（时间点恢复）**：
    时间点恢复是指恢复数据到某个指定的时间点，通常在恢复了一个完整的备份后进行，该备份将服务器恢复到备份时的状态。然后，时间点恢复通过增量的方式将服务器恢复到从完整备份时间到更近时间的状态。

26. **使用二进制日志进行时间点恢复**：
    要从二进制日志恢复数据，必须知道当前二进制日志文件的名称和位置。可以使用以下命令查看：

    ```
    mysql> SHOW BINARY LOGS;
    ```

    要确定当前二进制日志文件的名称，可以执行：

    ```
    mysql> SHOW MASTER STATUS;
    ```

27. **应用二进制日志中的事件**：
    要应用二进制日志中的事件，可以使用 `mysqlbinlog` 输出通过 `mysql` 客户端处理：

    ```
    shell> mysqlbinlog binlog_files | mysql -u root -p
    ```

28. **处理加密的二进制日志**：
    从 MySQL 8.0.14 版本开始，二进制日志可以被加密。在这种情况下，`mysqlbinlog` 不能像上述示例那样直接读取加密的日志，但可以使用 `--read-from-remote-server` 选项通过服务器读取。例如：

    ```
    shell> mysqlbinlog --read-from-remote-server --host=host_name --port=3306 --user=root --password --ssl-mode=required binlog_files | mysql -u root -p
    ```

29. **查看二进制日志中的事件**：
    要查看日志中的事件，可以将 `mysqlbinlog` 输出传递到分页程序中：

    ```
    shell> mysqlbinlog binlog_files | more
    ```

    或者将输出保存到一个文件中，并用文本编辑器查看：

    ```
    shell> mysqlbinlog binlog_files > tmpfile
    shell> ... edit tmpfile ...
    ```

30. **编辑日志文件并执行**：
    将输出保存到文件中是处理日志的预备步骤，特别是当需要删除某些事件（如意外的 `DROP TABLE`）时。你可以从文件中删除不想执行的语句，编辑完成后，通过以下命令应用文件内容：

    ```
    shell> mysql -u root -p < tmpfile
    ```

31. **多个二进制日志的安全恢复**：
    如果有多个二进制日志要应用，安全的方法是使用单个连接来处理它们。以下是一个可能不安全的示例：

    ```
    shell> mysqlbinlog binlog.000001 | mysql -u root -p # 危险!!
    shell> mysqlbinlog binlog.000002 | mysql -u root -p # 危险!!
    ```

    这种方式存在问题，如果第一个日志文件中包含 `CREATE TEMPORARY TABLE` 语句，而第二个日志文件包含使用该临时表的语句，当第一个 `mysql` 进程终止时，服务器会丢弃临时表。第二个 `mysql` 进程尝试使用该表时，服务器会报告“未知的表”。

32. **恢复最后的完整备份**：  
    假设你在某个特定时间点（例如 2020 年 3 月 11 日 20:06:00）执行了删除表的 SQL 语句，你需要进行时间点恢复来恢复到该时间点之前的数据库状态。  
    首先，恢复在该时间点之前创建的最后一个完整备份（假设时间点是 `tp`，即 2020 年 3 月 11 日 20:06:00）。恢复完成后，记下恢复到的二进制日志位置，以便稍后使用，并重启服务器。

33. **查找精确的二进制日志事件位置**：  
    接下来，需要找到对应时间点（`tp`）的二进制日志事件位置。在这个例子中，假设你知道大致的时间（`tp`），可以通过使用 `mysqlbinlog` 工具来检查该时间附近的日志内容，找到精确的事件位置。  
    使用 `--start-datetime` 和 `--stop-datetime` 选项来指定一个短时间段，围绕 `tp` 时间点进行查询。然后在输出的日志中查找相关的事件（如删除表的 SQL 语句）。

34. **应用二进制日志中的事件到服务器**：  
    首先，将步骤 1 中找到的二进制日志位置（假设是 155）作为起始位置，步骤 2 中找到的、在你感兴趣的时间点（例如 `tp`）之前的位置（假设是 232）作为结束位置，将二进制日志事件应用到服务器。  
    使用 `mysqlbinlog` 工具，指定 `--start-position` 和 `--stop-position`，例如：

    ```shell
    mysqlbinlog --start-position=155 --stop-position=232 /var/lib/mysql/bin.123456 | mysql -u root -p
    ```

    执行该命令后，你的数据库将恢复到感兴趣的时间点 `tp`，即在删除 `pets.cats` 表之前的状态。

35. **恢复时间点之后的所有语句**：  
    如果你还希望重新执行恢复后的时间点 `tp` 之后的所有 SQL 语句，可以再次使用 `mysqlbinlog` 工具，将时间点之后的所有事件应用到服务器。  
    在步骤 2 中，我们注意到在需要跳过的语句后，日志的位置为 355，可以使用这个位置作为 `--start-position` 参数，包含所有此位置之后的语句。例如：
    ```shell
    mysqlbinlog --start-position=355 /var/lib/mysql/bin.123456 | mysql -u root -p
    ```
    这样可以将时间点 `tp` 之后的所有事件重新应用到服务器。

## 14 MySQL Partitioning

1. 所有数据库都有类似的问题、所以解决方法也有通用性

2. 一个表在一个机器上放不下、所以分区

3. 是不是放不下了才去分？也不是、分区做搜索、会把搜索的范围限制到一个区域里，即缩小了、所以搜索会更快

4. 分区：单个的一张表的不同部分可以分布在不同的机器上存储

5. 都是水平分区支持，mysql 垂直分区不支持（不如变成两张表、然后做外键关联，而且也会需要水平分区）

6. 有了分区之后、也可以逐个增加分区

7. 也可以指定分区进行搜索以及其他操作

8. 有四种分区的方式：

   - RANGE 分区：根据某个列的范围进行分区（连续的值）
   - LIST 分区：根据某个列的值进行分区（离散的值，eg 名字）
   - HASH 分区：根据某个列的 hash 值进行分区
   - KEY 分区：根据某个列的 hash 值进行分区（except that only one or more columns to be evaluated are supplied）

9. range partition：

   - （按照小于来表达，因为如果是限制左右范围的话、那么删去一个分区、就会有很多的分区不连续，而且会有重叠）
   - 也可以不用 less than maxvalue、相当于做了一个筛选
   - 这个 less than value 要严格升序（比如在 UNIX_TIMESTAMP 里面统一的时候），不然就会报错
   - 多列 range partition 也是可以的，是按照列的顺序来依次判断的
   - 如果有 6-8-aaa 会存在哪里？

10. List partition：

    - 是根据 id 的值来分区的，如果 id 的值不在、插入的时候会报错
    - 也可以在单次插入多个值的时候、用 ignore 来忽略掉不在的值，避免报错

11. 列的字段

    - range 不支持：浮点数、TEXT and BLOB columns
    - 使用 RANGE COLUMN 指定多行

12. hash partition：

    - 可以指定 hash 的方法，比如指定日期里的年份做 hash
    - 也有一个 LINEAR HASH 的方法，即不是用取余，而是用一个 2 的幂次方来做 hash，为了就是当有分区变化的时候、变化和调整小一些（一个公式、ppt 有例子、会分布得比较均匀）

13. key partition：

    - 这个 hash 函数没法指定、是 mysql 自己决定的

14. 可以做 subpartition，比如在一个分区里按照日期再分一次

    - 每一个分区里面的子分区也可以不一样，也可以给对应的子分区的名字

15. 设置分区也会让删除更快，可以直接把一个分区 drop 掉了

16. 解决 null 的值

    - 空值 null 会存到 range 的最小的分区里面
    - 可以把 null 加到某个分区 list 里面
    - hash 的时候会把这个值看成 0

17. 分区管理

    - 可以改变分区方式
    - 可以指定操作的分区
    - 分区的数字需要有意义、比如对应有意义的年份、对应经常的查询操作等、不然意义不大
    - 追加 range 分区只能加在最后的分区那边、放在前面会报错、因为会涉及到很多数据的转移（但是也可以指定重新组织 reorganize table）
    - 新加 list 分区不能有重复的值
    - hash 和 key 可以合并分区、但是不能删除分区（但是合并的数值不能超过原来的数值）
    - 这个分区和表是可以双向进行替换的（结构要一样、没有外键关联，分区条件要满足）（也可以 without validation，结果是啥应该是就是插入了）

18. 也有 maintenance partitions，用来解决分区里的数值的问题

19. 分区的逻辑要和查询的业务相关、比如如果要查整 5 整 10 的 sql、那么就要按照这个来分区（因为 query plan 的优化）

## 15 NoSQL & MongoDB

1. 为什么要非关系型数据库、因为数据量太大了

2. mysql 处理结构化数据，而 nosql 处理非结构化数据

3. 有结构化数据、半结构化数据和非结构化数据

4. ppt 上有传统关系型数据库和 map-reduce 的区别

   - 也可以会有 dynamic schema，动态调整结构和字段
   - map reduce 对分布式做了优化、锁竞争被减弱了、所以可以达到线性的扩展

5. mongodb 是文档型数据库

   - 支持全索引
   - 通过备份提供高可用性
   - 有 auto-sharding，可以自动做的（类似于分区）
   - 就地修改会快

6. 文档 document 的定义

   - A document is the basic unit of data for MongoDB
   - A collection can be thought of as the schema-free equivalent of a table, the documents in the same collection could have different shapes or types. （就相当于在一个文档中的数据的结构可以不一样，很多文档构成了一个 collection，是 schema-free 的）
   - 默认在每个文档都有的一个特殊的键\_id 上建索引（id 是唯一表示的，但是不是主键的概念，因为在数据库表里有很多候选键）
   - 很多 collection 构成了一个 database

7. 文档的特点

   - 他里面的键值对是有序的，然后不同的序的数据可以存在一个 collection 里面
   - 里面的数据是类型敏感和大小写敏感的
   - 里面的 key 不能重复
   - schema-free

8. 与 mysql 的优势、比如对于下面这个数据、在 mysql 里面就要定义两张表，然后用外键关联，所以在一个 collection 里面可以表示两个外键关联的表的关系、所以形式比较简单

```json
{
  "name": "John Doe",
  "address": {
    "street": "123 Park Street",
    "city": "Anytown",
    "state": "NY"
  }
}
```

9. collection

   - 所有的数据不能存在一个 collection 里面，首先会更快，而且可以利用数据局部性、只拿出自己想要的数据、避免拿出数据的时候拿出多余的不想要的数据，然后和索引也有关系
   - subcollection 相当于对字段做了分割，但是两个 subcollection 直接没有直接的关系（比如外键关联）

10. database

    - admin 是 root database、用来鉴权
    - local: This database will never be replicated and can be used to store any collections that should be local to a single server.
    - config 的服务器会做这个操作、当某个请求来了、根据\_id 的范围来去哪个 sharding 的服务器上找对应的数据

11. mongoDB 里面有一个聚集操作、可以做流水线式的操作、即有不同的操作阶段

    - $unwind 做摊平操作

12. 注解与查询字段绑定：@Param("name") 注解用于明确告诉 Spring Data，将方法参数 name 映射到查询中的对应字段值（在 MongoDB 查询中，如果你通过 findByLastName 查询，则会根据 lastName 字段进行查询）。

```java
public interface PersonRepository extends MongoRepository<Person, String> {

    List<Person> findByLastName(@Param("name") String name);
    List<Person> findByFirstName(@Param("name") String name);

}
```

13. 下面的@Transient 注解用于告诉 Spring JPA，不要将这个字段持久化到数据库中

```java
    private PersonIcon icon;
    @Transient
    public PersonIcon getPersonIcon(){
        return icon;
    }

    public void setIcon(PersonIcon icon) {
        this.icon = icon;
    }
```

14. 这里把图片存到 mongoDB 里面只是为了演示、实际上用 blob 也可以

15. 有一个 normal index，还有一个 geospatial index，就可以根据坐标的范围来求最近点之类的内容了，但是这里的$maxdistance 代表的是一个单位，就是需要提前设置这个搜索的单位精度，然后根据这个单位来搜索

16. **创建地理空间索引**：  
    可以使用 `createIndex` 函数创建地理空间索引，但要传递 `"2d"` 作为值，而不是 1 或 -1。  
    这意味着索引类型为二维地理空间索引。

17. **改变默认精度**：  
    要改变默认的精度，可以在创建二维索引时指定 `bits` 值。  
    你可以指定一个介于 1 和 32 之间的 `bits` 值（包含 1 和 32）。  
    `bits` 值越大，索引的精度越高，查询的效率也可能有所变化。

18. 这里有一个删除索引的事情？回放？

19. 这个貌似有一个就是不加自定义语句的话、就是按照经纬度去计算（貌似只要是一对数值就可以自动进行计算了）、如果加了、就按照二维坐标系里的距离去计算（相当于是二维索引的范围搜索）

20. 注解里的 fields 可以用来做结果的返回的过滤

21. $nearSphere 也可以不止是一个球、比如说正方形之类的

22. 进一步地、这个非关系型数据库就可以解决下面这种很多种枚举的情况、不可能枚举完、而且也不能在关系型数据库里用很多张表去维护（相当于 schema 不一样的时候、做某些操作、比如说搜索的时候、即使有不存在的字段、也不会报错）

```sql
INSERT INTO `mobile_params` (`id`, `mobile_id`, `name`, `value`) VALUES
   (1, 1, 'Standby time', '200'),
   (2, 1, 'Screen', 'OLED'),
   (3, 1, 'Quality', 'SSS'),
   (4, 2, 'Standby time', '300'),
   (5, 2, 'Screen', 'Curve'),
   (6, 2, 'Price', 'Attractive');
```

20. 貌似这个 repository interface 只能返回对应管理的类，而不是自定义的什么比如字符串的返回值

21. 在这段代码中，我们首先通过 ID 查找用户，如果找到则更新用户的 email 字段，并保存回数据库。这种方式会替换整个文档，但如果仅修改了一个字段，MongoDB 会进行内部优化以只更新变化的部分。

22. 我服了、搞了半天原来是数据源里的数据库的名称没配置对。。。应该是 mongodb://localhost:27017/ebookstore

23. 通过 spring 操作完之后、里面多了一个 class 是表示这个对象的类类型信息，它是 Spring 框架（特别是 Spring Data 或类似的框架）序列化对象时添加的额外字段。

```json
{
  "id": 1,
  "description": "This book provides a comprehensive introduction to the field of computer science. It covers topics such as algorithms, data structures, programming languages, and computer architecture. The book is suitable for beginners and does not require any prior knowledge of programming. 123123",
  "class": "com.example.backend.entity.BookInfo"
}
```

24. 这里下完订单保存的时候、会把原来的 description 给覆盖掉、需要修改

25. sharding，把一个大的 collection 切成很多小的 chunks，然后可以分布到不同的服务器上（autosharding 是自动的）

    - 然后会按照索引在不同的 chunk 里找（索引也是 B+树）
    - 会使用一个 router 来找到对应的 chunk（router 在 mongodb 里的）
    - 客户端里的 driver 会先和 router mongos 联系、根据 router 里的 config 找到对应的集群里的 mongod

26. 什么时候需要做 shard

    - 磁盘空间不够了
    - 要增加写数据的速度（比单一个 mongod 能承受的）（在这个 nosql 的情况下、写是一次性写入的）
    - 也有可能是内存不够用，需要怎加机器作为集群

27. 增加机器的时候、会有一个 balancer 来做数据的迁移（到不同的服务器上）

    - 因为一个 shard 里面会有不同的 chunk
    - 如何实现的：就是 router 会去 primary db 里面找一个 config、这个 config 保存了其中对应的每个 chunk 保存的\_id key 的 range，当每个 chunk 超过一定大小、比如 128MB 的时候、就会切开这个 chunk
    - 当每个 shard 直接的 chunk 树相差小于等于 2 的时候可以接受、如果超过了、就会自动迁移（是不是 2 是可以配置的，也可以把这个自动分配关掉）
    - 均匀分布的意义在于：数据量访问的频繁次数，前提是数据是对等的（每个数据的访问量差不多）（反例就是新闻，光靠数据量均匀、不能提高性能）（所以在这种情况下可以根据数据访问的情况去做均衡）
    - client 是不知道这个的，无感知的

28. **分片是 MongoDB 的扩展方式**：  
    分片是 MongoDB 用来扩展处理能力的方式，它允许你通过添加更多机器来应对不断增加的负载和数据量，而不影响应用程序的运行。

29. **手动分片的挑战**：  
    手动分片虽然能起作用，但在添加或移除节点，或者数据分布和负载模式发生变化时，会变得难以维护。

30. **自动分片的优势**：  
    MongoDB 支持自动分片（autosharding），这减少了手动分片所带来的一些管理难题。

31. **MongoDB 分片的基本概念**：  
    MongoDB 的分片概念是将集合（collections）分成小块（chunks）。这些小块便于分布在多个分片上。

32. **应用程序和路由器的工作方式**：  
    我们不需要知道每个分片上存储了哪些数据，因此在应用程序前面运行一个路由器。路由器知道数据的位置，应用程序可以正常连接到它并发出请求。

33. **路由器如何工作**：  
    应用程序会连接到一个普通的 `mongod` 实例，路由器知道哪个分片存储了哪些数据，它会将请求转发到适当的分片。

34. **分片的适用情况**：  
    一般来说，应该从非分片配置开始，当有需要时再转换为分片配置。适合分片的情况包括：

    - 当前机器的磁盘空间已用尽。
    - 你希望比单个 `mongod` 实例处理数据的速度更快。
    - 你希望将更多的数据保存在内存中以提高性能。

35. **分片键的选择**：  
    在设置分片时，你需要选择一个集合中的键，并使用该键的值来拆分数据。这个键被称为**分片键（shard key）**。

36. **分片键示例**：  
    例如，如果选择 "name" 作为分片键，那么一个分片可能存储名字以 A–F 开头的文档，另一个分片存储名字以 G–P 开头的文档，最后一个分片存储名字以 Q–Z 开头的文档。

37. **动态平衡数据**：  
    当添加或移除分片时，MongoDB 会重新平衡数据，使每个分片的流量和数据量合理分配。

38. **分片和数据块**：  
    假设添加一个新的分片。一旦该分片启动并运行，MongoDB 会将集合拆分成两个数据块（chunks）。

39. **数据块的定义**：  
    一个数据块包含了某个范围的分片键值对应的所有文档。例如，如果使用 "timestamp" 作为分片键，一个数据块可能包含所有时间戳在-∞ 到 2003 年 6 月 26 日之间的文档，另一个数据块则包含时间戳在 2003 年 6 月 27 日到 ∞ 之间的文档。

## 16 Neo4J & Graph Computing

1. 图就是节点和边的集合，每个节点也可以有不同的含义，边也是

2. 这个 graph DBMS 引擎类似的，会有两层

   - The underlying storage：用于存储结构之类的
   - The processing engine：用于处理查询的优化之类的

3. 问题：在关系型数据库里缺乏 relationship 的表示，需要做多次 join，而且 join 会有很多的性能问题（费时）（还是可以做的），在 ppt 的例子里、反过来做、如果问谁买了土豆、可能更难做。（join 的频繁会导致很差的性能，对应于 ppt 上的第二个例子，即使对于简单的需求，也需要做多次 join）

4. nosql 也缺乏 relationship 的表示、对于 ppt 上的例子，如果正向还是比较好做的、但是如果反向了就不好做了（比如要知道谁买了一个 item ，必须得扫）

5. 图数据库的区别：如果找单个节点的所有关系、只需要从一个节点开始、然后沿着边找就可以了（不需要 join）（就不需要访问所有的 nodes 和所有的边）

6. A labeled property graph is made up of nodes, relationships, properties, and labels.

   - Nodes contain properties.
   - nodes 上面是可以有 label 属性的. Labels group nodes together, and indicate the roles they play within the dataset.
   - A relationship always has a direction, a single name, and a start node and an end node

7. 图查询语言：Cypher

   - ASCII 的表示：(emil:Person {name:'Emil'}) <-[:KNOWS]-(jim:Person {name:'Jim'}) -[:KNOWS]->(ian:Person {name:'Ian'}) -[:KNOWS]->(emil)
   - 图的描述方式不唯一
   - 基于上面这个用 cypher 描述图的方式、就可以用 cypher 去进行 query

```
MATCH (a:Person)-[:KNOWS]->(b)-[:KNOWS]->(c), (a)-[:KNOWS]->(c)
WHERE a.name = 'Jim'
RETURN b, c
```

8. 对于 ppt 上的例子，如果用关系型数据库来表示的话，会有需要不同张表示一对多关系这些的表

```
MATCH (user:User)-[*1..5]-(asset:Asset) //通过1条到5条边找到所有的 asset
WHERE user.name = 'User 3' AND asset.status = 'down' RETURN DISTINCT asset //返回所有的user3可以访问的down的asset
```

9. 对于上面的查询语句、会把上面转化成下面这样的很多的搜索路径(比关系型数据库里的很多 join 操作要好)

```
(user)-[:USER_OF]->(app)
(user)-[:USER_OF]->(app)-[:USES]->(database)
(user)-[:USER_OF]->(app)-[:USES]->(database)-[:SLAVE_OF]->(another-database)
(user)-[:USER_OF]->(app)-[:RUNS_ON]->(vm)
(user)-[:USER_OF]->(app)-[:RUNS_ON]->(vm)-[:HOSTED_BY]->(server)
(user)-[:USER_OF]->(app)-[:RUNS_ON]->(vm)-[:HOSTED_BY]->(server)-[:IN]->(rack)
(user)-[:USER_OF]->(app)-[:RUNS_ON]->(vm)-[:HOSTED_BY]->(server)-[:IN]->(rack) <-[:IN]-(load-balancer)
```

9. Use nodes to represent entities—that is, the things in our domain that are of interest to us, and which can be labeled and grouped.

   - Use relationships both to express the connections between entities and to establish semantic context for each entity, thereby structuring the domain.
   - Use node properties to represent entity attributes, plus any necessary entity metadata, such as timestamps, version numbers, etc.

10. 有一个例子、比如说可以做一个简单的推荐系统、比如说一本书被几个读者所喜欢、可以给其中一个读者推荐喜欢这本书的其他人的看过的其他书推荐给这个读者

11. neo4j 可以作为嵌入式进程地去跑、也可也作为一个独立的服务器进程地去跑

    - 可以做分布式存储，也可以做主从备份

12. 所有节点和边都是等长的

    - node ：15 bytes，里面只存第一条边
    - relationship ： 34 bytes：里面存了节点的下一条边和这条边的属性
    - 见 ppt 上有一个存储结构的例子

13. 需要注意的是、这里的 neo4j 类里的 Id 注解应该是从 neo4j 包里导入的，而不是从 Jakarta persistence 里面导入的、不然会在启动的时候报错找不到对应的 neo4j 主键的错误

14. 貌似这里有几页 ppt 没听，可以听一下回放，貌似后面的都是一些例子
