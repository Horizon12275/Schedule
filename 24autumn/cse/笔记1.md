## 02 A High Scalability Web Server

1. 特点：

   - 高请求量
   - 大量数据
   - transparent scale（隐藏用户规模）

2. 处理每个请求的所耗的时间是不一样的（ai 时代中），推理所耗的时间会长

3. LAMP 架构不能 scale，只能在单台服务器上

4. scalability 1：解耦，把不同的功能分开，不同的功能用不同的服务器来处理，比如数据库服务器，文件服务器，web 服务器

5. scalability 2：缓存，把一些数据缓存起来，比如 page cache，memcached（专门的缓存服务器，用内存来存）memecahed 可以支持在多台服务器上，可以 scale。一个经典的方法就是把 key hash 到不同的服务器上。但是原始的 hash 在 scale 的时候，会造成很多 miss。解决方法是 consistent hashing

6. scalability 3：更多的 app server，比如用负载均衡器来分发请求

7. scalability 4：数据库分库和分表

8. scalability 5：文件系统的扩展，比如一个很大的文件、可以存在不同的服务器上，然后用一个文件系统来管理这些文件。eg. GFS，NFS，HDFS

9. scalability 6：用 CDN，把静态文件放到 CDN 上，CDN 会根据用户的位置，选择最近的服务器来提供服务

10. scalability 7：分开不同的 application 和业务+分布式计算。

11. 分布式系统：A distributed system is a collection of independent computers that appears to its users as a single coherent system.

12. 数据中心的规模。rack -> row -> data center

13. 出错：用户体验到的错误。随着 scale，出错的概率会增加。出错的原因：硬件故障，软件 bug，网络故障，人为错误

14. 例子：unreliable network 的很多原因，见 ppt

## 03 & 04 From single to distributed: inode-based File System & CAP

1. 分布式系统要给人一种一直在线的感觉，即使有一台服务器挂了，用户也不会感觉到。

2. Availabity: 高可用 （x nines）

   - 3-nines = 99.9% = 8.76h/year
   - 5-nines = 99.999% = 5.26m/year
   - 7-nines = 99.99999% = 3.15s/year

3. reliability: 高可靠性

   - 1. MTTF (Mean Time To Failure): 平均故障间隔时间
   - 2. MTTR (Mean Time To Repair): 平均修复时间
   - 3. MTBF (Mean Time Between Failure): 平均故障间隔时间
   - 4. MTTF + MTTR = MTBF

4. 达成高可用：

   - Replication: 相同的副本，比如多个文件服务器
     - 挑战：一致性
   - Handling failures via retry: 重启系统，重新发送请求
     - 挑战：What about consistency? Must made trade-off

5. CAP theorem: 一个分布式系统，最多只能满足三个特性中的两个

   - Consistency: 所有的节点看到的数据是一样的
   - Availability: 每个请求都能得到一个响应
   - Partition tolerance: 系统在网络分区的情况下，仍然能够工作

6. single-node inode-based file system

   - inode: 一个文件的元数据，包括文件名，文件大小，文件权限，文件的地址等
   - block: 文件的内容
   - 一个文件的 inode 会指向多个 block，一个 block 会指向下一个 block，形成一个链表
   - 一个目录的 inode 会指向多个文件的 inode

7. 文件是持久化的、也是有名字的

8. driver 和应用对文件的读写的 api 是不一样的。应用需要更高层的抽象。driver 只可以抽象出 block 的读写。系统层需要在基于硬盘的 block 读写的 api 上，提供文件的读写的 api。

9. naive file system：用 sector index 和 append 和 reallocate 操作。但是这样会有很多问题，比如文件的大小会有限制，文件的读写会很慢，文件的删除会很慢，文件名的查找不可行，文件的权限控制不可行和安全不可行。

10. 所以有了系统层对文件系统的抽象、比如 unix 文件系统的 api

11. 文件系统有很多层。

12. L1 : Block Layer:把 sector 变成 block，一个文件由很多 block 组成。能保证文件系统可以在不同大小的 sector 的硬盘上工作，保证 block 大小相同就行。

13. 如何选择合适大小的 block？

    - 太小：会有很多 block，会有很多 seek，会很慢
    - 太大：会有很多内部碎片，会浪费空间
    - tradeoff = 4KB

14. boot block: 保存文件系统的引导信息

15. super block（每个文件系统一个）: 保存文件系统的元数据，比如 block 的大小，block 的数量，inode 的数量，inode 的大小

16. bitmap: 保存 block 的使用情况

    - 1byte = 8bit，一个 bit 代表一个 block 的使用情况

```c
struct inode
   integer size
   integer block_nums[N]
   integer type
   integer refcnt
```

17. L2 :file layer:

    - inode，一个文件的元数据 (index node),记录了哪个 blocks 属于哪个文件
    - 我们把 inode 存在哪里？因为每个 block 都有大小限制，所以可以把一个父的 inode block 中的几个 index 指向另外几个 indirect block，这样就可以存更多的 block 了
    - 和页表的区别：inode 头上的几个 block 可以只访问一次就可以访问到。但是页表的话，要跳转几次才能访问到。而且文件头上的数据的访问比较常见。

18. L3 : inode Number Layer：Mapping inode number -> inode

    - inode table: 保存 inode number
    - 可以再存一个 inode table 的 table，提高资源利用率

19. L4 :file name layer：Mapping file name -> inode number

    - directory entry: 保存文件名和 inode number 的对应关系
    - mapping table is saved in directory
    - 但是需要遍历
    - 当前运行的目录同样是一个文件
    - 目录的厚度由存储的文件的文件名的长度决定
    - 目录里面记录的是一个字符串到一个 inode number 的映射

20. L5 :所以有了 path name layer：Mapping path name -> inode number

21. links：读一次 path 就可以读到文件

    - 存在多个引用的情况、所以用 refcnt 来记录引用次数，保证正常增删，文件的 refcnt 为 0 的时候就删掉
    - 但是如果文件夹结果有环的话，就会有问题
    - 不允许有环的话、就不允许 link 到 directory（除了.和..）

22. 文件重命名

    - UNLINK(to_name)
    - LINK(from_name, to_name)
    - UNLINK(from_name)
    - 需要原子操作、否则如果在中间出错、就会有问题

23. ppt 中、每一行机器代码都是一个 ext4_dir_entry

24. L6 : Absolution Path Layer

    - 因为会有多个用户，所以需要绝对路径
    - 引入了绝对路径，根目录是 /
    - 根目录下的/.和/..都是指向自己的

25. 例子：找到"/programs/pong.c"

    - 从 super block 找到 inode table 的 block
    - 找到根目录的 inode：根目录的 inode number 是 1，是固定的、因为不能通过递归的方式找到根目录
    - inode table 中的 number 存的就是 block 的 number
    - block 中文件的 number 存的是 inode table 的 number
    - 最终找到文件在 61 的 block 中
    - 一个 block 可以是一个 directory 或者是一个 file

26. 也可以创建硬连接 LINK，但是 naive 的方法、没法在多个文件系统中使用，比如一台电脑上插了两个硬盘

27. L7 : Symbolic Link Layer

    - 可以跨文件系统，即跨磁盘
    - ppt 中的例子：即使/tmp/abc 还不存在、但是可以创建一个指向/tmp/abc 的 symbolic link
    - 但是 cat s-link 会报错，因为/tmp/abc 不存在
    - 创建了之后、再 cat s-link 就可以查看/tmp/abc 中的内容了
    - ls 中的 8 代表一个 symbolic link 的大小（就是保存了/tmp/abc 的路径长度）

28. 所以有了两种连接：hard link 和 symbolic link（就是 soft link）

    - hard link：文件名和 inode number 的映射（通过 inode number 找到文件）
    - symbolic link：文件名和路径的映射

29. Context Change

    - /CSE-web -> /Scholarly/programs/www
    - cd /CSE-web
    - cd ..
    - 这样会回到根目录 / 这是一个 bash 的优化（feature）
    - 只有 cd -P .. 才会回到/Scholarly/programs

30. 文件名是不是文件的一部分？

    - 一个文件的文件名不属于这个文件
    - 因为这个字符串不保存在这个文件的 block 中
    - 一个文件的文件名是保存在他的目录中的
    - name is data of a directory ， and metadata of a file system
    - 元数据是什么？就是文件的属性，比如文件的大小、文件的权限、文件的创建时间等

31. 每一个 hard link 都是等价的

32. directory size 都是很小的（见上原因）

33. 文件系统要实现的 API，包装之后实现为 system call

34. open 和 fopen 的区别（后续都是基于 linux 的 open）：

    - open 是系统调用，fopen 是库函数
    - open 返回的是文件描述符，fopen 返回的是文件指针
    - open 是低级 I/O，fopen 是高级 I/O
    - open 是 POSIX 标准，fopen 是 C 标准
    - open 只能用在 Unix/Linux 系统，fopen 可以用在大多数系统
    - fopen 会有缓冲区，open 不会，所以 fopen 会有更好的性能

35. 真实的 inode 还需要加一点内容

```c
struct inode
    integer size
    integer block_nums[N]
    integer type
    integer refcnt

    integer userid
    integer groupid
    integer mode
    integer atime // last access time （by read）
    integer mtime // last modify time （by write）
    integer ctime // last change time （by LINK）
```

    - 为什么要有ctime呢？要记录文件权限的变化，所以要有 ctime（比如说 chmod的时候）

36. fd : 文件描述符

    - 0: stdin
    - 1: stdout
    - 2: stderr

37. 为什么要这么设计 fd 呢？

    - 选项 1：OS 可以返回一个 inode pointer
    - 选项 2：OS 可以返回文件的所有 block numbers
    - 但是为了 Security（用户永远不能访问到系统的 data structure），和 Non-bypassability（不能绕过系统的安全机制），所以用 fd
    - 又是一层抽象，能控制一个进程没法访问其他的文件

38. file cursor：每个 fd 都有一个 cursor，记录文件的读写位置。进程和他的子进程会共享这个 cursor（即从同一个位置读写）

    - 为什么要这么设计？什么时候合理？要写的时候是合理的、如果不共享一个 cursor，就会覆盖
    - 写东西的时候最好只有一个写者，single writer principle

39. fd_table：记录了所有的 fd,里面有 fd 和 index，index 指向 file table 中的一条

40. file table：记录了所有的 file。其中有 inode num，file cursor，file refcnt 等。

41. 关掉一个 fd 之后、新的 fd 是当前可用的 fd 中最小的那一个（导致不能做并行）

42. ppt 上有一个 disk 的例子图片，还有一个 open read 的时序图的例子

43. 会默认加一个参数 -noatime。

44. 例子 1

    - read 里面的 write、是要写 inode 的 atime

45. 例子 2

    - create 里面、read write inode bitmap 是因为要创建一个文件的写

    - bar inode 里面第一个 read 是因为要先读再写（粒度是 4K）

    - foo 的 inode 第二个写：是因为自己也有 atime，然后更新目录的大小

    - 写里面为什么要读 databitmap，要先读、找到一个空的 block，然后写

    - bar inode 的第二次写、更新 metadata

46. 三种顺序哪种好？

    - Update block bitmap, write new data, update inode (size and pointer)
    - 第一种的顺序稍好、无非是浪费磁盘空间，而且可以监测出来（先扫一遍 inode，再和 bitmap 对比一下，如果一个 block 没有被任何 inode 指向，但是 bitmap 上是 1，就是浪费的，可以回收）（问题不是那么严重）
    - （第二种）要避免数据泄漏：如果新的指针给到了上一次删除的数据，但是在 write data 之前断电了，就会泄漏数据。
    - （第三种）两个 inode 可能指向了同一块 bitmap，动态数据泄漏，也很不安全

47. SYNC

    - 数据不落盘，好处是快、坏处是断电了之后、数据就没了
    - 所以需要 sync，把数据落盘

48. delete after open but before close

```
一个进程已打开文件：
    当一个进程打开文件时，操作系统为该文件创建一个文件描述符，并增加其引用计数。

另一个进程删除文件：
    在Unix/Linux等类Unix系统中，删除文件实际上是从目录中移除该文件的名称，而不是立即删除文件的内容。此时，文件的引用计数会减少。

通过移除指向文件的最后一个名称：
    如果删除了文件的最后一个链接（例如，文件名），引用计数会变为0。

引用计数现在为0：
    这意味着没有任何进程通过文件名再访问这个文件，但文件的实际数据仍然存在，因为某个进程仍然打开着它。

inode在第一个进程调用CLOSE之前不会被释放：
    在Unix/Linux系统中，文件的inode（包含文件元数据的信息）不会被释放，直到所有打开该文件的进程都关闭它。这是为了保持数据的完整性。

在Windows上，可能会禁止删除打开的文件：
    不同于Unix/Linux，Windows系统通常不允许删除正在被任何进程打开的文件。这样做是为了防止数据损坏或不一致性。

在打开后但在关闭之前删除：
    即使在Unix/Linux系统中，文件可以被删除，但只要有进程持有它的文件描述符，文件的数据仍然可以被访问，直到所有相关的文件描述符被关闭。
```

48. renaming

    - 原来的先删除的方法、不行、因为如果断电、那么 a.txt 就没了
    - 所以要用原子性的操作
    - mv 的操作、只改目录的内容、不改文件的内容（inode 不变）

49. 复杂系统的 M.A.L.H 原则

    - M: Modularity 模块化
    - A: Abstraction 抽象
    - L: Layering 分层（每一层只和相邻的层交互）
    - H: Hierarchy 层次化（组合成一个整体）

## 05 Remote Procedure Call

1. 开始的例子：一个占用多的硬盘，性能可能不如占用少的硬盘（inode）

2. 在不改变应用程序的情况下（open，read，write），把文件系统放到远程服务器上，通过网络访问，节省本地的硬盘空间（即 without coding the details for the remote interaction）

3. 例子：本地调用服务器上的 GET_TIME() 函数，CALL(GET_TIME())，返回结果。call 里面的参数基本用宏，因为要快。

4. 因为不知道用的是大端还是小端，所以传参之前需要先 convert2external。network byte order 是大端。（逻辑如此、但是这个实现改了代码）

5. 发现这些为每个函数处理这个 rpc 的实现都是差不多的、所以可以根据宏来生成代码。

6. 所以有了 stub。保证了应用层代码的一致性。Stub: hide communication details from up-level code, so that up-level code does not change.

7. Client stub

   - Put the arguments into a request
   - Send the request to the server
   - Wait for a response

8. Service stub

   - Wait for a message
   - Get the parameters from the request
   - Call a procedure according to the parameters (e.g. GET_TIME)
   - Put the result into a response
   - Send the response to the client

9. inside a message:

   - Service ID(function ID)
   - Service parameters(function parameters)
   - using marshal/unmarshal to convert data to/from network byte order (Marshal 和 Unmarshal 本身是很费时的，重点在于内存的拷贝)

   “marshal” 和 “unmarshal” 是计算机科学中常用的术语，特别是在数据序列化和反序列化的上下文中。以下是这两个词的含义及其在网络字节顺序转换中的应用：

   ### Marshal

   - **定义**：将数据结构（如对象或数据类型）转换为一种适合存储或传输的格式。这种格式通常是二进制或文本格式，以便在网络上传输或保存到文件中。

   - **用途**：在网络编程中，marshal 通常是指将数据转换为网络字节顺序（通常是大端字节序），以确保在不同平台之间的一致性。

   ### Unmarshal

   - **定义**：与 marshal 相对，将已序列化的数据转换回原始的数据结构。这个过程通常涉及从网络字节顺序转换回主机字节顺序（可能是小端字节序）。

   - **用途**：当接收到数据时，需要 unmarshal 将接收到的字节流转换为易于处理的数据结构。

10. RPC request （in 1984）:

    - Xid （transaction ID，比如读内存，发包）（client reply dispath uses xid, client remembers the xid of each all）
    - call/reply （它是一个 call 还是一个 reply 的 request）
    - rpc version （新旧版本的兼容性）
    - program # （program number：即二进制文件的号码，# 表示 number）
    - program version （即二进制文件的版本号）
    - procedure # （即调用二进制文件的哪个函数）
    - auth stuff
    - arguments

11. RPC reply message:

    - Xid
    - call/reply
    - accepted? (Yes, or No due to bad RPC version, auth failure, etc.) （是否由于 RPC 版本不对、auth 失败等原因）
    - auth stuff
    - success? (Yes, or No due to bad prog/proc #, etc.)（是否由于 prog/proc # 不对等原因）
    - results

12. 通过 binding sever and client，建立连接。核心是要找到 function ID，然后调用。

13. 接下来就是怎么传递数据和参数了（无论是参数还是返回值）。一开始有 pass by value 和 pass by reference 的区别。在本地都是可以的。

14. 怎么传一个变量的指针呢，而且要求是对的？让不同机器上的虚拟地址空间不重叠。然后当发生 page fault 的时候，发出一个远端的 handler。但是这样会有很多问题，比如读完写同步的情况。所以有指针会复杂。但是可以在多个显示器上有用，只有一个显示器能写，其他只能读。访问一台机器上的内存和硬盘（DSM）（distributed shared memory）（但是如果一块内存要在很多机器上共享，就会有很多问题，比如 cache 一致性问题，所以应用不多）

15. 还要考虑大端和小端的问题。以及服务器 32 位和 64 位的问题，如果客户端的位数和服务器的位数不一样，比如 int 的位数就不一样。还有 float 的表达也不一样。还有比如数据类型对齐的要求不一样。

16. 还有更多的挑战。比如兼容性的问题，backward compatibility 和 forward compatibility。

17. 然后是数据格式。比如 json 有一定缺陷，比如传的数字会有二义性；或者传二进制字符串的时候会有问题。所以要用 xml 存更多的 type 信息。

18. binary formats 性能更好（工业）

    - IDL： interface definition language

19. each field has:

    - type annotation
    - type field
    - a length indication(optional for string, list etc.)
    - data

20. 可以做压缩、比如压缩 type 和 tag 和 length

21. rpc 也会使用到不同的数据传输协议

22. rpc 遇到错误的时候需要处理

23. How RPC handles failures?
    - Depends on the semantic: at-most-once, at-least-once & exactly-once
    - What makes failure handling simpler? Idempotence
    - (幂等性：一个函数调用多次和调用一次的效果是一样的)

## 06 Distributed File System （DFS）：NFS & GFS

1. NFS, GFS 即可以在本地像访问本地文件一样访问远程文件

2. upload & download 的好处：简单

   - 问题 1：客户端只需要其中小部分的数据，但是要下载整个文件
   - 问题 2：如果文件很大，本地客户端的硬盘可能不够用
   - 问题 3：一致性问题，比如一个文件被多个客户端同时修改

3. 一种实现方式：Remote access model （通过 RPC 的方式，访问远程文件）

   - 问题：Possible server and network problem (e.g., congestion)
   - Servers are accessed for duration of file access
   - Same data may be requested repeatedly

4. NFS: network file system 设计目标
   Any machine can be a client or a server
   Support diskless workstations
   Support Heterogeneous deployment ( 支持不同的硬件、操作系统、底层文件系统)
   Different HW, OS, underlying file system
   Access transparency
   Use remote access model
   Recovery from failure
   Stateless, UDP, client retries
   High performance
   Use caching and read-ahead

5. Sun 的实现：挑选了部分的 api 使用 RPC （有些做了修改）

6. 比如 OPEN 和 CLOSE 被删掉了

7. 还有，和之前的 fd 不同，这里引入了 fh（file handler）

8. 然后多了一个 offset

9. NFS client 在本地。

10. 这里 READ（fh，0，n 的时候）不需要传 buffer、因为 buffer 是一个指针。

11. 为什么不提供 open、而是提供 lookup？

    - 因为如果用了 open、那么 RPC 就是 stateful 了。就要考虑 performant 和 scalable
      的问题了。
    - 比如如果 server 挂了、再重启，那么 open 就做不了了。（因为 open 的 context
      是存在内存里的）
    - 如果有多个 client 连到了同一个 server？（所以就要维护很多的 context）
    - 因此 stateless 的 RPC 的可扩展性好

12. 所以要带 offset、这样 read 就是 stateless 的 api 了（否则要维护 cursor）

13. stateless handle failure 的时候、也不能简单地只使用 at least once 策略、因为
    如果有多个 client 的话、遇到网络问题的时候、会冲突。

14. 什么是一个 file handler？是指明要访问服务器上的哪个文件

    - 我们不能用 fd、因为 server 没有 file table
    - 我们能用 path name 吗？其实可以、但是 NFS 没有用，因为会有一个问题：
      用 path name 的时候，如果
      两个 client rename after open 的情况。（会读到另外的文件夹里）
    - 能不能用 inode number 呢？也不行。会有一个 delete After Open 的问题
      会读到被删除的文件里（单机不会出问题、因为多机没有维护额外信息）
      （有一种补救方法，让 program2 知道这个文件已经被删掉了）
    - 所以未了避免这种情况范数、引入了一个 generation number。每个 inode 号除了
      自己的 inode 号之外、还有一个 generation number。
      fh 就包括了 inode number 和 generation number
    - 如果 read 的 fh 和当前的 generation number 不一样，就会返回一个错误（abort）

15. 还有一个问题，就是性能问题

    - 分布式文件系统不一定比单机的慢（看网速和磁盘速度，目前网速会大于磁盘速度）
    - 一种优化：cache at Client

16. cache 最大的难点、coherence 一致性问题

    - close to open consistency:解决方法是 close 的时候，把 cache 数据写回服务器

    - read/write consistency：server 和 client 存 timestamp、定期询问并更新（30s）

17. 还有一种优化就是 transfer data in large chunks（8KB default）

18. 还有就是 read ahead（prefetch，提取读未来可能用到的数据）

19. nfs 的缺陷

    - 它只能利用单台服务器上的磁盘资源，有磁盘数上限
    - 如果这台服务器挂了、那么所有的文件都不能访问了
    - 这个性能会被限制在单个文件和单个网络带宽上

20. 进一步扩张机器数（重新设计文件层）： L1：Distributed block layer

    - 把 block_id 延申为<mac_id,block_id>
    - 一个 client 如何知道哪台机器有 free block？
      - 第一种方法，随机直到找到
      - 用一台 master server 去记录哪台机器有 free block，然后所有的
        分配和释放都通过 master server 进行

21. L2：File Layer 不用改

22. L3 ：distributed inode number Layer

    - 把 inode table 存在 master 上面

23. L4 ：file name Layer 也不用改

24. L5 ：path name Layer 也不用改

25. 这个 naive design 目前还有问题

    - 比如性能：现在需要多个 RTT 找
    - 可靠性：如果挂了、还是访问不了
    - 正确性：如果发送异常，会对整个系统产生影响

26. 解决方法：

    - 可以用 data replication 解决 performance 的问题（不过会影响正确性）

27. 例子：GFS（Google File System）

    - 设计初衷：传统的文件系统没法满足业务了
    - 设计场景：文件很大、而且很多文件被 appended
    - 要解决：scalability，Fault-tolerant，performance

28. Interface（其中要关注 append，因为保证 append 的一致性比 write 简单）

29. 文件系统就是一个大的 byte array 然后把文件分成很多的 chunk

30. 每个 chunk 在不同的 chunk server 上（备份）（chunks size 64MB 很大）

31. chunk 很大、减少和 master 的通信次数。

32. chunk 变大、也能减少 tcp 连接的连接数（重新建立 tcp 连接很耗时，每个 linux 系统对支持的 tcp 连接的个数有上限限制）

33. chunk 变大、master 需要存储的文件的元数据就会变少，就能存更多文件

34. 总结：GFS 和 naive 的对比

    - Master stores current locations of chunks (blocks)
    - Data are stored in large chunks
    - Chunks are replicated for fault tolerance & high performance

35. 为什么 GFS 只用一个 master？

    - 简单，在大部分时间够用

36. GFS 用了 GFS adopts a relaxed consistency model 来保证写一致

37. name-to-chunk mapping 是存在 memory 又持久化在 an operation log 中的

38. GFS 没有目录结构

## 07 Google file system & from file system to key-value store

1. client 如何和 GFS 交互

   - no caching data at client nor server
   - cache metadata at client

2. 读的情况类似，但是写不一样。

3. 写的时候会带来不一致的问题、一个读没有办法读到写（读了不同的 chunk？）

4. GFS 的解决结果最终实现了 chunk 最终内容都是一致的、但是读不一定保证读到最新的（relaxed consistency model）

5. master 会判断哪个 chunkserver 是 primary 的

   - primary can request extensions of the lease 因为可能 primary 会挂

6. write in GFS

   - 阶段 1：传输数据，通过链传递的方式在 chunkserver 之间传递数据（数据流）
   - 阶段 2 ：写入数据，写入数据到 chunkserver，先写 primary，然后写 secondary（广播）（控制流）

7. 判断 chunk 的复制是不是旧的，通过 chunk version number

8. 并发写的问题在 GFS 中问题并不大、因为多数文件是 append 的（并发写不会被覆盖掉）

9. HDFS : Hadoop Distributed FS(架构和 GFS 一样)

10. NFS cant scale，is not fault-tolerant，is not high performance

11. Key-value store 的 workload 和 file system 的 workload 有不同，kv 更多是小数据、file system 更多是大数据

12. naive 用文件系统存储 kv store 的问题

    - 会造成多余的系统调用
    - 空间利用率不高

13. 为什么 Why KV builds upon the file system?

    - We still need a system to interact with the disk hardware!

14. Idea: using one or few file to store key-value store data

    - Different key-values can pack into the same disk block
    - Reduce system call overhead: open file once, serve all the requests then

15. - cpu <-> memory : 100ns
    - memory <-> disk : 10ms

16. - HDD performance (for reference)
    - Sequential: 100 – 200 MB/s
    - Random: 0.5 – 2 MB
    - SSD performance (fore reference)
    - Sequential: 2,000 – 3,500 MB
    - Random: 200 – 300 MB

17. update 变成对日志里的 append 顺序写

18. delete 使用 null entry、然后定期清理

19. 所以叫 log-structured file, 是一种 append-only 的

20. index：使用 offset

21. 但是存在内存里的 kv 和磁盘里的 kv 是不一样的

22. cuckoo hashing 避免了 naive hash 的 list 过长的问题，保证了 get 在 O(1)时间内完成，但是 put 的时候会慢

23. 解决 cuckoo hash 不够的方法：不够的时候扩充或者 rehash，循环踢

24. 解决这个文件永远增长的问题：compaction
    - 或者 compaction with segmentation（让 compact 变快）
    - 然后再 merge，否则一个文件的浪费的空间就会变多
