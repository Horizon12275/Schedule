## 16 Multi-Paxos, Raft & NewSQL Consistency across replicas

8. 后面讲的就是 raft 了

   - paxos 是 bottom-up 的（先保证单值）
   - raft 是 top-down 的（直接解决多个 log 一致的问题、不用先解决单值一致的问题）
   - raft 和 paxos 是等价的协议
   - 一个值被 majority 了、也会在 raft 里被改

9. raft 的分解：

   - Leader election
     - Select one server as the leader
     - Detect crashes, choose new leader
   - Log replication (normal operation)
     - Leader accepts commands from clients, append to its log
     - Leader replicates its log to other servers (overwrites inconsistencies)
   - Safety
     - Keep logs consistent
     - Only servers with up-to-date logs can become the leader

10. raft：通过状态机 RSM+log

    - 三种状态：leader、follower 和 candidate
    - Leader: handles all client interactions, log replication
      - Invariant achieved: At most 1 viable leader at a time
    - Follower: passive (only responds to incoming RPCs)
    - Candidate: used to elect a new leader
    - 所有 log 只能由 leader 能做（限制），每个 term 里面只有一个 leader，leader 挂了会选一个新的

11. raft 通过 heartbeat 来决定是否需要 election

    - 如果收到了大多数 server 的投票、就变成 leader
    - 每个 server 只能投一票
    - 也是有一个 term 号、和 paxos 里的 proposal 号类似
    - 如果没人赢、就重试

12. election 需要 saftey 和 liveness

    - liveness：有可能从 candidate 里面永远选不出：解决方法：用一个随机 timeout 的 heartbeat
    - 每台机器只需要记住 currentTerm 和 votedFor（对于 election）

13. Log entry = index, term, command （log[]）

    - commit 就是把 log 输入到 statemachine 里
    - crash 的时候会造成 log 不一样的问题
    - 所以有两步走：先把这个 log 变成大概率是一样的、然后在这些大概率是一样的 log 的里面去做 consistency

14. 它保证的是一个 prefix 的 coherence

    - 就是如果 prefix 一样的话、才 appendEntries
    - 如果 prefix 不一样的话、会由 leader 发起进行一个覆盖的操作
    - 如果有很多不一致、所以就会递归覆盖、就是全部覆盖
    - 所以会导致 majority 写了、还是最后被覆盖了

15. 如何保证一个 log 被 commit 之后不被 overwrite 呢？

    - 可以避免一个比较少的 log server 变成 leader（选 leader 的时候加限制就行了，要有最新的 term 和 index 数，先按照 term 比、再按照 index 比）
    - 但是 ppt 上有一个例子、就是会覆盖掉 majority 的值，但是也有解决方法

## 17 Introduction to Network

1.  OSI, TCP/IP & Protocol Stack

    - OSI 有 7 层
    - TCP/IP 有 4 层（最常用）
    - CSE 只 care 3 层

2.  Layers in Network

    1. Application

    - Can be thought of as a fourth layer
    - Not part of the network

    2. End-to-end layer

    - Everything else required to provide a comfortable application interface （解决数据传输出问题的问题）

    3. Network layer

    - Forwarding data through intermediate points to the place it is wanted (找节点的邻居，怎么找到一条线连接到目标节点)

    4. Link layer

    - Moving data directly from one point to another （两个节点直接相连）

3.  更关注的是速度，不保证发出的包一定能收到

4.  因为这个包的数据是往前增长的、所以会用 socket buffer 预留前面的空间

5.  transport layer

    - 为了建立多个连接、所以会有 port（共 65535 个）
    - 握手的时候的 seq number 是根据直到当前发的数据决定的、所以可以防止第三方进入

6.  The Link Layer

    The bottom-most layer of the three layers
    Purpose: moving data directly from one physical location to another

    1. Physical transmission
    2. Multiplexing the link
    3. Framing bits & bit sequences
    4. Detecting transmission errors
    5. Providing a useful interface to the up layer

7.  传输数据的方法：

    1. 用 shared clock，时钟上升沿的时候就去数据线上读数据（但是需要很精确，容错空间小、数据的周期也需要和时钟一样）（只在 cpu 内部这么传）
    2. 不用 shared clock、可以用异步传输（ppt 过程，一开始 ready 和 ack 是 0）（不需要对表、而且采样不会出错、因为 ready 了才读数据）（缺点是 2Δt 才能传一个 bit）（解决方法：一次传 N 个 bit，parallel transmission）
    3. serial transmission（usb 使用）（有时候这个波形会变形，所以会需要使用压控震荡器，能恢复出信号对应的时钟周期）（问题在于有时候这个信号没有变化、所以需要编码）

8.  曼彻斯特编码：

    - 0 -> 01
    - 1 -> 10
    - 还有一个 8B10B 编码（8 bit -> 5 + 3 bit）（10 bit -> 6 + 4 bit）
    - 缺点：数据率只有 50%

9.  如何共享一个连接

    - Isochronous：对一条数据线分时复用（好处就是这个 8bit frame 是预留的）（坏处是这根线上只能有 703 个同时的 conversations，因为带宽有限）
    - 所以会有一个 multipexing/demultiplexing 的过程，可以把包放在一个 buffer queue 里面（单纯加内存不能解决问题、因为会导致等待的时间变长、然后就会 timeout、然后会重新发包、然后会导致时延变长）（所以丢包是一个好的选择）
    - 还有一个 Frame and Packet: Asynchronous Link 的方法，用发包的方式进行数据传输

10. 包头和包尾的作用：识别什么时候开始和结束一个 frame

    - Simple method
    - Choose a pattern of bits, e.g., 7 one-bits in a row, as a frame-separator
    - Bit stuffing: if data contains 6 ones in a row, then add an extra bit (0) （需要对数据做额外处理、如果数据里面有 6 个 1，就加一个 0）

11. Error Detection

    - 用一个 checksum 来检测错误
    - 最好也能够纠错：所以需要冗余
    - hamming distance：两个 bit 之间的距离（两个 bit 之间的差异的数量），可以通过 hamming distance + simple parity check 来监测错误
    - 4bit -> 7bit 中为什么要在 1，2，4 位上，因为这样就能够推出 3，5，6，7 位的错误的位置（为了电路设计）（不过错了两位就没办法了）

## 18 Network Layer All about routing

1. 网络传输中的所有数据不是同样重要的、比如包头信息会更重要

2. The Network Layer

   - Network attachment points 网络接入的节点（AP）
   - Network address
   - Source & destination

3. Managing the Forwarding Table: Routing

   - Static routing vs. adaptive routing 就是找到一条路线，而且要考虑到路线上面节点的状态

4. IP Route Table

   - 落在不同的网络段、就从哪个网络口出去
   - 里面存的是下一跳的地址+端口？

5. Control-plane VS. Data-plane

   - Control-plane：如何构造这张表（和性能相关度不是特别大，要求 adaptive 就可以了）
   - Data-plane：如何通过这张表去转发数据（性能相关度很大）

6. 每台路由器上的路由表都是不一样的，会更多关注 local 的节点

7. Distributed Routing: 3 Steps in General

   - Nodes learn about their neighbors via the HELLO protocol

   - Nodes learn about other reachable nodes via advertisements

   - Nodes determine the minimum-cost routes (of the routes they know about)

8. Two Types of Routing Protocol

   - link-state routing 一个是 flooding 所有的 neighbor 状态
   - Distance-vector Routing 第二个是只告诉 Nodes advertise to neighbors with their cost to all known nodes

9. W 是还没有处理过的节点

   - 然后讲了一下 link-state routing 就是用了 Dijkstra 算法去找最短路径
   - flood 对 failure 的容忍还是比较高的（因为是全都发）
   - 但是 flooding 的 overhead 很高、因为会充满这个邻居数据的包

10. Distance-vector Routing

    - 每个节点收到自己 neighbor 的 advertisement
    - A’s neighbors do not forward A’s advertisements; they do send advertisements of their own to A
    - 相当于 bellman-ford 算法
    - 但是对 failure 的容忍度不高、因为只是告诉邻居
    - limits 是 failure handling

11. 有一个 infinity 的概念、就是这个节点不可达的时候、会不停地给邻居发值、导致这个值变成无穷大

    - 通过 split horizon 的方法来解决这个问题，即这个节点不会把来自源的信息再发给源

12. 上面这个 Distance Vector 和 Link-State Summary 的方法都只适用于比较小的网络、因为会有很多的 overhead，怎么 scale 呢？

13. 3 Ways to Scale

    - Path-vector Routing

      Advertisements include the path, to better detect routing loops

    - Hierarchy of Routing

      Route between regions, and then within a region

    - Topological Addressing

      Assign addresses in contiguous blocks to make advertisements smaller

14. Path Vector Exchange

    - 网络中的每个节点都维护一个 path vector
    - 也是通过 advertising 进行更新

15. 这里的例子是关于站在 G 节点的视角的

    - 通过 path vector 的方法、可以降低收敛的时间
    - 也提到了几个对 path vector 的问题的处理方法

16. 通过 region 的方法做 Hierarchical Address

    - 即相当于做 local 的 routing，然后再做 global 的 routing（层次结构）
    - 可以大大压缩路由表的大小
    - 但是缺点就是更加复杂了、而且需要把 region 和 ip（地址） 进行绑定了（因为需要通过 ip 快速定位到 region）
    - 也可以有很多层次的 region

17. Routing Hierarchy

    - 跨 region 的路由需要用到 BGP （Border Gateway Protocol）

18. Topological Addressing

    - 类似于掩码的逻辑 18.0.0.0/24（CIDR notation）
    - CIDR: Classless Inter Domain Routing
    - 就可以用一条去表达很多条
    - 为了 scale：所以要让路由表变小

19. 上面讲的都是 control-plane，下面讲的是 data-plane

20. data-plane 里面只需要考虑单机了

    - net_packet.destination != MY_NETWORK_ADDRESS 的话、才需要 link_send
    - 如果收到一个不是自己的包、那自己可能就是路由器（就需要把包转发啊）
    - 一共有四条路径：收包的时候、可能是自己的或者不是自己的、发包的时候、也可能是自己的或者不是自己的

21. forward 一个包的时候、需要做很多事情

    - 先查表
    - TTL：在网上能够通过多少个路由器转发、防止无限循环，如果 TTL 是 0 的话、就把这个包丢掉
    - 更新包的 checksum（因为要减少 TTL）（有专门的硬件进行加速）
    - 然后再转发

22. Data-plane Case Study: Intel's DPDK

    - 是 bypass 了 kernel
    - RX 和 TX 分别代表了收包和发包
    - 是通过轮询 hardware queue 的方式去做的、就没有 kernel 的介入（很快，没有中断）

23. RouteBricks

    - 用了很多个便宜的网卡机来代替了一个昂贵的路由器
    - 降低了成本

24. NAT (Network Address Translation)

    - 就是在发送的时候、把自己发出去的包的内网 ip + port 翻译成一个公网 ip （相当于是 router 的 ip）
    - 然后在接受的时候、再把这个公网 ip 翻译成内网 ip + port
    - 其实不太好、因为在网络层用到了不是网络层的 port 的信息、破坏了层和层之间的封装性
    - NAT 是有限个的，如果 10 个笔记本连一个路由器、最多每个分到十分之一个 NAT
    - 对于 IP-set，不能经过 NAT、因为是加密的
    - ftp 也不能经过 NAT（其实也破坏了封装性的设计）

## 19 End-to-end Layer Best-effort is not enough

1. 以太网 Ethernet

   - 需要监测一个人发包的时候没有其他人在发包

2. 发包的时候、拓扑结构有两大类

   - Hub：所有的包都会发给所有的人（广播）（意味着可以做监听）
   - Switch：会通过路径的调转，只会发给目标的人（不会广播）

3. 例子里的上面这些 block 都是 ip 地址、下面的 ethernet 是 mac 地址

   - 在 ip 层发的时候、也会在包头包含 ip 的 source 和 destination 的地址
   - 到 ethernet 层、通过 mac 地址进行传输（相当于在包头填了一个 mac 地址）
   - 但是会有一个现象、就是 mac 地址是可以重复的，所以会有不同的设备有同样的 mac 地址
   - 自己的 mac 地址也可以修改
   - 例子里面、如果要发包给 E、就要设置目标的 mac 地址为 15（E 的 mac 地址），然后设置目标的 ip 地址为 router K 的 ip、即 19，这样 K 才能收到这个包

4. layer mapping（arp cache）

   - 会在表里面存储 ip->mac 的映射
   - 上面的 routing table 是通过 hello protocol 和 advertisement 来更新的
   - 这里使用了 ARP 协议（Address Resolution Protocol）来解决当一个新加入网络的时候、如何找到这个新加入的设备的 ip 地址和 mac 地址的映射
   - 会通过广播的方式来找到这个映射（如果有人回应、就会更新这个表）（这里的表是存在哪个设备上？L 还是 E？这个存的 station 为啥是 19？）（貌似每个机器都会存一个 cache）
   - 这个表就像 cache 一样、会不断更新

5. RARP：Reverse ARP

   - 通过 mac 地址找 ip 地址

6. gateway 貌似就是 router 的意思，这里有一个例子

   - 在例子里面，会先填 target mac 为 gateway 的 mac
   - 然后根据 route table 修改接下来的包的 source 和 target 信息
   - 所以 link layer 通过修改 mac 地址来完成相邻节点的包的传输

7. 会有一个 arp spoofing 的攻击

   - 核心方法是 arp poisoning（本质上是对 arp 表的污染）
   - hacker 在没人问路的情况下广播很多消息，把正确的 ip-mac 映射关系给替换掉，然后这个包就被发到了 hacker 的机器上
   - 相当于 hacker 会变成一个中间人，可以看到所有的包（在不需要路由的场景下、hacker 把自己变成了一个路由器，就可以监听）

8. 如何防止 arp spoofing

   - 这个主要是因为协议问题、所以会比较难解决
   - 方法是：定期监听网络中的 arp 包的流量，然后发现有异常流量和数据的包的时候、就会发警告。但是没法根本性地解决。
   - 还有一个方法是：用静态的人为设定的 arp 表，但是这样的话、就会导致网络的可扩展性变差

9. 上面讲的都是网络层的问题，下面讲的是 end-to-end 层的问题

   - NAT 和 ARP 都是在网络层中会发生的变化

10. end-to-end 层

    - 因为 network layer 没有保证很多东西、所以需要 end-to-end 层根据不同的应用场景进行设计
    - TCP 的 flow control 是可以当数据发得太快的时候、控制发送的速度
    - 这里介绍了 UDP、TCP 和 RTP

11. end-to-end 考虑了 7 种问题

    - Assurance of at-least-once delivery
    - Assurance of at-most-once delivery
    - Assurance of data integrity
    - Assurance of stream order & closing of connections
    - Assurance of jitter control
    - Assurance of authenticity and privacy
    - Assurance of end-to-end performance
    - 最难的是第 7 个问题、performance 的问题

12. Assurance of At-least-once Delivery

    - 问题是：如何确定一个包丢失了
    - 可以通过 timeout 来确定（但是需要首先知道 RTT）
    - nonce：一个随机数、用来标记这个包是第几个包（网络中常用的方法）
    - Dilemma 是不知道是哪个阶段丢的包，可能是 ack 的时候或者是发送的时候丢的
    - Try limit times before returning error to app(方法即是每隔一段 timeout 就重新发一个 data 包)
    - 所以他不能保证 No assurance for no-duplication （就是有可能会重复发送）（at least once）

13. 怎么设置这个 timeout 呢？

    - 首先不能设置 fixed timer，因为没有要到时间就会轮询、然后轮询了之后就更要不到时间、因为会大量阻塞
    - 可以用一个 adaptive timer，比如说根据 RTT 来设置这个时间，或者根据指数增长的方式来设置这个时间
    - 当前 linux 的设置是通过一个不断更新的 avg（存在权重） 和 dev（标准差） 来进行设置的 timeout = avg + 4 \* dev（经验数据）
    - 还有一个是 nak，就是当收到几个包的时候，receiver 会告诉 sender 哪些包丢了，然后对方会重新发这个包，这样 sender 就不需要对每个包维护一个 timer 了、只需要 sender 维护一个全局的 timer
    - 只要 at least once、就要有 timer

14. SNTP 里面会有一个 go away 的设置、即如果超过了这个时间、就会让他不要再发包了

15. Assurance of At-most-once Delivery

    - 最基本的实现：receiver 收到一个 nonce 相同的包、就不处理
    - Maintains a table of nonce at the receiving side
    - 但是这样就会让这个 table 无限增长，导致搜索开销变大和 tombstones 的问题
    - 另一个方法：通过幂等操作来解决这个问题，允许多次处理同一个包（即一个操作多次执行和一次执行的效果是一样的）
    - 第三个方法：还可以设置一个最大的 sequence number、小于这个 number 的包就不接受、大于这个 number 的包就接受，但是这个 old nonce 也会变成一个 tomstone（但是要保证递增的、而且不会乱序出现）
    - 第四个方法：每次用不同的端口处理请求、但是旧的端口就会变成一个 tombstone

16. 实现 At-most-once 会比较难、需要更多的资源，而且 crash 了之后因为 table 在 cache 里、会丢失

17. Assurance of Data Integrity

    - 使用 checksum（但是还是有很小的概率，但是不用考虑）

18. Segments and Reassembly of Long Messages

    - 需要进行拆包
    - out of order 的时候、会有不同的方法解决这个问题（各有优劣）
    - 方法 1：只顺序接受包
    - 方法 2：等待顺序位置的包到了之后、再把这个包放到 buffer 里面
    - 方法 3：combine the two methods above
    - 方法 4：当发现一个包没收到的时候、主动返回去要这个包，发一个 NAK 的消息
    - 这个 buffer 是 os buffer，所以不能允许一个 app 的占用的 buffer 空间太大、如果 NAK causing duplicates，stop NAKs
    - TCP 基于 ACK 的重传，而不是 NAK （NAK 会快一点）

19. Assurance of Jitter Control

    - 相当于看视频的时候的提前缓冲，来避免卡顿，因为看到的是缓存好的，而新来的数据有快有慢地来是感受不到的

20. Assurance of Authenticity and Privacy

    - 加密：公钥私钥（非对称加密）

## 20 TCP Congestion Control & DNS

1.  End-to-end Performance

    - lock-step protocol:收到 ack 之后再发下一个包，但是问题就是、时延会变成一个瓶颈
    - 所以引入了 Multi-segment message，可以一次先发多个包（Overlapping Transmissions ）
    - 但是 Overlapping Transmissions 的时候、需要考虑丢包的时候怎么办
    - 性能由 receiver 收包的能力和网络带宽决定
    - 所以引入了 fixed window（这个 window 的大小是 receiver 给的，例子里是每次收发 3 个包）
    - 然后为了提升性能、引入了 sliding window（这个 window 会根据目前收发到了多少个包进行滑动）（就能缩短 receiver 的 idle 时间，减少浪费的带宽）

2.  有一个解决丢包的例子

    - 因为 2 号包丢了、所以不能滑到 3 号包的 window 那边
    - timeout 之后会重发 2 号包
    - 所以这个 TCP 解决丢包的方案是比较保守的、会等待 2 号包的 ack 回来之后、再继续发包

3.  Sliding Window Size 的大小需要合理进行设置

    - window size ≥ round-trip time（一个来回的时间） × bottleneck data rate（瓶颈的带宽）
    - 500KBps（receive） \* 70ms （RTT）
    - 但是单单根据这个公式得到的还是不准确的、因为其中可能还会有其他的瓶颈、如果传得太快、就会发送拥堵

4.  TCP Congestion Control

    - 需要既涉及到 network layer 也涉及到 end-to-end layer
    - 因为要排队、所以会需要把包存在队列里面，会存在网络资源的浪费

5.  所以为了 congestion control，也要控制 window size 的最大值

    - window size ≤ min(RTT x bottleneck data rate, Receiver buffer)
    - 但是 bottleneck data rate 是动态的、所以问题在于怎么找到这个值

6.  congestion control basic idea

    - Increase congestion window slowly
    - If no drops -> no congestion yet
    - If a drop occurs -> decrease congestion window quickly
    - 缓慢地增加、迅速地下降、很保守的方法

7.  有一个 AIMD 的方法（Additive Increase, Multiplicative Decrease）

    - Every RTT:
    - No drop: cwnd = cwnd + 1
    - A drop: cwnd = cwnd / 2

8.  但是有很多问题

    - 比如一开始的一段时间、速度巨慢
    - 解决方式：只有在第一阶段用指数的方式增长（slow start）
    - 这里 ppt 上有一个增长的示意图

9.  duplicate ack received 是实际中引入的、receiver 当很多时间没有收到一个包的时候、会重发一个已经发过的 ack、然后告诉 sender 快点发、这样主动地告知 sender 出问题了，就能标识网络问题

    - 然后如果有一段时间 expires stop sending 之后、会重新从头开始 slow start
    - 但是问题还是有、就是图像上会有区域的带宽没有充分利用
    - 所以有了 DC-TCP 这些改进的方法、比如说针对数据中心的场景进行使用（针对不同场景有不同优化）（数据中心带宽很高、而且网路情况很稳定）

10. AIMD Leads to Efficiency and Fairness

    - 就是会先都腰斩变成一半、然后再每个都加 1、然后最后会收敛到这个 y=x 的直线上面
    - 这样不断调整就会调整到中间的点

11. 为什么不用减法？因为就没法自动保证 fairness

12. 丢包不一定是由于 congestion 导致的、比如在不稳定的无线网里、由于信号导致的丢包、我们甚至应该加快发包速度

    - 所以传统的 TCP 直接应用在无线网上的时候、会导致恶性循环

13. 总结：Congestion window is adapted by the congestion control protocol to ensure efficiency and fairness

14. Domain Name Service

    - 需要从 host names 转换成 ip 地址（计算机通过 ip 访问）

15. 为什么不只用 ip

    - 首先不够用户友好
    - 其次 ip 是动态的、会进行变化

16. Questions on DNS

    - 一个 name 可以有多个 ip address（这样就可以选一个近的、就会更快）
    - 一个 ip 也可以有多个 name（这样就可以有多个域名指向一个 ip、比如说一个网站有多个域名）（server consolidation）
    - ip-name 的映射是动态的、可以变化（用户无感知）

17. Look-up Algorithm

    - 一开始是存在每个机器里的 txt 里、但是不能 scale
    - 所以就有了 DNS server，一开始叫做 BIND（Berkeley Internet Name Domain）

18. Distributing Responsibility

    - 需要对 name 域名进行结构化、做分层 hierarchy 的结构，然后按层去寻找

19. Name Servers

    - The root zone
      Maintained by ICANN, non-profit（仅负责一级域名）（数量不用特别多）
    - The ".com" zone
      Maintained by VeriSign, add for money
    - The ".sjtu.edu.cn" zone
      Maintained by SJTU

20. Context in DNS

    - Names in DNS are global (context-free)
      - A hostname means the same thing everywhere in DNS
    - Actually, it should be "ipads.se.sjtu.edu.cn."
      - A hostname is a list of domain names concatenated with dots
      - The root domain is unnamed, i.e., "." + blank

21. Fault Tolerant

    - Each zone can have multiple name servers
      - A delegation usually contains a list of name servers
      - If one name server is down, others can be used

22. 可以指定地绑定和搭建 dns

    - 通过修改 hosts 文件
    - 设置/etc/resolv.conf，指定 dns server，如果找到了对应的、就直接从这个 dns server 里返回、如果没有找到、就会去找其他的 dns server
    - 好处是什么呢？既可以自己主动地去改动一些指定的域名、也可以在小范围内增强对域名访问的控制力

23. recursion 的方式的通信性能会更好一点、因为服务器之间的网络稳定、性能会更好

    - 但是要求就是对 root 的要求太高了
    - 而且是需要服务器维护有状态的
    - 如果 non-recursion、就不需要维护状态
    - 实际中如何利用他们两个的优点呢？
    - 可以使用 caching

24. caching

    - DNS clients and name servers keep a cache of names

      - Your browser will not do two look-ups for one address(就不用每次都去找一次 dns server 了)
      - 电脑上自己也会有 cache

    - Cache has expire time limit

      - Controlled by a time-to-live parameter in the response itself
      - E.g., SJTU sets the TTL of www.sjtu.edu.cn to 24h

    - TTL (Time To Live)

      - Long TTL VS. short TTL
      - 如果太长了、就会体验到错误（比如服务器迁移更新了）

25. 找第一个的地址的时候会比较麻烦、因为还没上网、所以需要口口相传

26. Comparing Hostname & Filename

    - File-name and host-name are hierarchical; inode num and IP addr. are plane

    - They are both not a part of the object

      - File-name is not a part of a file (stored in directory)
      - Host-name is not a part of a website (stored on name server)

    - Name and value binding

      - File: 1-name -> N-values (no，没有意义); N-name -> 1-value (yes，hard link)
      - DNS: 1-name -> N-values (yes，加速和容错); N-name -> 1-value (yes，website consolidation，服务器聚合)

27. 貌似最后还讲了一点东西，dns 就是类似于 metadata server，单机的时候会有瓶颈、所以可以扩展几个 dns server，然后通过负载均衡的方式来进行访问

## 21 P2P Network

1. Good Points on DNS Design

   - Global names (assuming same root servers)
     - No need to specific a context
     - DNS has no trouble generating unique names
     - The name can also be user-friendly
   - Scalable in performance
     - Simplicity: look-up is simple and can be done by a PC
     - Caching: reduce number of total queries
     - Delegation: many name severs handle lookups
   - Scalable in management
     - Each zone makes its own policy decision on binding
     - Hierarchy is great here
   - Fault tolerant
     - If one name server breaks, other will still work
     - Duplicated name server for a same zone

2. Bad Points on DNS Design

   - Policy
     - Who should control the root zone, .com zone, etc.? Governments?
   - Significant load on root servers
     - Many DNS clients starts by talking to root server
     - Many queries for non-existent names, becomes a DoS attack
   - Security
     - How does a client know if the response is correct?
     - How does VeriSign know "change Amazon.com IP" is legal?

3. 怎么解决域名服务器逐层 cache miss 的问题（转化为 DoS 攻击）

   - 用分层定时扩散域名的方式（定时获取）

4. Security：使用证书机制保证安全

5. Naming for Modularity

   - Retrieval（检索）
   - Sharing
   - Hiding
   - User-friendly identifiers
   - Indirection:与实际位置解耦

6. 比如：EAX - 是一个寄存器的名字

   - 所有机器上都有 EAX，而且叫这个名字的寄存器有很多个（实际映射到很多不同个的隐藏的寄存器上）
   - 所以可以并发地去做一些操作

7. 或者 phone number 的虚拟号码、也是保护隐私

8. naming model 里的 context 有些是 global 的、有些是 local 的，比如针对说一块磁盘（这张图很好）

9. bind 和 unbind，unbind 做得不好

10. recursive lookup 是递归的，multiple lookup 是平行的

11. FAQ of Naming Scheme

    - What is the syntax of names? 比如说网址里的.分隔符
    - What are the possible value?
    - What context is used to resolve names?
    - Who specifies the context?
    - Is a particular name global (context-free) or local?

12. 这里说的 5-4-5 范式是什么意思？

13. content distribution

    - 相当于就是分布式的 cache，比如把视频主动推送到 sjtu 的服务器上、然后避免对主视频服务器的多次访问

14. content distribution network（CDN）

    - Network of computers that replicate content across the Internet

      - Bringing content closer to requests
      - End users only use local (not shared) network capacity

    - Content providers actively pushes data into the network

      - Improve performance and reduce costs

15. 访问时在选择目标服务器的时候

    - 一开始是通过 http 重定向在进行，但是这样会有很多的 overhead（额外的 round trips）
    - 然后是通过 dns 来选择、是选择最近的（避免了 redirect）（缺点是通过 ip adress 来找这个 dns 服务器，但是不一定是最快）
    - 然后演化成了 akamai 的形式、逐层地找 dns、直到找到最近的 cdn 服务器，最后从最近的 akamai 服务器获取内容数据
    - cdn 提供商会从 content provider 主动获取数据、然后再 cdn 的服务器上缓存，然后再把内容数据发送到对应的 end user 的机器上
    - 用户一定是从 cdn 上获取的内容、而不是从 content provider 上获取的
    - 好处是减少了带宽成本

16. P2P

    - 中心化的基础设施会有一些缺点：比如说单点故障、性能瓶颈
    - P2P 是一种去中心化的方式，没有中心化的节点

17. BitTorrent 有三个角色

    - Tracker：What peer serves which parts of a file
    - Seeder: Own the whole file
    - Peer: Turn a seeder once has 100% of a file (有一部分文件、100%文件的时候就变成了 seeder)

18. 首先会需要把 .torrent file 发到一个中心化的 web server 上（根节点，公共知道的地方）

    - URL of tracker, file name, length, SHA1s of data blocks (64-512Kbyte)
    - Tracker
      - Organizes a swarm of peers (who has what block?)
    - Seeder posts the URL for .torrent with tracker
      - Seeder must have complete copy of file
    - Peer asks tracker for list of peers to download from
      - Tracker returns list with random selection of peers
    - Peers contact peers to learn what parts of the file they have etc.
      - Download from other peers

19. 这个去中心化建立在中心化的基础上（traker）

20. 这个 bitTorrent 有不同的下载策略

    - Random
    - Rarest first
    - Strict
    - Parallel

21. bitTorrent 的缺点就是 rely on tracker

22. 所以引入了一个 DHT: Distributed hash table

    - 即在分布式下的 hash table

23. Chord IDs

    - SHA-1 is a hash function
    - Key identifier = SHA-1(key)
    - Node identifier = SHA-1(IP address)
    - 这个 key id 和 node id 是在同一个 id space

24. Consistent Hashing

    - 就是这个 node 对应负责小于等于这个 node id 的 key
    - lookup 的时候、node 会一个一个往前、然后直到找到一个 node 能够负责这个 key（O（n））
    - 然后可以用一个"Finger Table" 的优化、让这个查找更快（二分），然后每个节点也不需要保存所有的邻居节点、只需要保存比较少的邻居节点 O（log（n））
    - 如果有 failure 之后，会有一个问题：就算例子里的 K90 其实最后是保存在 N113 里的，但是 N120 不知道，所以会从 N120 继续往后找（相当于跳过了 N113，就找不到 K90 了）
    - 这时候最简单的方法就是顺序，所以解决方法是合二为一（同时维护 finger table 和 successer lists）

25. 这个初始化的例子里、新加入节点之后、原来节点里的内容也不用删除，因为查找对应 key 的时候自己的负载也不会变大

26. 做负载均衡的时候、可以引入虚拟节点的概念、然后和实际节点做映射

## 22 The distributed (and parallel) programming: it’s all about scalability

1. 怎么去加快计算呢，方法就是 Parallelism

2. 一开始是硬件上的 pipeline，但是单核上的频率有瓶颈、因为超标量和主频的提升会停滞。所以第一个方法变成了多核

3. 在多核情况下、会有一个 Cache coherence protocols，保证内存的一致性

   - 一个是 Snoop-based cache，还有一个方法是用一个来统一

4. 但是问题是不能 scaling、因为会涉及到要 snoop 很多个核（gpu 是没有 cache coherence 的）

5. 第二个方法是通过加更多个 alu 单元（本质区别是和更多的硬件空间留给了 alu）

6. New ISA: SIMD processing：用了 SIMD 之后、需要额外写很多代码

   - 每次可以做 8 个 4bit 操作
   - why not 8X faster? 因为不仅仅是做计算、还需要做访存（访存速度没有那么快）

7. 访存有两个限制

   - 一个是访存的延迟
   - 一个是访存的带宽
   - Both factors matters ，取决于实际应用的场景和需求

8. 优化之后、cpu 的性能就受制于访存上了

## 23 The distributed (and parallel) programming on a single device & MapReduce

1. mapreduce 是一种分布式的计算框架

2. 超标量用于单核，但也有上限（pipeline）

3. 可以用 SIMD，但是传统的 cpu 不支持、而且需要写额外的代码

4. SIMD 之后、继续增加的话就是用多核（还有多个 ALU），（不能无限增加数量是因为硬件的数目和空间是有限制的）

5. 如果还没从内存中 load 出来的话、cpu 就会空转

6. roofline model 就能刻画一个设备时受制于算力还是访存（y 轴代表每秒需要计算多少浮点数）（横竖线时设备的计算能力）

7. 怎么刻画访存需求：访存本质上是间接需求（好的应用是每次 load 能做很多次计算、而不是每次计算都需要 load 内存）

8. x 轴刻画的是 operation intensity

9. 斜线即代表了它带宽的需求（斜率）

10. 左边 intensity 和斜线相交、就是访存 bottleneck、在右边和 peak flops 相交、就是计算 bottleneck（给出应用的优化方向）（比如说可以优化 gpu 算力、将 intensity 竖线右移）

11. 因为 gpu 上面没有 cache coherence，也没有预测分支，也没有预取、所以可以承载更多个 SIMD，所以 ALU 能放更多

12. 但是 SIMD 的操作只是针对底层硬件的，比较原始（而且是 vector 中批量做的）

13. 对于 relu 这种函数怎么做优化呢、可以先都做计算、然后如果小于零那么就让结果不生效（mask 方法）

    - ppt 上的例子是：比如在分支中、先全部做 True 的情况、然后再把对应的情况写入、然后再做 False 的情况、再做写入（实现了 conditional branch 中的减少计算、这样只需要做两次）
    - 但是这种方法还是会有 50%的计算浪费

14. cuda 是 nvidia 的 SMID 的框架

    - 是一种 SIMT，在 SIMD 上抽象了一个更高层的概念
    - 然后就不需要写很多 SIMD 中的重复的代码了，比如 if 里的 mask 操作
    - 每个 thread 在同一个时间都只执行同一条指令
    - 有了这个方法之后、就能用 CUDA 去写多 kernel 上的代码了
    - 矩阵乘法的硬化是会有很大的效能提升的
    - 性能提升主要在于 tensor core

15. 主频的提升是有上限的、因为会有很多的热量产生

16. 但是单卡还是不够、所以需要分布式

    - 存在很多分布式计算框架
    - batch processing（spark、hadoop）

17. batch processing

    - 场景：谷歌要统计很多网站每天的点击量
    - 传统的是在 log 中用命令行去统计，但是它是单线程的，很难 scale
    - 所以可以手动做一个任务划分、每台机器做一个 log 的统计，然后最后再做汇总

18. mapReduce

    - map：把一个函数应用到很多个数据上（因为 map 这个事情非常适合并行化）
    - reduce：把 map 的结果做一个累加（必须等 map 全部做完）
    - 这里假设了函数不会有 side effect、所以如果出错了之后、直接重做就可以了

19. 这个 reduce 其实是根据 key 做 reduce 的

    - 这样就可以让 reduce 也具有并行性
    - 用 partition 的方式来类似地去做 reduce
    - 有了 mapreduce 之后、我们只需要考虑 functionality 就可以了

20. 例子：统计页面中出现了多少次每个单词

    - 貌似就是分 shard 来进行

21. 只需要把代码直接给 mapReduce runtime 就可以了

22. MapReduce 执行流

    - master 负责启动所有的 mapper 和协调 reduce worker 什么时候开始执行
    - 对于用户是不可见的
    - 第一步：把大的文件分为很多小的 chunks（不能太小也不能太大）（太大的话在不同的 chunkserver 上、需要从很多个 server 上面拉这个数据）（选择了 64MB 的大小、因为 gfs 的 chunk 大小是 64MB，就能保证一次 rpc 就可以了）
    - 第二步：用 master 起所有的 map worker 和 reduce worker（一台机器上可以起多个 map worker 和 reduce worker，比如 reduce worker 不做事情的时候其实不怎么耗费资源）
    - 第三步： map task（它的所有中间结果都是先放在内存里、这样更快、然后再异步的方式去刷回到磁盘）
    - 第四步：这个 intermediate files 需要保证每个 reduce worker 负责的 key value 要放在一起、不然的话会有很多的磁盘上的随机访问、就会降低性能（会把文件做 partition，如果是 reducer1，就写到 partition1 里，每个 partition 对应于一个 reducer）
    - 第五步：mapper 结束之后、会告诉 master 可以启动 reducer 了，然后 master 会指定哪些 reducer 去读哪些 partition。这里的先 sort 是为了保证 reducer 在读的时候、一次可以把一个 key 的所有 value 都读完（如果排序整个文件、会有很多 overhead、所以有一个优化，就是类似于归并排序，每个 map worker 会把一个 partition 的 key 先做一个排序，然后合并的时候就用归并排序的方式）
    - 第六步：做 reduce，产出最终 output file
    - 第七步：把结果返回给用户

## 24 Distributed Computing frameworks MapReduce, computation graph & Distributed training

1. mapReduce 的优化：首先是要做 fault tolerance

   - 一种是 worker failure
   - 一种是 master failure

2. 有两个让 mapReduce 很好 handle failure 的原因

   - 因为这个 map or reduce 的 function 没有 side effect，所以可以直接重做
   - 基于一个可靠的服务 GFS

3. worker failure

   - 第一个是 detection：监测心跳包（或者是 timeout 时间到了）
   - 第二个是 recovery：重做这个任务，会给这个 worker reset 到初始状态、或者直接把这个 job 给其他的 worker（re-execution）

4. master failure

   - 因为这个 job 往往会需要很长的时间、比如 8 个小时。所以直接重做不太好
   - 所以 master 会把自己的状态定期地持久化到 GFS 中、然后如果 master 挂了、就会从这个状态中恢复
   - 它是一个 periodical 的 checkpoint（所以就不需要重做所有的操作了）

5. 用户给 mapReduce 的函数，如果会有第三方库、有可能会触发这些第三方库的 bug

   - 一种是 report 给 developer、但是 developer 不一定写的这个库
   - 后来发现这些很多 bug 是由于一些特点和报错、比如会导致 segmentation fault
   - 如果出了这些错、那就把对应的 input 数据给直接丢掉不计算了

6. Optimization for Locality

   - 当时的计算和存储是在同一个机器上的、所以可以直接从本地读取数据然后在本地计算

7. stragglers 是那些跑得特别慢的机器

   - 因为一个机器上可能不仅仅跑了 mapReduce 的进程、所以会影响性能
   - 或者 cpu 没有 cache（由于配置错误）
   - 所以会出现如果只有一个 mapper 没有做完的话、那么就需要等待这个 mapper 做完才能做 reduce
   - 解决方法：做冗余，每一个 job 可以分在不同的机器上面去执行，如果这个 map 在任意一个机器上被做完了、那么就认为它做完了。
   - 但是这个在训练的时候不太使用、因为训练模型需要很多计算

8. Sort 的时候也是分 partition 去做的

9. 但是，MapReduce cannot address all the issues

   - 因为本身这个 map reduce 的函数比较简单（通过迭代，就是没有方法去编写一些比较复杂的例子）
   - 但是就不适用近似实时的计算和场景了

10. start penalty：启动 mapper 的时候需要预热

11. 可以通过直接把 map 的结果发送给 reducer，而不是先写到磁盘上，来做优化

12. computation graph

    - 一个节点代表一个数据或计算（要么是一个数据、要么是一个计算）（没有 side effect）
    - 一个边代表数据的依赖关系
    - 一个 job 就可以表示为一个 DAG
    - 因为 DAG 对于容错比较友好、如果一个红色的节点挂了怎么办、那么就可以去它的上游找对应的数据还在不在，然后做对应的恢复
    - 有环就不干净了
    - DAG 还能判别出哪些任务是可以进行并行执行的（没有路径依赖的话）

13. 用 kv store 存储中间状态的数据

14. 这个 failure 也会有 recursive 的情况

15. computation graph 也把系统和底层的算法做了解耦（机器学习）

16. 也可以去做图优化（把几个小的 kernel 合并成一个大的 kernel，好处就是 kernel 的访存效率提高了）（cuda graph）

17. 下面开始讲 distributed training

18. x 是数据、y 是 label（期望值），w 是参数，然后一轮之后就是更新

19. 但是这个计算图的问题是、它没有办法并行、因为全是 dependency

20. 所以分布式训练研究的就是怎么把一个计算图变成一个可以并行化的计算图

21. 有两种主要的并行化方式

    - 一种是 data parallelism，就是把数据的 batch 分成很多份，然后每个机器上都做子 batch 的计算，这里的限制就是每个计算图的节点上面都有一个完整的模型、但是对现代的深度学习模型来说、这个模型是很大的、所以这个方法不太适用
    - 一种是 model parallelism，就是把模型的计算过程分成很多份，然后每个机器上都做一次计算，然后再把结果合并
    - 还有一种是跨 iteration 的 parallelism，就是可以做同步或者异步的更新（讲得会比较少）（现在基本所有大模型都是同步更新出来的）

22. 同步的 data parallelism

    - 图中就显示出了讲输入的数据去做分割，然后每个机器上都做一次计算，然后再把结果合并
    - 最后的梯度需要相加
    - 最后的问题是它什么时候做汇总，因为要 syncronize，所以会有一个 bottleneck（要等所有的都算完）

23. 这个 allreduce 是因为要把汇总的结果给所有人广播、所以叫 all

24. 里面最大的问题是需要减少 network overhead

    - 一个是带宽的 bottleneck
    - 还有一个是瞬时的 concurrent 的 message 的数量（比如 1 万张卡的数据同时给到一个机器）

25. 实现 1：parameter server

    - 好像就是一个简单的模型
    - 对 parameter server 的性能需求（带宽和并发数）和机器数是成正比的 O(N \* P)

26. Co-located & sharded PS

    - 每台机器只负责 1/n 份的数据
    - O(N \* (P – 1) / P )

27. fan-in 就是很多机器在同一个时间给同一台机器发很多流量

    - 所以希望同一时间只有一台机器和 x 通信

28. 第二个方法：De-centralized approach for allreduce

    - 做一些协调、让每个机器的发送有一定间隔、而且不在同一瞬时
    - 每个机器做完之后主动通知下一台机器去发
    - 然后用类似方法发回信息
    - 这个 fan-in 就是 1
    - 但是还是有问题、就是每台机器要传的参数量还是很多，而且这个通信的轮数还是很多 O(P \* P)

## 25 Distributed Computing frameworks MapReduce, computation graph & Distributed training

0. naïve decentralized reduce有个坏处、就是虽然它的fan-in是1，但是它需要传的总数据量很大、是N * (P – 1) data 

1. 也可以采取一个优化方法Ring allreduce、就是每个机器只和邻居相连，然后然后每次传的时候做一个 reduce，然后再传给下一个邻居。然后最后一个传到的机器就有一个完整的数据。（这样的话每台机器的网络连接数就是constant了）

2. 但是问题是其他人没有对应的最终结果。所以需要再做一个 broadcast，然后每个机器都有了最终的结果（训练中的梯度）。

3. 但是由于有些gpu计算中的浮点操作（比如加法）是不满足交换律的，所以这个Ring allreduce会改变一些计算顺序、所以应用的时候需要注意。

4. 总数据量 2 * (P-1) * N / P

5. 但是还是有问题、因为它还是需要做k轮通信

    - Total communication rounds: O(P) 
    - Much higher than the parameter server approach (O(1)) 

6. Double Binary Tree All-Reduce

    - 为什么需要double呢，因为这个单个的tree，load balance不好，所以需要两个tree（因为有些节点是根节点、有些则是叶子节点）
    - Log(p) rounds of communication
    - 所以可以把数据切成一半、然后把一半给叶子节点做、一半给父节点做
    - 这个fan-in就只有1

7. 主要就是三个参数的权衡

    - Communication round
    - Peak node bandwidth
    - Fan-in

8. 上面都是data parallelism，下面开始讲model parallelism

    - data parallelism中，每台机器都需要存储完整的参数，但是现在的llm太大了
    - 而且现在的llm还需要存储优化器、比参数还大

9. 这个model的切法就是切分开来这个w，然后横着切（不同gpu放不同层）、就是pipeline parallelism，纵着切（把一个很大的 w切到不同的机器上）、就是tensor parallelism

10. pipeline parallelism

    - 会有一个bubble的问题、所以需要reduce bubble

11. 第一个方法是 micro-batching，就是把一个batch切成很多小的batch，然后每个小的batch都可以并行计算（类似于cpu pipeline中的流水线并行）

    - 这个在forward path里面还不错
    - 但是在backward path里面就不太好了、因为还是需要等到所有的都算完才能做进行计算，所以还是有bubble

11. Question: what is the overhead of pipeline parallelism? 

    - Bubble time fraction: (𝑝 −1)/𝑚 

12. Optimization directions 但是都不可行

    1. increase m (e.g., increase the batch size) （首先、每个batch size不能太小因为需要每个batch都保证一定的最小大小、因为如果太小、这个gpu的计算效率会很低）（然后这个batch size不能太大、因为会导致这个模型的收敛会很慢）
    2. reduce p (reduce the #partitions) （不太现实、因为gpu放不下那么多层的参数）

13. 所以有了 tensor parallelism

    - Partition the parameters of a layer
    - Each partition is deployed on a separate GPU 
    - 使用了分块矩阵乘法
    - 涉及了forward pass
    - 也涉及backward pass，要算∇X和∇W

14. tensor parallelism和pipeline parallelism

    - 前者比后者的通信量要多很多
    - 一般现在的卡会先在一个scala network上面、这个network用nvlink连接、带宽差不多在800GB/s左右、然后这个network里面、用tensor parallelism
    - 然后在机器之间、会用比较慢的网络、比如RDMA、是100GB/s，在跨机器的时候用pipeline parallelism

15. 3D parallelism就是把上面三种并行放在一起

    - data parallelism用在四台机器的data replica中
    - 一台机器内用tensor parallelism
    - 机器之间用pipeline parallelism

16. 异步更新

    - 算完之后、马上把梯度发给下一个人、然后用现有的结果再去做下一轮计算
    - 讲得比较少、因为会影响精度（可能只有在传统的图计算里面有用）

## 26 Introduction to System Security

1. 一开始的例子、就是说一个商家假扮成一个买家、然后篡改了交易的签名、然后amazon就会打钱给他、而不是给原来的商家。

2. Internet makes attacks fast, cheap, and scalable

3. Users often have poor intuition about computer security

4. Security is a negative goal

    - 指目标是一件事情不能发生，所以安全很难判断
    - 比如说有很多可以获取一个文件的方法

5. Why not using fault tolerant techniques for security?

    - 有时候可以 Some security issue can be seen as a kind of fault
    - 但是其中的攻击和fault的原因是不一样的
    - 而且这个attack导致的failure基本是correlated的、而单纯的failure是independent的
    - 所以No complete solution; we can't always secure every system

6. What are we going to learn?

    - How to model systems in the context of security
    - How we think about and assess risks
    - Techniques for assessing common risks
    - There are things we can do to make systems more secure
    - Know trade-offs

7. Policy: Goals （下面的CIA），拆开来的原因是因为他们三个相对来说是正交的。

8. Information security goals:

    - Confidentiality: limit who can read data （加密解决）
    - Integrity: limit who can write data

9. Liveness goals:

    - Availability: ensure service keeps operating

10. 拆解目标之后、就可以针对每个目标做对应优化

11. Threat Model: 就是做出 Assumptions

    - What does a threat model look like?
    - 建模之后、就能进一步简化问题
    - 一个假设是只有打开的一个软件是恶意的、其他的都不是恶意的

## 27 ROP and CFI

1. Authentication: Password

    - 这个攻击的例子是、如果密码的第二部分在内存里的另一页、那么如果前面几位匹配了、那么就会换页、就会慢、如果前面几位不匹配、那么就不会换页、就很快（这样就可以通过时间来判断密码的对错）（时间就变成了26*8而不是26^8）
    - 但是今天这个攻击不太好做、因为要把内存设置成这个样子的话不太方便，但是在当时的os里面很容易实现
    - 解决方法：先hash或者全部匹配完再返回true or false

2. Idea: Store Hash of Password

    - 如果密码到hash值是一一对应的、那么就可以通过hash值来判断密码是否正确（构造）
    - Often called a "rainbow table"
    - 所以为了解决这个问题、需要往密码里加盐（salt）

3. Salting: make the same password have different hash values

    - 加盐是明文的、但是从根本上还是没有解决这个问题
    - 但是为攻击者加高了很多攻击的代价
    - 因此这个是从工程的角度解决这个安全的问题的

4. Bootstrap Authentication

    - web里面是用cookie的方式做保存的
    - cookie里面因为用了server_key即服务器端的key做hash、所以难以伪造

5. Session Cookies: Strawman

    - 但是其中会有一个二义性、就是在做字符串拼接然后做hash的时候、会有不同的用户名+日期会得到相同的结果字符串

6. Phishing Attacks

    - 钓鱼攻击、就是让你输入密码到别人的网站里
    - 这个就是密码最大的问题、一旦知道了密码、别人就可以伪造你的身份

7. 下面的R是：a random value R

7. Tech 1: challenge-response scheme

    - 如果朴素地做传递密码、会在通信的时候密码被截获，不管这个值是不是被hash过了的、都会有这个问题（如果被hash过了、之后也可以继续用这个值）
    - 服务器给一个随机数、然后客户端用密码和这个随机数做hash、然后再发给服务器
    - 服务器也有这个随机数、然后也做hash check、然后比较
    - 这样就不会有密码泄露的问题
    - 但是缺陷是server这里需要保存明文

8. Tech 2: use passwords to authenticate the server

    - 这里有两种小的子方法、第一种是Client chooses Q, sends to server, server computes H(Q + password), replies， Only the authentic server would know your password!
    - 就是用密码去反向验证服务器的真实性（是用来反钓鱼的）
    - 但是真实的系统很少有这么做的，因为可以用同样的一种方式去欺骗server
    - 第二种是First, try to log into the server: get the server's challenge R
    - Then, decide you want to test the server: send it the same R
    - Send server's reply back to it as response to the original challenge
    - 简单概括就是发server一个随机数、然后server对这个随机数做一个hash、然后发给client、然后client用这个随机数发给server、就可以登录了
    - 但是这个貌似也是不行的、会变成一个中继，什么都不用干就可以假冒一个用户登录

9. Tech 3: turn offline into online attack

    - 就是加一个验证码、这个验证码是每次都不一样的、所以就不会有中继的问题了
    - 然后服务器还可以记录哪些ip是在不停地发送请求、有可能会有固定的ip发送不同的用户名密码、就可以ban掉这个ip

10. Tech 4: Specific password

    - 用户端：每个服务设置一个不同的密码

11. Tech 5: one-time passwords

    - 就是用一次就不能用，比如手机的验证码
    - 所以会有一个Hash n的密码发送策略、然后每次用完之后就会把这个hash n-1次后的密码发给server、然后server再做一次hash，然后发一次、本地的下一次发送、减去一个1再发送一次
    - 这个是为了防止攻击者投到中间的hash密码
    - 这里还有一个银行用于小的验证码的验证的工具、就是一个不联网的设备、通过初始状态相同、然后同步时间信息、就可以和银行中目前的验证码相同、然后且保证这个验证码不是固定的。然后这里还会需要涉及到银行端做时间上的容错、因为这个时间可能会和设备的时间有微小差异、所以应该在银行端可以计算出一个时间范围内的验证码，都合法。
    - Google's App-specific password：每个密码只能用一次、然后每次都会有一个新的密码，上一次登录会记录登录的用途

12. Tech 6: bind authentication and request authorization

    - 比如转账这个操作、单单到cookie是不行的、还需要再输入一次密码

13. Tech 7: FIDO: Replace the Password

    - 比如说指纹
    - 把密码放在一个安全的区域、还是用指纹去获取这个密码
    - 最后还是用密码去做登录的验证
    - Three Bindings是阐述了指纹密码的实现：就是服务器先通过userID加上个人的公钥、生成一个东西、然后发给client、client用这个东西加上自己的私钥做一个签名、然后发给服务器、服务器用这个签名和自己的公钥做一个验证、然后就可以登录了
    - 怎么去做上面的签名呢、就是用fingerprint、这样就不用输密码了

14. Bootstrapping

    - 在重置密码的时候、如何通过邮箱里的url来重置你的密码呢？会涉及到一个参数。但是其中包含一个随机数、所以一般人构造不出这个能够让你重置密码的url（进而修改任何人的密码）
    - 对于随机数、很多时候通过设备里的/dev/random来获取这个随机数（其中包含很多内容、比如键盘敲击的次数等）
    - 一个随机性更强的方法就是用熔岩灯的照片

15. 下面讲的是一些 Security Principles 

16. Principle of Least Privilege

    - 比如linux不要用root、要用普通用户、这样可以保护自己不做rm -rf /这种事情

17. Least Trust最小化可信

    - Privileged components are "trusted"，要尽可能减少这类组件
    - 增加 Untrusted components （does not matter if they break）
    - trust本身是好的、但是要尽可能减少

18. Users Make Mistakes

19. Cost of Security（安全是有代价的）

20. 下面开始讲ROP和CFI

21. ROP （Return Oriented Programming）

    - 一开始讲了一个例子、是如何通过buffer overflow中注入一点点代码就执行很多操作的、就是只需要注入batch命令的对应的二进制代码就可以了
    - 所以有了Defense: DEP (Data Execution Prevention)：就是用硬件来保证栈和堆可写不可执行、来避免上面的问题
    - 有了不可执行之后、第二个攻击是Attack: Code Reuse (instead of inject) Attack。这个核心就是复用当前的代码片段（ROP）、所以可以在覆盖的时候、多覆盖几个地址、然后通过return将很多个代码片段粘在一起（后面的几张ppt讲的是攻击的方法）
    - 怎么去防御呢：第一种方法是藏起来所有的二进制文件；第二种方法是ASLR、就是随机化地址空间、就是你看得到对应的代码、但是这样就不容易找到对应的地址了；第三种方法就是canary、就是在栈上面放一个随机数（因为canary肯定放在了return addr 前面）、然后在return的时候检查这个随机数是不是对的、如果不对就直接退出、去监测overflow

22. ASLR

    - 每次运行程序的时候、二进制的地址都是随机的
    - 但是如果是fork的话、子进程的地址是和父进程一样的、就没法这样随机了（如果要保证的话、那么成本就是很高的）

23. CFI：Control Flow Integrity

    - 就是用来defend ROP的
    - ROP的本质是覆盖掉原来的正常的control flow、然后用攻击者自己的control flow
    - 所以本质上需要在每次return的时候、返回call的下一行、而不是其他地方
    - 所以需要在二进制代码中、把固定的控制流约束给它嵌入进去、就能控制了

24. 控制流主要有直接跳转和间接跳转

    - 直接跳转：地址是固定的（direct call，jump）
    - 间接跳转：地址是不固定的（return，indirect call，jump）
    - 这里有一个表、显示了大量的运行时的控制流都是间接跳转的（然而在二进制文件中、更多的是直接跳转的代码）然后下面的数据显示、程序跳转到的位置并不是那么地发散
    - 所以能不能在binary里面加入程序的控制流的信息、让他不要乱跳

25. 这个上面有一个CFG的例子（Control-Flow Graph）

    - 具体是两个地方call同一个地方的函数、以及同一个地方call、两个不同的函数
    - 12345678是随机数、可以改的、用来比较是否是正确的control flow，如果不一样、就跳到一个error label（所以攻击者只能修改ecx、而不能修改这个硬编码的值12345678），因为其他地方也会检查
    - 这个硬编码的例子也有问题、因为有些是一些代码是发布的一个库、所以它不兼容（就是library里的代码已经是安全的了、但是实际应用程序里的代码可能还没有打patch、所以就会导致出错）
    - 所以这里有一个工程上的小trick 加入了一条prefetchnta 12345678的指令、它本身是一条可以执行的代码、但是这个12345678显然不是一个合法地址、所以它这个prefetchnta就偷偷地不执行了、就可以正常地运行没有打patch的代码了（可以正常执行那个硬编码的数字）（好处就是兼容性很强）

26. Suppose a call from A goes to C, and a call from B goes to either C, or D (when can this happen?) 

    - 这个会带来一个问题、就是原来A调用D是非法的、但是A和D的label是同样的12345678，就导致了A就能调用D
    - 还有一个问题、就是这个关于A、B、F中，return的标签都是一样的（就是两个地方调用一个地方的问题）（比如说A到F，B到F，然后F返回的时候、可能会返回到A或者B，这个时候就会有问题）

27. 所以需要做一个优化 CFI: Control Flow Enforcement

    - 每个标签都需要是唯一的，但是很难做到（需要复制很多的call、如果一个call的地址有10份）
    - 所以目前的选择是目前的CFI并不会这么细、目前还是粗粒度的CFI
    - 这个是微软提供出来的（CFI）

28. CFI: Security Guarantees

    - 防御了Stack-based buffer overflow, return-to-libc exploits, pointer subterfuge
    - 但是不防御 Does not protect against attacks that do not violate the program's original CFG （就是不防御不违背控制流图的攻击）

29. 后面这个Possible Execution of Memory的图就显示了在不同的实现下面、程序会有不同的可能的执行内存地址