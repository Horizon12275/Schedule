## 2021-2022-B

### 选择题

1. C

   - 选项 A 正确。过拟合常因样本不足或模型过于复杂（如高阶多项式拟合简单数据），导致模型过度捕捉训练集细节而非泛化规律。
   - 选项 B 正确。噪声数据会使模型学习无关特征（如数据中的随机波动），忽视真实特征，引发过拟合。
   - 选项 C 错误。欠拟合时，模型因拟合能力不足（如线性模型拟合非线性关系），其表现由偏差（Bias）​ 主导（高偏差），而非方差（Variance）。方差主导通常对应过拟合（高方差）。
   - 选项 D 正确。列举的方法（正则化、数据扩增等）均为经典过拟合解决方案，通过约束模型复杂度或增强数据多样性来提升泛化性。

2. A

   - ​ 线性回归用于预测连续型变量 ​（如房价、温度等），其输出为连续数值，符合题目要求。
   - ​ 逻辑回归用于解决分类问题 ​（如判断是否患病），输出是概率值（0-1 之间），不适用于连续变量的直接预测。

3. 在 K-折交叉验证方法中，数据集被分成 K 份，每次使用 K-1 份数据进行训练，剩下的一份数据用于测试。这个过程重复 K 次，每次使用不同的一份数据进行测试。因此，实际上所有样本都会参与训练。K-折交叉验证方法相比于留出法更适合于小样本数据，因为它能够充分利用有限的数据。自助法通过有放回抽样的方式从原始数据集中生成新的训练集，这可能会导致数据分布改变，并引入估计偏差。A 选项是错误的，留出法在划分数据集时，应该尽量保证训练集和测试集的数据分布一致。

4. 支持向量机（SVM）是一种二分类模型，它通过寻找一个最大间隔超平面来划分数据。线性可分 SVM 对噪声或异常样本不敏感。SVM 的代价函数是凸函数，不存在局部最小值。非线性 SVM 通过将原空间数据映射到新空间中，再使用线性分类学习方法进行分类。

5. 逻辑回归和支持向量机（SVM）都不属于无监督学习算法。逻辑回归是一种常用的分类算法，它通过拟合一个逻辑函数来预测类别标签。支持向量机（SVM）也是一种分类算法，它通过寻找一个最大间隔超平面来划分数据。

### 简答题

1. 激活函数在神经网络中起到非线性变换的作用。它将神经元的输入信号进行非线性变换，然后输出到下一层神经元。激活函数的引入使得神经网络能够拟合复杂的非线性关系。比如 Sigmoid 会把输入压缩到 0 到 1 之间，适合判断概率；ReLU 更简单直接，负数直接归零，正数原样输出，计算快还能缓解梯度消失；Tanh 则把数据压到-1 到 1 之间，适合需要区分正负的场景；好的激活函数要计算简单、容易求导，同时能控制输出范围。

2. 在回归模型中，如果不同特征的尺度差异过大，会导致梯度下降过程中收敛速度变慢。这是因为梯度下降算法通过沿着梯度的负方向更新参数来最小化损失函数。当不同特征的尺度差异过大时，损失函数的形状会变得扁平，导致梯度下降算法在不同方向上的更新速度不一致，从而影响收敛速度。为了解决这个问题，可以对特征进行归一化处理。常见的归一化方法包括最大最小归一化和 Z-Score 归一化等。归一化处理能够消除不同特征之间的尺度差异，使得损失函数的形状更加规则，从而加快梯度下降算法的收敛速度。

3. 支持向量机（SVM）是一种二分类模型，它的主要思想是通过寻找一个最大间隔超平面来划分数据。最大间隔超平面能够最大化两个类别之间的距离，从而提高模型的泛化能力。硬间隔 SVM 和软间隔 SVM 是两种不同的支持向量机模型。硬间隔 SVM 要求所有的样本都被正确分类，即所有的样本都满足间隔大于等于 1 的条件。硬间隔 SVM 适用于线性可分的数据，但对于线性不可分的数据，它无法找到一个合适的超平面。软间隔 SVM 则允许某些样本不满足间隔大于等于 1 的条件。它通过引入松弛变量和惩罚系数来允许一些样本被错误分类，从而解决线性不可分问题。软间隔 SVM 能够更好地处理噪声数据和异常值。核方法是支持向量机中用来解决非线性问题的一种技巧。它通过使用核函数来隐式地将数据映射到更高维的空间，使得原本线性不可分的数据在新的空间中变得线性可分。核方法能够避免直接计算高维特征映射带来的计算困难，提高了支持向量机处理非线性问题的能力。

### 计算题

2. 交叉熵直接衡量模型输出的概率分布与真实标签的差异，天然适合分类任务（尤其是多分类）。它鼓励模型对正确类别赋予高概率，对其他类别赋予低概率。​MSE 衡量的是数值上的平方误差，更适合回归任务（如预测连续值）。在分类中，MSE 会强制模型输出接近 0 或 1 的数值，但分类的本质是概率比较而非精确数值匹配。

## 2020-2021-A

### 判断题

1. F（错误。逻辑回归（Logistic Regression）是一个线性分类模型，它通过学习一个线性决策边界来对数据进行分类。当数据的特征空间存在非线性决策边界时，逻辑回归无法直接拟合这种非线性关系，因此不适合对其进行分类。对于非线性决策边界的问题，可以考虑使用以下方法：1. 对特征进行非线性变换（如多项式特征）2. 使用核方法的模型（如支持向量机 SVM）3. 使用非线性模型（如决策树、神经网络等）只有在原始特征空间或经过适当变换后的特征空间中，决策边界是线性可分的情况下，逻辑回归才是适合的。）
2. T
3. F
4. T？
5. T
6. F

### 选择题

1.
2. C 线性分类问题（如圆形或螺旋分布的数据）通常无法用原始特征空间的线性超平面分开。使用核函数（如 RBF 核）将数据映射到高维空间，使其线性可分。
3. B
4. A？
5. C？
6.

### 简答题

2. 第一层的词向量是直接从输入词得到的，只包含词本身的表面信息；经过多层 Self-Attention 后，最后一层的词向量融合了全局上下文，每个词都带着整个句子的关系信息；比如"苹果"在第一层可能只是水果，到最后一层就能区分是吃的苹果还是手机品牌；本质区别在于：第一层是独立的词表示，最后一层是理解了语义和关系的词表示。

### 计算题

### 系统设计题

### 1. 图书价格区间预测系统设计（分类任务）

**输入**：新图书的属性特征

- 结构化特征：定价（需排除，因是预测目标）、出版日期（可转化为时间戳或年份）
- 文本特征：标题、简介（需文本向量化，如 TF-IDF 或 BERT 嵌入）
- 可选衍生特征：简介长度、标题关键词（如"精装版"可能关联高价）

**输出**：三分类概率分布（低价/中价/高价）

- 最终类别取 argmax

**模型选择**：

- 基础方案：随机森林（适合结构化+文本特征组合，抗过拟合）
- 进阶方案：XGBoost/LightGBM（处理类别不平衡时可调 class_weight）
- 若文本特征重要：可尝试双层模型（文本用 NN 提取特征后与传统模型拼接）

**损失函数**：类别加权交叉熵（应对价格区间分布不均问题）

**优化方法**：

- 随机森林：默认即可
- 梯度提升树：早停法+网格搜索学习率/树深度
- 若用 NN：Adam 优化器

**关键环节**：

- 数据预处理：定价分箱时检查分布（避免区间样本量悬殊）
- 特征工程：出版日期转化为"距今时长"可能更有效
- 评估指标：F1-macro（兼顾各类别性能）

---

### 2. 用户图书推荐系统设计（序列推荐任务）

**输入**：用户最近 50 次阅读的图书 ID 序列

- 需映射为稠密向量：
  - 方案 1：图书 ID 直接嵌入（12000 维 →128 维）
  - 方案 2：结合图书属性特征（如将标题 BERT 向量与 ID 嵌入拼接）

**输出**：12000 维概率分布（下一本书的预测概率）

- Top-K 推荐取概率最高的 K 本

**模型选择**：

- 基础方案：GRU（比 LSTM 更轻量）+ 注意力机制（捕捉关键阅读行为）
- 进阶方案：Transformer 编码器（适合长序列依赖）
- 冷启动处理：对新用户用属性特征初始化隐藏状态

**损失函数**：Sampled Softmax（应对 12000 类别的计算效率问题）

**优化方法**：

- AdamW（带权重衰减防止过拟合）
- 学习率预热+余弦退火

**关键设计**：

- 序列增强：滑动窗口生成子序列（提升数据利用率）
- 负采样：混合热门书与随机负样本
- 评估指标：HR@10 + NDCG@10（关注 Top 推荐质量）

**强化学习四要素参考**（若采用 RL 框架）：

- State：当前阅读序列的隐状态表示
- Action：推荐某本书
- Reward：即时（点击/阅读时长） + 长期（留存率）
- Policy：DDPG 或 PPO 算法优化推荐策略

---

### 系统级考量（两者共用）

- 在线服务：模型需部署为微服务，推荐系统需支持高并发
- 数据闭环：收集用户反馈（如实际购买/忽略推荐）持续迭代模型
- AB 测试：新模型上线需对比点击率/客单价等业务指标

### (1) 深度学习（CNN）用于障碍物识别的设计思路

**关键要素及流程：**

1. **输入与数据准备**

   - **数据收集**：通过车载摄像头、激光雷达等传感器采集道路图像/点云数据（输入为 RGB 图像或多模态融合数据）。
   - **预处理**：归一化（像素值缩放到[0,1]）、数据增强（旋转、翻转、亮度调整）、标注（对障碍物如车辆、行人、交通标志进行边界框或语义分割标注）。

2. **模型架构（CNN）**

   - **主干网络**：采用预训练的 CNN（如 ResNet、EfficientNet）提取特征，或设计轻量化模型（如 MobileNet）以适应实时性需求。
   - **输出层**：
     - _分类任务_：Softmax 输出障碍物类别（如车辆、行人）。
     - _检测任务_：结合 Faster R-CNN 或 YOLO 输出边界框和类别概率。

3. **损失函数**

   - **分类任务**：交叉熵损失（Cross-Entropy Loss）。
   - **检测任务**：复合损失（如 YOLO 的定位损失+置信度损失+分类损失）。

4. **优化方法**

   - 使用 Adam 或 SGD 优化器，动态调整学习率（如 Cosine 衰减）。

5. **预测与部署**
   - 模型量化/剪枝后部署到车载计算单元，实时处理传感器输入并输出障碍物信息。

---

### (2) 强化学习（RL）训练自动驾驶控制系统的思路

**四要素框架：**

1. **状态（State）**

   - 包括车辆状态（速度、位置、航向角）、环境感知（周围车辆位置、车道线、交通灯）、历史轨迹等。可通过传感器或 CNN 提取的抽象特征表示。

2. **动作（Action）**

   - 离散动作：转向（左/右/保持）、加速/刹车。
   - 连续动作：方向盘转角、油门/刹车力度（需用 Actor-Critic 框架）。

3. **奖励函数（Reward）**

   - 正向奖励：安全驾驶（保持车道）、遵守交规（如绿灯通行）。
   - 负向奖励：碰撞惩罚、偏离车道惩罚。可设计为：  
     \[
     R_t = w_1 \cdot \text{速度跟踪} + w_2 \cdot \text{车道保持} - w_3 \cdot \text{碰撞}
     \]  
     （权重\(w_i\)需平衡不同目标）。

4. **策略（Policy）**
   - **算法选择**：
     - 离散动作：Deep Q-Network（DQN） + 经验回放。
     - 连续动作：Proximal Policy Optimization（PPO）或 SAC。
   - **训练流程**：
     1. 在模拟环境（如 CARLA）中初始化策略网络。
     2. 通过探索-利用（如 ε-greedy）生成轨迹，更新 Q 值或策略梯度。
     3. 目标网络稳定训练（DQN）或约束策略更新（PPO）。

梯度下降算法的步骤包括：用随机值初始化权重和偏差（d），把输入传入网络，得到输出值（c），计算预测值和真实值之间的误差（a），对每一个产生误差的神经元，调整相应的（权重）值以减小误差（e），重复迭代，直至得到网络权重的最佳值（b）。

半监督学习和无监督学习都是机器学习中常见的算法类型。它们的主要区别在于训练数据的标签信息。

半监督学习是介于监督学习和无监督学习之间的一种学习方法。它利用大量的未标记数据和少量的标记数据来进行模型训练。半监督学习的目的是利用未标记数据来改善监督学习中仅使用标记数据的性能。常见的半监督学习模型包括自学习（Self-Training）、共训练（Co-Training）、生成式方法（Generative Methods）和图方法（Graph-Based Methods）等。

无监督学习则是一种不需要标签信息的机器学习方法。它通过对无标签数据进行建模来发现数据中潜在的结构或模式。常见的无监督学习模型包括聚类（Clustering）、降维（Dimensionality Reduction）、关联规则（Association Rule Learning）和密度估计（Density Estimation）等

2. 请阐述机器学习中的欠拟合和过拟合现象，并解释造成过拟合的主要原因及主要解决办法。
   欠拟合和过拟合是机器学习中两种常见的问题。

欠拟合是指模型在训练数据上的拟合程度不够，模型过于简单，不能很好地捕捉数据中的关系。欠拟合会导致模型在训练数据和测试数据上的性能都不佳。

过拟合则是指模型在训练数据上的拟合程度过高，模型过于复杂，以至于捕捉到了数据中的噪声和异常值。过拟合会导致模型在训练数据上的性能很好，但在测试数据上的泛化能力较差。

造成过拟合的主要原因包括模型过于复杂、训练数据量不足、数据噪声过大等。解决过拟合的主要方法包括简化模型、增加训练数据量、使用正则化方法、早停（Early Stopping）等。

3. 在机器学习算法中，一种常见的策略是构造一个非线性函数 ϕ(𝑥)，将原始输入空间的输入 𝑥 通过 ϕ(𝑥)投射到另外一个空间。试描述 ϕ(𝑥)在两种不同的机器学习算法建模中的作用。
   在机器学习算法中，构造一个非线性函数 ϕ(𝑥)将原始输入空间的输入 𝑥 投射到另外一个空间，这种方法通常被称为特征映射（Feature Mapping）。特征映射可以用来解决线性不可分问题，通过将数据投射到更高维的空间，使得原本线性不可分的数据在新的空间中变得线性可分。

下面是两种不同的机器学习算法中特征映射的作用：

支持向量机（SVM）：支持向量机是一种二分类模型，它通过寻找一个最大间隔超平面来划分数据。当数据线性不可分时，可以使用核技巧（Kernel Trick）将数据投射到更高维的空间，使得数据在新的空间中变得线性可分。核技巧通过使用核函数来隐式地计算特征映射后的内积，避免了直接计算高维特征映射带来的计算困难。

神经网络（Neural Networks）：神经网络是一种非线性模型，它通过多层神经元的连接来拟合数据。每一层神经元都可以看作是对输入数据进行一次非线性变换，将数据投射到新的空间。通过多层神经元的叠加，神经网络能够学习到复杂的非线性关系。

4. 深度学习中，经常会遇到类别不平衡，请解释什么是类别不平衡；若遇到类别不平衡，有哪些缓解方法并说明各自的优缺点？

类别不平衡（class-imbalance）是指分类任务中不同类别的训练样例数目差别很大的情况。例如，有 998 个反例，但是正例只有 2 个，那么学习方法只需要返回一个永远将新样本预测为反例的学习器，就能达到 99.8%的精度；然而这样的学习器往往没有价值，因为它不能预测出任何正例 12。

若遇到类别不平衡，有一些缓解方法。一种基本方法是调节样本权重。另一种看起来“高端”一点的方法则是各种魔改 loss（比如 Focal Loss、Dice Loss、Logits Adjustment 等）3。

另外，还可以采用欠采样和过采样方法。欠采样是指直接对训练集中多数类样本进行“欠采样”（undersampling），即去除一些多数类中的样本使得正例、反例数目接近，然后再进行学习。过采样是指对训练集里的少数类进行“过采样”（oversampling），即增加一些少数类样本使得正、反例数目接近，然后再进行学习 2。

这些方法各有优缺点。例如，随机欠采样可能会导致丢弃含有重要信息的样本。随机过采样或数据增强样本也有可能是强调（或引入）片面噪声，导致过拟合 4。
