## 2.18 机器学习基础

1.  人工智能：能够处理⼀般需要⼈类知识和智⼒介⼊任务的计算机系统

    - 机器学习：从若⼲输⼊输出样例中学习数据规律的统计技术（机器学习是一类通过从数据中发掘潜在规律来不断优化自身性能，以实现特定目标的计算机算法。）
    - 深度学习：⼀种⾃动学习数据表征的机器学习技术

2.  传统编程：包含太多决策规则（比如关键词）

3.  机器学习的核心要素

    - 数据 (经验)
    - 模型 (假设)
    - 损失函数(目标)
    - 优化算法(提升)

4.  归根结底，机器学习是什么？

    - 定义：机器学习是一种人工智能技术，使机器能够在无需显式人工编程的情况下，通过从数据中自动发现潜在的统计规律（模型），并不断优化自身性能（优化），以实现特定的目标（如最小化损失函数）。

5.  推广到其他类型的数据

    - 向量化/特征提取(转化成高维向量空间中的点)

6.  以下人类学习行为分别对应哪种机器学习要素？

    1. 查文献 : 数据
    2. 考试 : 损失函数
    3. 错题总结 : 优化算法
    4. 挂科 : 没有

7.  如何构建一个机器学习系统？

    1. 考虑数据可⾏性和规模
       - 能获得多少数据? 需要多少代价 (时间、算⼒、⼈⼒成本)?
    2. 选择输⼊数据的合理表示形式
       - 数据预处理,特征提取,等等.
    3. 选择合理的模型假设
    4. 选择合适的损失函数
    5. 选择或设计⼀个学习算法

8.  机器学习分类

    - 监督学习：学习器的每个输入样本都有一个目标输出标签（有 “⽼师”的学习举例：老师教学生识别各种类型的动物。练习题提供标准答案）
      - 分类问题，回归问题
    - ⽆监督学习：Training examples as input patterns, with no associated output.
      - 聚类问题，降维问题，数据的概率密度估计，数据⽣成，异常检测
    - 强化学习：通过与环境交互来学习。学习状态(states)到动作(actions)的映射，以最大化长期奖励(reward)。（例: 练习题只给总分不给参考答案(刷题模式)）

9.  监督学习 vs.无监督学习

    - 监督学习
    - 数据:(x, y) x 表示数据,y 表示标签
    - 学习⽬标:学习函数映射 x →y
    - 典型任务:分类、回归、⽬标检测、语义分割等

    - 无监督学习
    - 数据:x 表示数据,⽆标签
    - 学习⽬标:学习数据隐含或潜在的结构
    - 典型任务:聚类、降维、概率密度估计等

10. 泛化性

    - 学习后的分类器能否处理没有见过的水果？
    - 学习的泛化性问题:模型应该学习普遍的规律还是记住特定细节?
    - 左边的模型更好，不复杂，泛化性更好

11. 欠拟合 vs 过拟合

    - 欠拟合–因模型不够复杂无法刻画真实规律，模型假设可能不成立
    - 过拟合–因模型过于复杂导致记住数据的细节(如随机噪声)而不是潜在的规律

## 2.25 机器学习基础

1. 避免过拟合（常见的避免过拟合方法）

   - 增加训练数据
   - 正则化(惩罚模型复杂度)
   - 数据划分&交叉验证(训练时用未知数据测试确保泛化能力)
   - 早停
   - 引入先验知识(如贝叶斯先验)

2. 措施一：数据划分

   - 模型拟合的是总体数据的分布而不仅仅是训练集的分布.
   - 为了评估泛化误差，需要在训练的时候保留一部分未知数据。
     - 数据划分(Hold-Out):
       - 训练集(e.g.,50%)
       - 验证集(e.g.,25%)
       - 测试集(e.g.,25%)

3. 措施二：交叉验证(CrossValidation)

   - 如何既划分数据，又能充分利用数据训练? (特别是当数据规模较⼩的时候)
   - K-折交叉验证
     - 将数据集分成 K 份
     - 依次将每份数据作为验证集，其余数据作为训练集
     - 计算 K 次验证结果的平均值

4. 措施三:早停(EarlyStop)

   - 在有过拟合迹象时及时停止训练

5. 措施四:正则化(Regularization)

   - 对参数增加惩罚项防止模型过拟合训练数据。
   - L1 正则化: $L1 = \lambda \sum_{i=1}^{n} |w_i|$
   - L2 正则化: $L2 = \lambda \sum_{i=1}^{n} w_i^2$

6. No free lunch 理论

   - 没有通用的最好的模型
   - 不同模型需要适配数据特征和应用

7. 机器学习中的回归问题

   - 回归问题:预测连续值

8. 线性回归

   - 使用一个线性函数来拟合数据
   - 损失函数:均方误差(Mean Squared Error)
     - $MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2$
   - 优化算法:梯度下降(Gradient Descent)
     - $w = w - \alpha \frac{\partial L}{\partial w}$

9. 线性回归的矩阵形式

   - 转化为矩阵求解问题

10. 正则化

    - 问题:XTX 可能不可逆
    - 解决方法:正则化(对损失函数引入一个惩罚项)
    - 对模型的惩罚变相地相当于数据的增强

## 2.28 决策树&随机森林&贝叶斯分类

1. 介绍了一下线性回归的具体算法实现

2. 然后讲了一下决策树模型

3. 回归 vs.分类

   - 回归:(predicts real-valued labels)
   - 分类:(predicts categorical labels)

4. 分类问题是机器学习处理的主要任务

5. 决策树和随机森林是最简单的分类算法

6. 分类=寻找一个决策边界，将特征空间划分成 2 个部分（每部分代表一个水果类型）

   - 非线性(cannotbelinearly separated)
   - 可能非数值 (e.g.,“male”, “female”)
   - 但是，沿着某些维度局部线性可分

7. 决策树的基本思想

   - 分而治之
   - 划分数据属性
   - 创建 if-then 规则

8. 决策树的模型结构,决策树包含三种类型的节点:

   1. 根节点(root node)
   2. 中间节点(Internal nodes)
   3. 叶子节点(Leaf nodes，terminal nodes)

   - 每个叶子节点对应一个类别标签
   - 每个非叶子节点包含属性测试条件， 用来根据属性不同特征划分数据.

9. 如何构建(训练)一棵决策树: 自顶向下分治学习

   1. 构造一个根节点，包含整个数据集.
   2. 选择一个最合适的属性
   3. 根据选择属性的不同取值，将当前节点的样本划分成若干子集；
   4. 对每个划分后的子集创建一个孩子节点，并将子集的数据传给该孩子节点；
   5. 递归重复 2~4 直到满足停止条件.

10. 如何高效率的构建决策树?

    - 选择每个属性 X 的时候，尽可能最大化标签 Y 的纯度
    - 决策树将数据逐步分割成越来越小的子集。最理想的情况是叶节点对应
      的样本属于同一类别(节点的标签纯净)。

11. 如何选择属性以最大化标签的纯度?

    - 选择 X 以最大化信息增益、Gini 系数等
    - 每⼀种指标对应⼀种决策树构造算法

12. 按照不同的属性划分原则，有以下常⻅算法：

    - ID3 (基于信息增益)
    - CART(基于 Gini Index)
    - C4.5 (基于信息增益率)

13. ID3 算法

    - 试着对每个属性进行划分，看看哪个划分效果“最好”
    - 评估样本的整体信息熵的降低程度
    - H(D) - H(D|X) = 信息增益 Gain(D, X)（即划分前后信息熵的差值）

14. 举例

    - Step1– 创建根节点
    - Step2– 计算每个(⼦)数据集的熵.
    - Step3– 对每个属性计算信息增益 IG,选择具有最⼤ IG 的属性
    - Step4–将最⼤ IG 的属性赋给当前(根)节点。对每个属性取值扩展⼀个分⽀⽤于创建新节点。
    - Step5–将数据集按选中的属性取值进⾏拆分，然后从数据集中移除该属性。
    - Step6–对每个划分后的⼦数据集，重复 2-5 步直到满⾜停⽌条件（递归）

15. 停止划分的条件

    - 纯度 – 叶⼦节点包含的样本均属于同⼀类
    - 数据量过小 – 叶⼦节点包含的样本数⼩于⼀个阈值
    - 没有属性可分(ID3)

16. 算法优点

    - 易理解, 可解释性, 便于可视化分析,
    - 数据预处理要求低,
    - 可以轻易处理非线性边界,
    - 数据驱动，可以以任意精度拟合训练集

17. 算法缺点

    - 容易过拟合
    - 某些噪声样例总能被树涵盖并学习

18. 随机森林

    - 将数据随机划分子集，各自构建决策树，将结果合并。
    - 往往比决策树更准确（因为随机划分的时候、会把噪声给去掉）
    - 训练速度快, 容易并行化 (训练时树与树之间是相互独立的).
    - 可以处理高维(feature 很多)数据，并且不用做特征选择 (因为特征子集是随机选择的).
    - 可以处理缺失属性.
    - 结果易解释 (i.e.,在训练完后，能够给出哪些 feature 比较重要).
    - 在训练过程中，能够检测到 feature 间的互相影响。
    - 对于不平衡的数据集来说，它可以平衡误差。

19. 贝叶斯分类

    - 就是这两类数据不能用直接进行划分，但是符合一定的概率分布，然后通过概率分布来进行分类，刻画这个模型（比如正态分布）
    - 就是一开始、我们把这个数据认为是从线来产生的、然后现在把这个数据看作是投色子的概率来产生的
    - 参数就是 XY 的联合概率分布，然后通过这个联合概率分布来进行分类
    - 损失函数就是最大后验概率

## 3.4 朴素贝叶斯&KNN

1. 朴素贝叶斯分类（根据上面的概率思想、进行分类）

   - 一开始的场景是，我们分类邮件是否是垃圾邮件
   - P(C1) = 85%(是正常邮件的概率)(先验概率)
   - P(C2) = 15%(是垃圾邮件的概率)(先验概率)
   - X：邮件中的词（特征）（比如钱、免费、优惠）
   - P(X|Ci) :似然概率，即在某种邮件下，某个词出现的概率
   - P(Ci|X) :后验概率，即在某个词下，这个邮件是垃圾邮件的概率
   - Decision rule: decide C1 if p(x|C1)P(C1) > p(x|C2)P(C2)

2. 如果是多维的、就是多个特征、就是多个词，多个随机变量（但是也可以把它先看作是一个变量，然后通过联合概率进行计算，就是一个概率分布表，统计出来就可以做分类了）

   - 问题在于：这个联合概率分布表、太大了、过于复杂难以计算

3. 朴素贝叶斯假设

   - 在我某一个类已知的情况下、这些特征是相互独立的
   - 就不用把所有的概率都理一遍了
   - 即 P(X|Ci) = P(X1|Ci)P(X2|Ci)P(X3|Ci)P(X4|Ci)P(X5|Ci)

4. How to estimate each P(c)?

   - 直接统计每个类别的概率

5. How to estimate P(xi|c) for each c?

   - P(xi|c) = count(xi, c) / count(c)
   - count(xi, c) : 在类别 c 下，特征 xi 出现的次数
   - count(c) : 类别 c 出现的次数
   - 本质上就是数数

6. 但是有一个问题、有可能在训练集中、有些词在某个类别下没有出现、这个时候就会出现概率为 0 的情况（zero-count）

   - 解决方法：拉普拉斯平滑（即给每个 value 加一个 1）
   - P(xi|c) = (count(xi, c) + 1) / (count(c) + 1)

7. 应用场景

   - 文本分类（因为容易计数，每一个单次就是一个属性）
   - 实时预测（因为计算量小，快）
   - 多分类问题（因为类别的数量是可以指定为多个的）

8. 垃圾邮件里的例子

   - 先算似然概率和先验概率
   - 然后通过贝叶斯公式计算后验概率，然后比对后进行分类

9. 优点：

   - 训练和预测速度快
   - 多分类问题效果很好
   - 对缺失数据不太敏感，易于维护、可以很快地处理新数据（只需要增加或者删改计数）

10. 缺点：

    - 假设过于简单，会导致欠拟合（但是实际上的效果还是可以的）
    - 就是有时候词之间的出现也是有关系的、但是通过假设、就是没有考虑这个关系

11. KNN：不需要模型的算法

    - 思想：物以类聚，人以群分
    - 算法：就是根据相邻的几个邻居做一个投票，然后根据投票结果进行分类
    - 问题：如何去度量这个距离

12. 欧氏距离的缺点：当维度变高的时候、受属性之间的区别关系就不大了

13. 实际数据分布的特点和缺点：

    - 就是在每个维度、数据的分布是不一样的，会导致在某些维度上、学习的效果不好
    - 解决方法：对每个维度进行归一化处理

14. 归一化方法：

    - 最大最小归一化：$x_{new} = \frac{x - min}{max - min}$
    - Z-score 标准化：$x_{new} = \frac{x - \mu}{\sigma}$

15. Similarity vs. Distance（相似度和距离）

    - 相似度：越大越相似
    - 距离：越小越相似

16. Cosine Similarity

    - 余弦相似度：$cos(\theta) = \frac{A \cdot B}{||A|| \cdot ||B||}$
    - 可以用来度量两个向量的相似度

17. 算法流程：

    - 决定 K
    - 计算测试样本和训练样本的距离
    - 选择距离最近的 K 个样本
    - 对这 K 个样本进行投票
    - 根据投票结果进行分类

18. 如何决定这个最好的 K 值？

    - 维诺图：就是 K 值和误差率的关系图（所有点之间等距离的边界）
    - K=1 的时候、就是垂直平分线
    - K>1 的时候、就是一个多边形，也会有很多空白区域（就是离邻居的距离都是相等的区域）
    - K 比较小的时候、分离面会比较小、过拟合
    - K 比较大的时候、分离面会比较光滑、会比较自然（ppt 上有图）

19. 优点：

    - 简单、易于理解
    - 无需训练
    - 新的数据很容易被加入

20. 缺点：

    - 计算量大
    - 高度数据相关
    - 需要大量的内存
    - 需要正则化处理

## 3.11 逻辑斯蒂回归

1. 决策树：构造决策面

2. 朴素贝叶斯：基于数据的概率分布

3. KNN：基于距离的分类找决策面

4. 上面方法的共性：找决策面

5. 把所有决策交给数据：抛开决策面和几何意义（逻辑斯蒂回归，利用数学函数进行投票统计）

6. 判别函数：用来将数据分开（可以是概率密度、也可以不是、只要能分开就可以了）

7. 决策边界：就是判别函数的值为 0 的地方

8. 判别函数：首先想到线性函数

   - 好处：简单、易于理解、可解释、准确
   - 几何意义：就是一个分离的超平面（半平面）
   - 由此便诞生了感知机

9. 感知机：就是一个最简单的线性分类器

   - 按照数据在超平面的哪一边来进行分类
   - 训练：如果预测出错，则更新权重（移动超平面进行修正）
   - 缺点：因为这个函数是阶跃的 01 函数，所以难以进行优化
   - 因此引入了刻画这个数据属于哪一类的程度（概率）

10. 为了体现 y 和 1-y 之间的关系，对 y/(1-y)取了 log、并与 0 对比

    - 即为 logit 函数：$logit(p) = log(\frac{p}{1-p})$
    - 可验证即相当于线性的

11. sigmoid 函数：logit 函数的反函数

    - $sigmoid(x) = \frac{1}{1+e^{-x}}$
    - 作用：将线性函数的输出映射到 0-1 之间
    - 完美的概率拟合公式

12. 逻辑斯蒂回归

    - 使用了 sigmoid 函数的分类器
    - 根据预测出的概率值进行分类

13. 损失函数设计

    - 要最大化 log p 的值
    - 因为当 r=1 时候、cost=-log(y)、当 r=0 时候、cost=-log(1-y)
    - 因此使用交叉熵损失函数：$L(y, \hat{y}) = -ylog(\hat{y}) - (1-y)log(1-\hat{y})$
    - 也相当于要最大化似然函数

14. 优化方法：梯度下降

    - 求导：$\frac{\partial L}{\partial w} = (y-\hat{y})x$
    - 更新：$w = w + \alpha(y-\hat{y})x$
    - 不断优化损失函数

15. 多分类的时候，对函数进行了扩展

    - softmax 函数：$softmax(x) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$
    - 作用：将线性函数的输出映射到 0-1 之间
    - 每个输入称为 logits
    - 最终得到所有类下的概率分布
    - soft：函数可微
    - max：选择最大的数值（原来是不可微的，离散的，softmax 把它变成了可微的）

16. softmax 里面、标签是一个向量、预测值也是一个向量

## 3.14 SVM 支持向量机

1. 最简单的线性分类器：感知机（通过函数的统一化思想）

2. 逻辑斯蒂回归：（概率）进行分类

3. 前言：线性分类：可能会有很多根线、要找到更好的那根分割线（横着还是竖着）

   - 应该选择把两边的间距最大化的那根线（因为这样可以更好的泛化）

4. 数学推导：要把那个小 δ 消去，相当于把两个平面加起来，然后这两个平面加起来之后、就是原来分割的超平面（w 是未知量、是要学习的变量）

   - 可以把那个小 δ 令为+1 和 -1，然后就可以得到一个等式
   - margin = 2/||w||
   - 优化目标：最大化 margin
   - 最后要最大化这个 margin 等效于要最小化 1/2 的 ||w||（||w||是 w 的范数，即 w 的长度）
   - 第二个约束：要在两个超平面外的至少距离为 1

5. 支持向量机：

   - 即一个凸优化问题（二次优化问题）
   - minimize $\frac{1}{2}||w||^2$
   - subject to $y_i(w \cdot x_i + b) \geq 1$
   - 结构：还是一个感知机
   - 区别：损失函数变化了

6. 损失函数：

   - hinge loss：$L(y, \hat{y}) = max(0, 1-y\hat{y})$
   - 作用：当预测正确的时候、损失为 0(因为已经满足了)，当预测错误的时候、损失为 1-y\hat{y}（相当于关注线和边界上的点）
   - 优点：对异常值不敏感

## 3.18 MLP 多层感知机

1. 根据人脑的神经元模型、提出了神经网络

2. 感知机：

   - 一个输入函数
   - 一个激活函数
   - 一个损失函数
   - 可以看作一个最简单的线性模型

3. 感知机最基本的作用：分类，逻辑运算（但是单个感知机没法解决异或问题）

4. 转折：但是隐藏层的加入、可以解决异或问题（即非线性问题，通过含隐藏层的多层感知机进行解决）

5. 还有一个问题：原来的 01 激活函数太简单的、不能进行优化，因此引入了 sigmoid 函数（连续的、可微的）

6. 三层：

   - 输入层：输入数据
   - 隐藏层：给数据进行初步的线性转换和非线性的激活（通过 sigmoid），然后告诉下一层
   - 输出层：把小的感知机的结果进行汇总

7. 多层感知机：万能的拟合工具（可以拟合很多非线性函数）

   - MLP 也叫 Fully Connected Neural Network

8. MLP 在干啥：

   - 狗的例子：第一层、先在图片里面找最基本的特征（比如有没有三角形、有没有轮廓），然后在后面层判断进一步细致的特征（比如是不是有两个眼睛）
   - 非线性变换的例子：相当于线性变换（旋转等）和非线性变换（拉伸和拟合）的组合，对一个空间进行一个揉捏

9. MLP 的相较于决策树的优势：连续可微的

10. MLP 中如何训练？学习的参数是巨大的

    - 但是本质上：还是要算梯度

11. 损失函数（需要最小化）：

    - MSE：$L(y, \hat{y}) = \frac{1}{2}(y-\hat{y})^2$

12. 通过导数的链式规则、推导得出了可以通过反向传播的方法、来进行参数的更新

    - 反向传播：就是通过链式法则、把损失函数的导数传递到每一层（得到结果之后、然后再反向传播）

13. 前向传播：

    - 从输入层到输出层的过程

14. 所以反向传播里有两个步骤，分别为正向和反向，然后合并起来就是梯度

    - 正向（forward pass）：计算 δa/δw
    - 反向（backward pass）：计算 δL/δa
    - 乘积：δL/δw（即梯度）

## 3.25 MLP 多层感知机 & Language Model

1. 问题：反向传播中使用递归的方式、会导致计算量巨大

   - 解决方法：使用动态规划的方式、把计算结果存储起来
   - a：就是激活函数的输入
   - w：就是权重
   - L：就是损失函数

2. 因此：权重的更新就是输入信号和误差信号的乘积

   - 输入信号：δa/δw
   - 误差信号：δL/δa

3. 算法过程总结

   - 正向传播：计算每一层的 δa/δw
   - 反向传播：计算每一层的 δL/δa
   - 更新权重：根据梯度更新权重

4. ppt 上有一个显示的流程

5. 为什么越深的神经网络越好呢？

   - 因为可以做模块化的学习（比如第一层学习轮廓、第二层学习眼睛等）
   - 而且不要想着一次性规划所有的任务
   - 这样模块化的分类可以使用更少的数据进行训练

6. 深度学习 deep learning

   - 通过数据，学习高层次的抽象
   - 通过多层次的特征提取，学习数据的高层次表示

7. 深度学习的表征：

   - 传统的：通过人工和专家知识来设计特征
   - 深度学习：通过数据来学习特征（通过训练学出来的特征）
   - 同时，深度学习学到的是层次化的特征（把数据逐层地去抽象）
   - 深度学习把原有的机器学习的特征提取和分类的过程合并到了一起（用一个黑盒）
   - 因为数据变多了、所以深度学习的效果会更好

8. 缺点：原有的 Feedforward DNN 非常难以训练

9. 下面开始讲 Language model

10. 首先：让计算机理解单词

    - 一开始，让单词变成编码（字典）
    - 但是这个符号并不能代表计算机能够理解单词
    - 因为这个编码没有意义
    - 因此有了 One-Hot Encoding：就是把单词变成一个向量（只有一个 1，其他都是 0）
    - 但是单词太多了、这个向量太稀疏了、因此引入了 Word Embedding
    - 因此这个 embedding 是基于：Two words are similar if they have similar word contexts

## 3.28 语言模型 Language Model

1. Word2Vec:如果两个词在同一个上下文中出现的概率相似，那么这两个词是相似的（通过一个上下文窗口 window 来进行判断）

2. 这个算法就是 Mikolov's CBOW 词袋模型

   - 输入：先用 one-hot 编码、然后通过一个线性变换、然后得到一个 word vector 的词向量
   - 所以这个 W 权重的特点：是一个查找表
   - 图中的例子、一个 one hot 的词向量乘以一个权重矩阵、就得到了这个单词在这个词向量空间的位置、
   - 最后通过一个 softmax 函数、得到了词汇表上的概率分布
   - 损失函数：NLL（负对数似然函数），使用梯度下降法进行优化
   - 结果：计算机能够理解单词的语义、就相当于计算机认识了单词

3. 接着就是要让计算机学会语言

   - 问题：如何评判计算机的语言能力
   - 即要刻画语言是大多数人说的、所以是标准的
   - 因此语言模型是概率模型，即计算一个句子的概率
   - 即如果知道概率、就能填词

4. 概率如何计算呢？

   - 把每个单词看成一组变量
   - 可以通过概率的链式法则去计算
   - P（w1, w2, w3, w4）= P(w1)P(w2|w1)P(w3|w1, w2)P(w4|w1, w2, w3)
   - 但是，这个会条件越来越长
   - 但是我们通过马尔科夫假设、就可以简化这个条件，即只需要前面的 k 个单词有关系（比如前两个单词）

5. 这个条件概率怎么算出来呢？

   - 核心就是：给了前面的单词、然后预测下一个单词
   - 朴素的想法就是计数（就是通过所有的语料库进行计算），但是弊端是没有办法应对变体，而且对相似词是不敏感的，而且本质上没有理解这句话
   - 为了克服相似词：即通过词向量的方式、把相似的词放在一起，但是也有一个问题，就是把顺序给忽略了
   - 为了顺序：就把每个向量拼接起来、然后通过一个神经网络进行训练（会考虑到顺序）但是也有问题，就是计算量太大了（而且有些句子有些是长的、有些是短的，没法固定一个大小的神经网络）而且这个模型把词的位置给限制死了，即使是一个空格，也会改变影响。同时无法刻画词之间的关系

6. 因此要设计一个神经网络，解决上面的问题

   - 保留单词间的关系，保留顺序，共享参数
   - 所以就有了 RNN（Recurrent Neural Network）
   - 先前网络的本质问题：一次只能处理一个单词
   - 因此 RNN 即要记忆前面的单词，处理的时候根据前面的记忆进行处理
   - 如何记忆？一个隐藏层本身就是一个记忆体（复用原来的隐藏层）
   - 把隐藏层加一个回路、就可以记忆了

7. RNN

   - 相当于把旧的状态作为了新的输入
   - 更新隐藏层：通过 tanh 函数
   - h*t = tanh(W_hh \* h*{t-1} + W_xh \* x_t)
   - 输出：y_t = W_hy \* h_t
   - 输入：x_t
   - 是一个典型的深度神经网络（因为在运行的时候、会随着时间递归，所以是一个很深的网络）
   - 如何训练：把所有的单词和标准值做一个比对、然后求一个 Loss 的和，然后反向传播（一样）
   - 应用：One to One、One to Many、Many to One、Many to Many

## 4.1 语言模型 Language Model 结尾

1. 拿到句子的向量（理解句子之后）、可以在输出那里再接一个分类器、就可以分类句子

2. 或者在一段句子的后面接一个分类器、就可以达到预测下一个字最大的概率、就可以生成句子。如果在这里引入交叉熵的话、那么也可以通过这个方法不断优化模型

## 4.8 Transformer

1. Sequence-to-Sequence Learning（序列生成问题）

   - ASR（语音识别）
   - Translation
   - Dialogue（对话）
   - 首先要对输入和输出进行理解

2. 编解码器模型（encoder-decoder model）

   - Encoder：通过 RNN 把输入的序列变成一个向量（上下文向量）（通常把最后时刻的状态作为编码、记为状态 c）
   - Decoder：把这个向量变成输出的序列（通过一个 RNN 来进行处理）（即是先前的语言模型、通过外加一个 MLP 多层感知机将每个结果文本输出）
   - 一开始做的是翻译问题（使用编解码器）
   - 后期如果用 CNN 作为解码器、就可以做图像生成问题（即把文本变成图像）

3. 不过如果当输入序列很长的时候

   - 就会导致信息的丢失（因为 RNN 是一个线性递增的、最后都是压成一个向量、没法把整段话去记录、没有办法对细节去做刻画）
   - 本质上的问题是最后只有一个向量（c）来进行表示、会导致信息的丢失
   - 解决方法：生成一个动态的向量（即每个单词对应一个特定的向量 ci，每个单词权重不同，这个即是注意力机制）

4. 注意力机制本质：每个单词在解码编码过程中关注不同的单词（即每个单词对其他单词的权重不同）

5. 这个权重 a 如何得到呢？

   - 通过一个神经网络的打分（score function）来进行计算（即通过一个函数来计算每个单词的权重）
   - score function：可以是一个简单的 dot product，也可以是一个复杂的函数（比如一个神经网络）
   - 然后通过 softmax 函数来进行归一化处理（转成 0-1 之间的概率），得到权重 a
   - 通过可视化 attention、可以看到这个模型确实是有效的，关注了句子中的不同单词

6. 接着人们把 attention 用在了 RNN 的编码上，即构造了 Transformer

   - RNN 的痛点：训练速度太慢了（因为天然的缺点：就是是一个串行的序列，而且当多层的时候、会有大量的 for 循环，而且解码器还需要再来一遍）
   - transformer 天然的优点：可以并行计算
   - **RNN**：通过维护隐藏状态，按顺序将当前处理的单词与之前的单词依次结合，是顺序性的依赖处理。
   - **自注意力机制**：能同时将其他相关单词的“理解”融入当前正在处理的单词，可并行处理所有单词间的关系，更高效地捕捉长距离依赖。（就是不是按顺序去读、前面的单词可以不需要读）

7. Self-Attention 可以看作是一个 query、然后对应要找的东西去找那个 key，然后把原句子中那个 value 取出来

   - 即 value vector \* score = value X score，随后求和得到 Sum 之后、即为对 query word 的新的状态

8. Self-Attention 具体实现：

   - 步骤 1 token embedding：即把所有的单词都变成一个向量（通过一个查找表）
   - 步骤 2 QKV vectors：通过一个线性变换，把每个单词的 embedding 变成 Q、K、V 三个向量（即 query、key、value）
   - 步骤 3 计算 attention score：通过 Q 和 K 的内积来计算每个单词的权重（score）(a = q \* k / sqrt(d_k))，然后通过一个线性变换来计算每个单词的权重（score）
     - 这里的 d_k 是 k 和 q 的维度（即 key 的维度），是一个缩放因子，用来防止 score 太大导致 softmax 函数的梯度消失
     - 这里的 score 是一个矩阵，表示每个单词对其他单词的权重（score）
   - 步骤 4 正则化 attention score：通过 softmax 函数来进行归一化处理（转成 0-1 之间的概率）
   - 步骤 5 计算 attention output：通过 V 和 attention score 的乘积来计算每个单词的输出 h'（即 value X score）（可以并行化进行一个 attention）
   - 每个单词，同时是 query、key、value（即每个单词都可以和其他单词进行交互）

## 4.11 Transformer && LLM

1. Self attention 的所有计算都可以表示成矩阵运算，可以使用 gpu 加速

   - 但是不是顺序处理的、为了保证保留顺序在其中的影响、需要加入一个位置编码（position encoding）
   - 位置编码：就是把每个单词的位置也加入到这个向量中（即把每个单词的位置信息 one-hot 也加入到这个向量中）
   - 可以完全取代 RNN、所以在 encoder 和 decoder 中使用了 self-attention layers 即喂 self attention model

2. transformer 的一个 encoder 有一个 attention 模块、decoder 有两个 attention 模块

   - encoder 中的第二个 attention 模块是一个交叉的注意力机制（cross attention），即 decoder 中的每个单词都可以和 encoder 中的每个单词进行交互

3. decoder 中的需要一个 mask

   - 因为需要做矩阵计算、然后矩阵的大小是要有限制的
   - 通过 mask 可以让模型只关心部分的单词
   - **自注意力掩码**：用于编码器和解码器，目的是忽略填充标记（padding tokens），填充标记对应位置为 -inf 。
   - **交叉注意力掩码**：仅用于解码器，用于忽略源序列中的填充标记，本质上和前一种相同
   - **因果掩码**：仅用于解码器，只关注已生成的词，当前词之后的词对应位置为 -inf ，当前词及之前已生成词对应位置为 0 。

4. 损失函数：交叉熵，优化：梯度下降

5. Pre-trained Language Models（预训练语言模型）

   - 首先通过无监督方法、对大规模的语料进行预训练（即通过无监督的方法进行训练）
   - 然后通过有监督的方法进行微调（即通过有监督的方法进行训练）

6. BERT

   ### 输入

   • **词元**：原始文本经分词（如 WordPiece）得到的单词或子词。
   • **特殊标记**：开头 `[CLS]`（用于分类等任务）、结尾 `[SEP]`（分隔句子）。
   • **位置编码**：表示词元在序列中的位置。
   • **词嵌入**：词元映射到固定维度的向量，与位置编码相加得到最终输入。

   ### 输出

   • **最后一层隐藏状态**：每个输入词元在最后一层 Transformer 编码器的输出向量，用于多种任务。
   • **[CLS]标记输出**：`[CLS]` 标记对应向量，常用于分类任务 。

7. BERT 具体步骤

   - 把 BERT 作为 encoder，CLS token（Classification Token）是 BERT 在输入序列起始位置添加的一个特殊标记，主要用于分类任务
   - Task1：通过 Masked Language Model（MLM）来进行预训练（即把句子中的一些单词随机的 mask 掉，然后让模型去预测这个单词是什么）（完形填空）
   - Task2：通过 Next Sentence Prediction（NSP）来进行预训练（即判断两个句子是否是连续的）
   - 微调，通过有监督的方法进行微调（即通过有标签的数据进行训练）比如情感分析、命名实体识别、信息抽取、句子因果关系预测等任务。这里是微调分类器

8. 把解码器也变成预训练的了、那么就是 GPT 了（Generative Pre-trained Transformer）

## 4.15 LLM

1. 一种使用方式是进行微调、还有一种方式是提示词工程加上 RAG 检索增强（Retrieval-Augmented Generation）

2. 微调：

   - 目前在非常大的模型下已经不太现实了（因为参数太多了、而且数据量也很大）
   - 但是在小模型下、还是可以进行微调的（比如 1 亿参数的模型）
   - 微调的本质：在本地通过新的数据进行重新训练（即通过新的数据进行训练）

3. PEFT（Parameter-Efficient Fine-Tuning）

   - 通过参数高效微调的方法、来进行微调（即只训练一部分参数）（但是能达成近似的效果）
   - 就是只训练一小部分参数、而不是全部的参数
   - LoRA：Low-Rank Adaptation，低秩适配（即把原来的权重矩阵分解成两个低秩矩阵的乘积）
   - 通过低秩矩阵的乘积来进行微调（即只训练一部分参数）（就是矩阵很大、但是有用的参数就一小部分）
   - “Tunable Soft Prompt”就是在提示微调方法中，那些能够被优化调整的、具有连续可优化特性的提示向量 。它们能够在不改变预训练模型主体参数的情况下，通过对这些软提示的调整来影响模型的输出行为。

4. LLM 的涌现能力

   - Emergent abilities：当模型的参数达到一定规模时，模型会展现出一些意想不到的能力（比如推理、数学、逻辑等）
   - 这个能力是随着参数的增加而增加的
   - 但是这个能力是不可预知的（即不是通过训练出来的）

5. 提示工程（Prompt Engineering）

   - 通过提示词来引导模型的输出
   - 提示词：就是给模型一个提示，让模型知道要做什么

6. Few-Shot Prompting

   - 通过给模型一些例子来引导模型的输出
   - 这个例子可以是自然语言、也可以是代码、也可以是其他的形式
   - 现学现用，但是没有参数更新

7. Chain-of-Thought Prompting

   - 思维链提示：一问一答的方式来引导模型的输出
   - 输出思考过程
   - 问题：没有与外界交互

8. ReAct

   - 解决了与外界交互的逻辑
   - ReAct​ 是一种结合 ​ 推理（Reasoning）​​ 和 ​ 行动（Action）​​ 的框架，主要用于增强大语言模型（LLM）在复杂任务中的表现，使其能够更灵活地解决需要多步推理和外部工具调用的任务。

9. RAG（Retrieval-Augmented Generation）

   - 检索增强生成：通过检索来增强模型的生成能力
   - 四个部分：
     - Embedding
     - Vector DB
     - 检索器
     - LLM
   - 首先进行 embedding，然后进行 retrieval（通过计算相似度的模式）

## 4.22 CNN

1. 彩色图片可以看作是一个三维的矩阵（长、宽、RGB），而黑白图片可以看作是一个二维的矩阵（长、宽）

2. 使用传统的 NN 的问题：会导致参数过多（因为每个像素都要连接到每个神经元）

3. 1-D Convolution

   - 1-D 卷积：就是对一维的序列进行卷积操作（比如时间序列、文本序列等）
   - 卷积核的大小可以根据需要进行调整（比如 3x1、5x1 等）
   - 卷积操作的好处：可以提取出局部特征，而且参数少了很多（因为只需要训练卷积核的参数）

4. CNN 通过卷积核（filter）来进行特征提取

   - 卷积核：就是一个小的矩阵（比如 3x3），通过这个卷积核和原始图片进行卷积操作，得到一个新的矩阵（feature map）
   - 卷积操作：就是把卷积核在原始图片上滑动，然后计算卷积核和原始图片的点积，得到一个新的矩阵（feature map）
   - 卷积操作的好处：可以提取出局部特征，而且参数少了很多（因为只需要训练卷积核的参数）

5. Multiple Input Channels（“3-D” Convolution）

   - 多通道输入：就是对多通道的图片进行卷积操作（比如 RGB 图片）
   - 卷积核的大小可以根据需要进行调整（比如 3x3x3、5x5x3 等）

6. Feature Map

   - Amapthatstores the locations of a specific feature activated by a filter
   - 特征图：就是卷积操作的结果（feature map）
   - 特征图的大小可以根据需要进行调整（比如 3x3、5x5 等）

7. Multiple Filters

   - 多个卷积核：就是对多通道的图片进行卷积操作（比如 RGB 图片）
   - 每个卷积核提取出不同的特征（比如边缘、纹理等）

8. Multiple Feature Maps

   - For example, if we had 6 5x5 filters, we’ll get 6 separate activation maps:
   - 多个特征图：就是对多通道的图片进行卷积操作（比如 RGB 图片）

9. Padding

   - 填充：就是在原始图片的边缘添加一些像素（比如 0 或者其他的值）
   - 填充的好处：可以保持卷积操作的结果和原始图片的大小一致（即不改变原始图片的大小）
   - 不至于把边缘的信息给忽略

10. Pooling (池化)

    - 池化：就是对特征图进行下采样操作（比如 max pooling、average pooling 等）
    - 池化的好处：可以减少特征图的大小，而且可以提取出更高层次的特征（比如边缘、纹理等）
    - 池化操作的好处：可以减少特征图的大小，而且可以提取出更高层次的特征（比如边缘、纹理等）
    - Max pooling：就是取特征图中最大的值（比如 2x2 的特征图，取最大的值）
    - Max pooling selects the brighter pixels from the image.（变得更亮）
    - Mean pooling：就是取特征图中平均值（比如 2x2 的特征图，取平均值）
    - Mean pooling method smooths out the image and hence the sharp features may not be identified.（变得模糊）

## 4.25 CNN & CV

1. 卷积核大小、图片大小和输出大小的关系

   - Input size: N×N
   - Filter size: F×F
   - Output size: (N - F) / stride + 1
   - 实际的时候把边缘 padding 了一层 0
   - Output size: (N +2P- F) / stride + 1

2. ​stride（步长）​ 是指卷积核在输入图像上滑动时的移动步幅。具体来说：​ 定义 ​：Stride 表示卷积核每次在水平或垂直方向上移动的像素数量。例如：当 stride=1 时，卷积核每次移动 1 个像素；当 stride=2 时，卷积核每次跳过 1 个像素，移动 2 个像素。

3. Can we let a CNN act as an MLP? How?

   - 仅保留 CNN 的全连接部分
   - 使用 1×1 卷积模拟全连接

4. Can we apply CNN for texts?

   - 把文本的矩阵当作图片，可以用的

5. Why use residual connection?（残差网络 ResNet ）

   1. gradients can propagate fasterthroughahighway(recallLSTM) （对于优化很有好处）
   2. within each block, only small residuals have to be learned
   3. letdeepermodeltoperformasgoodasshallowermodelbycopying the learned layers from the shallower model and setting dditional layers to identity mapping.（让深层的网络复习到浅层的特征）

6. 下面是 CV 的部分、讲了机器视觉的任务

7. 图像分割

   - 将图片分成不同的区域
   - 但是不需要对这个原图进行分割、可以先在 downsampling 的结果上进行分割然后再复原成原来的大小（unpooling，upsampling）
   - 对图像分割的时候实际不需要和原来的大小一样（所以可以 Upsampling）

## 4.29 CV

1. 为了在复原图片为原来的大小的时候、不仅仅采用原始的马赛克的放大方式（重复、取 0）

   - 把卷积核铺开来，得到 Convolution Matrix（卷积矩阵）
   - 转置之后得到 Transposed Convolution Matrix（转置卷积矩阵）
   - 通过 Transposed Convolution 做 Upsampling（逆卷积的思想，但是并不是在复原图片，依然有参数需要学习）
   - 类似于 encoder 和 decoder 的关系（在这里是 convolution 和 deconvolution 的翻译关系）
   - 这个图片版的模型就叫做 U-Net（因为长得像 U）

2. 然后讲了目标监测任务

   - 首先是 Single Object 的场景
   - 分类+定位的综合操作（Single Object = Class Scores + Box Coordinates）
   - 但是问题是不能预测有多少个目标、也不能预测目标的种类

3. 解决方法：Region Proposals: Selective Search

   - 通过一个算法来进行选择（即通过一个算法来进行选择）
   - 这个算法的思路是：先把图片分成很多个小块，这些小块里面可能有目标
   - R-CNN：首先通过 Selective Search 算法来进行选择，然后通过 CNN 来进行分类和定位（即通过 CNN 来进行分类和定位），但是原始的方法太慢了
   - Fast R-CNN：先卷积、拿到 feature map、然后再做 R-CNN
   - Faster R-CNN：做一个 local 的 Region Proposal 方法、及分割出可能有目标的区域

4. 进一步改进：Yolo

   - 把目标监测和区域监测合并到一步里

5. 其他任务：Image Captioning（图像描述）

   - 即是一个翻译问题
   - 使用 Encoder-Decoder with Attention 方法解决
   - 把 feature map 接到 Transformers
   - 进一步优化：不需要 CNN 的卷积、因为 Transformer 和卷积都是提取特征的
   - 所以有了 Vision Transformer（ViT）
   - ViT：把图片变成一个个区块，然后通过 Transformer 来进行处理（即把图片变成一个个区块，然后通过 Transformer 来进行处理）
   - BEIT：BERT Pre-Training of Image Transformers

## 5.6 CV & kmeans & Dimensionality Reduction

1. CLIP – Contrastive Language-Image Pretraining

   - CLIP 是一种结合了图像和文本的预训练模型，旨在通过对比学习来理解图像和文本之间的关系。它使用了大规模的图像-文本对进行训练，使得模型能够在没有明确标签的情况下进行图像和文本的匹配和检索。CLIP 的核心思想是通过对比学习来最大化图像和文本之间的相似性，从而实现跨模态的理解和生成。
   - 通过计算向量之间的相似度来实现

2. 还有一种任务：聚类任务

   - 场景：需要处理无标签的数据、即进行无监督的学习
   - 聚类：把数据按照相似度进行分组
   - 方法：给每个组选择一个代表（centroid），每个数据有一个标签来指向这个代表

3. K-means 聚类算法步骤：

   - 选择 k 个随机数据点作为初始质心（聚类中心）
   - 将每个数据点分配给最近的质心（使用 L2 距离公式计算）（label update）
   - 根据当前聚类成员重新计算质心（取均值）（centroid update）
   - 重复分配和更新步骤直至收敛

   1. 关键公式：

      - 距离计算：$L_2(x,\mu^k)=\sqrt{\sum_{m=1}^{d}(x_i-\mu_m^k)^2}$
      - 质心更新：$\mu^k=\frac{1}{C_k}\sum_{x\in C_k}x$

   2. 终止条件：
      - 达到预设的收敛标准（如质心不再显著变化）

4. 收敛准则：

   - 无（或最小）数据点到不同簇的重新分配
   - 无（或最小）质心的变化
   - 误差平方和（SSE）的最小减少量

5. 误差平方和（SSE）公式：

   - $\min_{\{\mu^k\}_{k = 1}^K}\sum_{k = 1}^K\sum_{x\in C_k}L(x - \mu^k)$
   - 其中质心计算：$\mu^k=\frac{1}{C_k}\sum_{x\in C_k}x$

6. K-means 时间复杂度分析：

   - 计算两个实例间距离的时间复杂度：O(d)，d 为向量维度
   - 重新分配聚类簇的时间复杂度：O(knd)，k 为簇数，n 为实例数
   - 计算质心的时间复杂度：O(nd)，每个实例向量被加到某个质心一次
   - 总时间复杂度：O(Iknd)，I 为迭代次数

7. 不好的地方：

   - 结果是不确定的
   - 首先、聚类的个数是不确定的
   - 其次、初始的质心是随机的
   - 即有 Sensitivity to Initial Centroids (Seeds)

8. 如何选择 K 值

   - 使用肘部法确定最佳 K 值
   - 计算每个 K 值对应的组内平方和（WSS）
   - WSS 定义为簇成员与其质心间距离平方和
   - 选择 WSS 下降趋势变缓的 K 值（肘部点）

9. 另一种方法：k-means++ 如何选择初始质心

   - 优化初始质心选择
   - 步骤：随机选择第一个中心点
   - 计算每个点与最近已选中心的距离 D(x)
   - 按 D(x)^2 的概率分布选择新中心点
   - 重复选择直到选出 k 个中心点
   - 最后运行标准 k-means 算法

10. 应用：

    - 图像分割
    - 文本聚类

11. Pros and Cons

    - Pros：简单易用、快速收敛、适用于大规模数据
    - Cons：对初始质心敏感、对噪声和异常值敏感、需要预先指定 K 值，把聚类看作是一个球形，数据是没有均值的（只有向量）

12. 然后再讲了 Dimensionality Reduction

    - 降维：把高维的数据降到低维（即把高维的数据降到低维）
    - 目的：减少数据的维度、去除冗余信息、保留重要核心的信息（使得能够做概率处理、降低复杂度等）

## 5.9 Dimensionality Reduction & Deep Generative Models

1. 降维有两类方法

   - 线性降维：PCA、LDA...
   - 非线性降维：t-SNE、UMAP...

2. PCA（主成分分析）

   - PCA 是一种线性降维方法，旨在通过正交变换将数据投影到较低维度的空间中，同时保留数据的主要特征。它通过计算数据的协方差矩阵并找到其特征值和特征向量来实现降维。PCA 的核心思想是选择具有最大方差的方向作为新的坐标轴，从而减少数据的维度。
   - 相当于保留了数据在某一个方向上变化最剧烈、这个就是一个主要的成分。因此有最大方差的方向就是主成分
   - 相当于一个 QP 问题（Quadratic Programming Problem），即一个二次规划问题（要最大化协方差）
   - 解法：对数据进行中心化处理（去均值），然后计算协方差矩阵，接着计算协方差矩阵的特征值和特征向量，最后选择前 k 个特征向量（特征值）作为新的基底进行组合
   - 如何决定 k 值：通过累计贡献率来决定（即前 k 个特征值的和占总特征值的比例）（info loss）
   - 应用：图像压缩、数据可视化、数据预处理

3. Auto-encoder

   - 自编码器：是一种无监督学习的神经网络模型，旨在通过编码器将输入数据压缩到低维空间中，然后通过解码器将其重建为原始数据。自编码器的核心思想是通过最小化输入数据和重建数据之间的差异来学习有效的低维表示。
   - 由编码器和解码器两部分组成
   - 编码器：将输入数据映射到低维空间（latent space）
   - 解码器：将低维表示映射回原始数据空间
   - 训练目标：最小化输入数据和重建数据之间的差异（通常使用均方误差作为损失函数）
   - 应用：图像去噪、异常检测、特征提取
   - Deep Auto-encoder：深度自编码器是自编码器的扩展版本，具有多个隐藏层。它通过堆叠多个编码器和解码器来学习更复杂的低维表示。深度自编码器可以捕捉到更高层次的特征，并在处理复杂数据时表现更好。
   - Denoising Auto-encoder：去噪自编码器是一种特殊类型的自编码器，旨在从部分损坏或噪声数据中恢复原始数据。它通过在输入数据中添加噪声来训练模型，使其能够学习到更鲁棒的特征表示。去噪自编码器的训练目标是最小化输入数据和重建数据之间的差异，同时忽略噪声部分。

4. Probability Distributions for Images：学习的是图片的概率分布

   - Goal: find a p𝑚𝑜𝑑𝑒𝑙(x) that approximates p𝑑𝑎𝑡𝑎(x) well.
   - Generator G：一个神经网络，将随机噪声 z 映射到数据空间 x（一个概率转换器）
   - Discriminator D：一个神经网络，判断输入数据 x 是真实数据还是生成数据（要让两个分布尽可能接近）

5. Generative Adversarial Network (GAN)

   - GAN 是一种生成对抗网络，由生成器和判别器组成。生成器负责生成新的数据样本，而判别器负责判断输入样本是真实的还是生成的。（两个目标其实是是相反的）GAN 的核心思想是通过对抗训练来优化生成器和判别器，使得生成器能够生成更真实的数据样本。
   - 通过对抗训练来优化生成器和判别器
   - 目标函数：最小化生成器的损失函数和最大化判别器的损失函数
   - 损失函数：交叉熵损失函数
   - 应用：图像生成、图像修复、图像超分辨率

## 5.13 Deep Generative Models

1. Deep Convolutional GANs (DCGAN)

   - DCGAN 是一种深度卷积生成对抗网络，结合了卷积神经网络（CNN）和 GAN 的优点。它使用卷积层来提高生成器和判别器的性能，从而生成更高质量的图像。DCGAN 的核心思想是通过使用卷积层来捕捉图像的空间特征，从而提高生成器和判别器的性能。
   - 通过使用卷积层来捕捉图像的空间特征
   - 通过使用反卷积层来进行上采样
   - 应用：图像生成、图像修复、图像超分辨率
   - 比如把男的戴眼镜-男的不带眼睛+女的不带眼镜=女的戴眼镜（向量运算）

2. Conditional GANs

   - 条件 GAN 是一种条件生成对抗网络，通过在生成器和判别器中引入条件信息来控制生成的样本。它可以根据给定的条件（如标签、文本描述等）生成特定类型的数据。条件 GAN 的核心思想是通过引入条件信息来控制生成的样本，从而实现更精确的生成。
   - 通过在生成器和判别器中引入条件信息来控制生成的样本
   - 应用：图像生成、图像修复、图像超分辨率

3. CycleGAN: Domain Transformation （风格迁移）

   - CycleGAN 是一种循环一致性生成对抗网络，旨在实现无监督的图像到图像转换。它通过引入循环一致性损失来确保生成的图像在两个域之间保持一致。CycleGAN 的核心思想是通过引入循环一致性损失来实现无监督的图像到图像转换，从而避免了对成对训练数据的需求。
   - 通过引入循环一致性损失来实现无监督的图像到图像转换
   - 具体实现：两个转换器 GAB 和 GBA，分别将图像从域 A 转换到域 B 和从域 B 转换到域 A。通过引入循环一致性损失来确保生成的图像在两个域之间保持一致。通过判断 A 的 Loss 来判断是否能够复原
   - 无监督
   - 应用：风格迁移、图像修复、图像超分辨率

4. Variational Autoencoders (VAE)

   - VAE 是一种变分自编码器，通过引入变分推断来学习数据的潜在表示。它将输入数据映射到潜在空间，并通过重参数化技巧来实现高效的采样。VAE 的核心思想是通过引入变分推断来学习数据的潜在表示，从而实现更高效的生成。（通过 encoder 和 decoder 来进行生成）
   - 通过引入变分推断来学习数据的潜在表示
   - 可以控制生成图片的风格
   - 应用：图像生成、图像修复、图像超分辨率

## 5.20 Deep Generative Models && Reinforcement Learning

1. 以前的模型的缺陷

   - 自编码器 VAE：降维模型、损失细节、生成的效果不好

2. 扩散模型：逐步生成图片

   - 把图片变成噪声，将生成图片的过程认为是不断的去噪声的过程
   - 有两个过程：正向扩散和反向扩散
   - 正向扩散：将图片变成噪声的过程
   - 反向扩散：将噪声变成图片的过程
   - 正向每一步：将图片加上一个噪声 gaussian noise
   - 反向每一步：将噪声减去一个噪声（让神经网络去预测哪里是噪声）

3. 有监督学习和无监督学习的区别

   - 有监督学习：通过标签来进行训练（即通过标签来进行训练）（例子：分类、回归等）
   - 无监督学习：通过数据本身来进行训练（即通过数据本身来进行训练）（例子：聚类、降维等）
   - 强化学习：通过奖励来进行训练（即通过奖励来进行训练）（例子：游戏、控制等）

4. 马尔可夫决策过程（MDP）

   - MDP 是一种数学模型，用于描述强化学习中的决策过程。它由状态空间、动作空间、转移概率和奖励函数组成。MDP 的核心思想是通过最大化累积奖励来优化决策策略，从而实现最优的决策。
   - 状态空间：所有可能的状态集合
   - 动作空间：所有可能的动作集合
   - 转移概率：从一个状态到另一个状态的概率
   - 奖励函数：每个状态下采取某个动作所获得的奖励
   - 目标：最大化累积奖励

5. Q-learning

   - Q-learning 是一种无模型的强化学习算法，通过学习状态-动作值函数来优化决策策略。它通过与环境的交互来更新 Q 值，从而实现最优的决策。Q-learning 的核心思想是通过学习状态-动作值函数来优化决策策略，从而实现最优的决策。
   - Q 值：表示在某个状态下采取某个动作所获得的预期奖励
   - 更新公式：$Q(s,a) = Q(s,a) + \alpha[r + \gamma \max_{a'}Q(s',a') - Q(s,a)]$
   - 目标：最大化累积奖励

## 5.23 Reinforcement Learning

1. 强化学习的目标是最大化累积奖励

   - 通过与环境的交互来更新 Q 值，从而实现最优的决策
   - 通过学习状态-动作值函数来优化决策策略，从而实现最优的决策
   - 通过最大化累积奖励来优化决策策略，从而实现最优的决策

2. 蒙特卡洛+强化学习

   - 蒙特卡洛方法：通过随机采样来估计期望值
   - 蒙特卡洛强化学习：通过随机采样来估计 Q 值
   - 蒙特卡洛强化学习的核心思想是通过随机采样来估计 Q 值，从而实现最优的决策
   - 通过与环境的交互来更新 Q 值，从而实现最优的决策
   - 通过最大化累积奖励来优化决策策略，从而实现最优的决策
   - 缺陷：需要完整的轨迹才能进行更新（即需要完整的轨迹才能进行更新），无穷尽

3. 时间差分学习（Temporal Difference Learning）

   - TD 学习：结合了蒙特卡洛方法和动态规划的方法
   - TD 学习的核心思想是通过与环境的交互来更新 Q 值，从而实现最优的决策
   - 通过最大化累积奖励来优化决策策略，从而实现最优的决策
   - TD 学习的核心思想是通过与环境的交互来更新 Q 值，从而实现最优的决策
   - 通过最大化累积奖励来优化决策策略，从而实现最优的决策
   - 递归公式：$Q(s,a) = Q(s,a) + \alpha[r + \gamma Q(s',a') - Q(s,a)]$

4. Q-learning 的流程

   - 初始化 Q 值
   - 选择动作（ε-greedy 策略）
   - 执行动作，观察奖励和下一个状态
   - 更新 Q 值
   - 重复以上步骤，直到收敛

5. Q-learning 的优缺点

   - 优点：简单易用、收敛速度快、适用于大规模数据
   - 缺点：对初始 Q 值敏感、对噪声和异常值敏感、需要预先指定学习率和折扣因子
   - 需要大量的训练数据才能收敛
   - 需要大量的计算资源才能收敛

6. DQN（Deep Q-Network）

   - DQN 是一种深度强化学习算法，通过引入深度神经网络来近似 Q 值函数。它结合了 Q-learning 和深度学习的优点，从而实现更高效的决策。DQN 的核心思想是通过引入深度神经网络来近似 Q 值函数，从而实现更高效的决策。
   - 通过引入深度神经网络来近似 Q 值函数
   - 通过经验回放和目标网络来提高训练效率
   - 应用：游戏、机器人控制、自动驾驶等
   - 把状态看作图片、输入到 DNN 里
   - 训练：预测 Q 值、计算 MSE 损失、更新参数
   - 目标：最大化累积奖励
   - 把迁移的历史存储下来、放到数据集中
   - 问题：这个 target 也在不断的更新、所以需要一个 target network 来进行更新

7. DQN+Experience Replay 的流程

   - 初始化经验回放池
   - 初始化 Q 网络和目标网络
   - 选择动作（ε-greedy 策略）
   - 执行动作，观察奖励和下一个状态
   - 将经验存储到经验回放池中
   - 从经验回放池中随机采样一批数据
   - 更新 Q 值
   - 重复以上步骤，直到收敛

8. Policy Learning

   - 策略学习：直接学习策略函数 π(a|s)，即在给定状态下选择动作的概率分布
   - 策略函数：表示在给定状态下选择动作的概率分布
   - 策略梯度方法：通过计算策略函数的梯度来优化策略
   - 策略迭代：通过交替执行策略评估和策略改进来优化策略
   - 策略评估：计算当前策略的价值函数
   - 策略改进：通过最大化价值函数来优化策略
   - Episode：一个完整的轨迹，从初始状态到终止状态
   - 轨迹：一个完整的状态-动作序列
   - 轨迹的价值：轨迹中所有奖励的总和
   - 轨迹的价值函数：表示在给定状态下采取某个动作所获得的预期奖励
   - 目标：最大化轨迹的价值函数

9. Policy Gradient 流程

   - 初始化策略函数
   - 选择动作（根据当前策略函数）
   - 执行动作，观察奖励和下一个状态
   - 更新策略函数
   - 重复以上步骤，直到收敛
